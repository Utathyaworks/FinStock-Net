{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-01T04:38:05.763601Z",
     "iopub.status.busy": "2025-07-01T04:38:05.763274Z",
     "iopub.status.idle": "2025-07-01T04:38:05.796827Z",
     "shell.execute_reply": "2025-07-01T04:38:05.796122Z",
     "shell.execute_reply.started": "2025-07-01T04:38:05.763577Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Paper 1\n",
    "* This Paper does Prediction on BSE Index data\n",
    "* The date of their data ranges from 3rd April 1979 to 13th January 2022\n",
    "* It uses MAE metric\n",
    "* The best result quoted by them is MAE:402.140232\n",
    "* Link to paper:-https://scholar.google.com/citations?view_op=view_citation&hl=en&user=yO6rbN8AAAAJ&citation_for_view=yO6rbN8AAAAJ:ULOm3_A8WrAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:38:05.798082Z",
     "iopub.status.busy": "2025-07-01T04:38:05.797770Z",
     "iopub.status.idle": "2025-07-01T04:38:05.841850Z",
     "shell.execute_reply": "2025-07-01T04:38:05.841239Z",
     "shell.execute_reply.started": "2025-07-01T04:38:05.798044Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/kaggle/input/personal-dataset/sensex_paper3.csv\")\n",
    "#data=data[::-1]\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.nunique()\n",
    "data.head(),data.dtypes,data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:38:05.843629Z",
     "iopub.status.busy": "2025-07-01T04:38:05.843400Z",
     "iopub.status.idle": "2025-07-01T04:38:05.947938Z",
     "shell.execute_reply": "2025-07-01T04:38:05.946939Z",
     "shell.execute_reply.started": "2025-07-01T04:38:05.843609Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"/kaggle/input/personal-dataset/sensex_paper3.csv\")  # Replace with actual filename\n",
    "\n",
    "# Strip spaces and convert Change % properly\n",
    "df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "# Convert numeric columns (except 'Vol.') to float\n",
    "numeric_cols = [\"Close\", \"Open\", \"High\", \"Low\"]\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col].str.replace(',', '', regex=True), errors='coerce')\n",
    "def clean_change(x):\n",
    "    if isinstance(x, str):\n",
    "        # Remove the '%' symbol (only at the end, if present)\n",
    "        if x.endswith('%'):\n",
    "            x = x[:-1]\n",
    "        return x.strip()\n",
    "    return x\n",
    "\n",
    "df[\"Change %\"] = df[\"Change %\"].apply(clean_change)\n",
    "df[\"Change %\"] = pd.to_numeric(df[\"Change %\"], errors='coerce')\n",
    "\n",
    "\n",
    "# Convert 'Vol.' column while keeping important data\n",
    "def convert_volume(val):\n",
    "    if pd.isna(val) or val == '-':  # Handle NaN or '-' cases\n",
    "        return np.nan\n",
    "    val = val.strip().replace(',', '')  # Remove spaces & commas\n",
    "    if val[-1] == 'K': return float(val[:-1]) * 1e3\n",
    "    if val[-1] == 'M': return float(val[:-1]) * 1e6\n",
    "    if val[-1] == 'B': return float(val[:-1]) * 1e9\n",
    "    return float(val)\n",
    "\n",
    "df[\"Vol.\"] = df[\"Vol.\"].apply(convert_volume)\n",
    "\n",
    "# Drop Date column\n",
    "df.drop(columns=[\"Date\",\"Vol.\",\"Change %\"], inplace=True)\n",
    "\n",
    "# Forward fill missing 'Vol.' values to retain important data\n",
    "#df[\"Vol.\"].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Drop rows where 'Close' is still NaN (essential for LSTM training)\n",
    "df.dropna(subset=[\"Close\"], inplace=True)\n",
    "\n",
    "# Final check\n",
    "print(df.info())  # Ensure all columns are numeric\n",
    "print(df.head())  # Verify cleaned data\n",
    "data=df\n",
    "data.isnull().sum(),data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:38:05.949630Z",
     "iopub.status.busy": "2025-07-01T04:38:05.949334Z",
     "iopub.status.idle": "2025-07-01T04:38:05.958087Z",
     "shell.execute_reply": "2025-07-01T04:38:05.957314Z",
     "shell.execute_reply.started": "2025-07-01T04:38:05.949609Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:38:05.959298Z",
     "iopub.status.busy": "2025-07-01T04:38:05.958991Z",
     "iopub.status.idle": "2025-07-01T04:40:48.448955Z",
     "shell.execute_reply": "2025-07-01T04:40:48.448091Z",
     "shell.execute_reply.started": "2025-07-01T04:38:05.959269Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Multiply, Activation\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Set Seed for Reproducibility\n",
    "# ---------------------------\n",
    "def set_random_seed(seed=20):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "set_random_seed(20)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load & Preprocess Data\n",
    "# ---------------------------\n",
    "# Make sure 'data' is already loaded (e.g., from a CSV)\n",
    "df = data.copy()\n",
    "\n",
    "# Scale all features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Get index of the \"Close\" column (target)\n",
    "close_index = df.columns.get_loc(\"Close\")\n",
    "\n",
    "# Create sequences for time series prediction\n",
    "def create_sequences(data, time_steps=15, target_index=0):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i : i + time_steps])                  # shape: (time_steps, features)\n",
    "        y.append(data[i + time_steps, target_index])        # single value: Close price\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences using all features to predict Close\n",
    "X, y = create_sequences(scaled_data, time_steps=15, target_index=close_index)\n",
    "\n",
    "# Train/val/test split\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.1)\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Build Model (unchanged)\n",
    "# ---------------------------\n",
    "def build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n",
    "    inputs = Input(shape=(seq_length, X_train.shape[2]))  # Updated dynamically based on feature count\n",
    "\n",
    "    # Branch 1: 15-day trend\n",
    "    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n",
    "    full_branch = Dense(64, activation='relu')(full_branch)\n",
    "    long_term_return = Dense(1, activation='tanh')(full_branch)\n",
    "    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n",
    "\n",
    "    # Branch 2: last 7 days\n",
    "    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)\n",
    "    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n",
    "    recent_branch = Dense(64, activation='relu')(recent_branch)\n",
    "    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n",
    "    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n",
    "\n",
    "    # Branch 3: last 1 day\n",
    "    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)\n",
    "    very_recent_branch = Flatten()(very_recent_input)\n",
    "    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n",
    "    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n",
    "    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n",
    "\n",
    "    # Gates\n",
    "    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)\n",
    "    gate_1 = Dense(1, activation='sigmoid')(short_term_return)\n",
    "\n",
    "    gated_mid_term_return = Multiply()([mid_term_return, gate_7])\n",
    "    gated_short_term_return = Multiply()([short_term_return, gate_1])\n",
    "\n",
    "    # Combine\n",
    "    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n",
    "    fused_returns = Dense(32, activation='relu')(fused_returns)\n",
    "    final_return = Dense(1, activation='tanh')(fused_returns)\n",
    "    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n",
    "\n",
    "    # Output layer (based on last close price)\n",
    "    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, close_index], axis=-1))(inputs)\n",
    "    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n",
    "\n",
    "    model = Model(inputs, final_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Build and fit the model\n",
    "model_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n",
    "model_attention.summary()\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\n",
    "history = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Evaluate the Model\n",
    "# ---------------------------\n",
    "y_pred = model_attention.predict(X_test)\n",
    "\n",
    "# Inverse transform only the Close column\n",
    "# So we need to reconstruct dummy full vectors with Close column at correct index\n",
    "def invert_close_only(pred_scaled, real_scaled, scaler, close_index):\n",
    "    dummy = np.zeros((len(pred_scaled), scaler.n_features_in_))\n",
    "    dummy[:, close_index] = pred_scaled.flatten()\n",
    "    y_pred_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "\n",
    "    dummy[:, close_index] = real_scaled.flatten()\n",
    "    y_test_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "    return y_pred_inv, y_test_inv\n",
    "\n",
    "y_pred_inv, y_test_inv = invert_close_only(y_pred, y_test.reshape(-1, 1), scaler, close_index)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "rmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\n",
    "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\n",
    "evs = explained_variance_score(y_test_inv, y_pred_inv)\n",
    "\n",
    "print(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test_inv, label='Actual')\n",
    "plt.plot(y_pred_inv, label='Predicted')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we try to add another parameter,namely the India VIX\n",
    "* However the VIX index ranges from Only 4th March 2008,so we have to run this model on a subset of the data to check our results\n",
    "* the data we try our model on thus ranges from 04-03-2008 to 30-04-2021 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:40:48.450741Z",
     "iopub.status.busy": "2025-07-01T04:40:48.450524Z",
     "iopub.status.idle": "2025-07-01T04:40:48.532091Z",
     "shell.execute_reply": "2025-07-01T04:40:48.531397Z",
     "shell.execute_reply.started": "2025-07-01T04:40:48.450721Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/kaggle/input/historical-india-stock-market/BSE Sensex 30 Historical Data.csv\")\n",
    "data=data[::-1]\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.head()\n",
    "data.nunique()\n",
    "\n",
    "data.sort_index(axis=1,ascending=True)\n",
    "data.rename(columns={'Price': 'Close'}, inplace=True)\n",
    "df = data.copy()  # Ensure we don't modify the original dataset\n",
    "\n",
    "# Convert financial columns to numeric (remove commas)\n",
    "for col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n",
    "    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n",
    "\n",
    "# Function to convert 'Vol.' column\n",
    "def convert_volume(vol):\n",
    "    if isinstance(vol, str):  # Ensure it's a string before replacing\n",
    "        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n",
    "        if \"B\" in vol:\n",
    "            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n",
    "        elif \"M\" in vol:\n",
    "            return float(vol.replace(\"M\", \"\")) * 1_000_000\n",
    "        elif \"K\" in vol:\n",
    "            return float(vol.replace(\"K\", \"\")) * 1_000\n",
    "    return float(vol)  # Convert directly if already a number\n",
    "\n",
    "df[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n",
    "\n",
    "# Convert 'Change %' column (remove '%' and convert to float)\n",
    "df[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n",
    "\n",
    "# Print final DataFrame\n",
    "print(df.dtypes)\n",
    "print(df)\n",
    "\n",
    "# Assign back to 'data' (if needed)\n",
    "data = df\n",
    "\n",
    "#data = pd.merge(data, vix, on='Date', how='inner')\n",
    "#data.drop(['Date'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:40:48.533551Z",
     "iopub.status.busy": "2025-07-01T04:40:48.533330Z",
     "iopub.status.idle": "2025-07-01T04:40:48.560840Z",
     "shell.execute_reply": "2025-07-01T04:40:48.560223Z",
     "shell.execute_reply.started": "2025-07-01T04:40:48.533526Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vix = pd.read_csv(\"/kaggle/input/vix-data/vix.csv\")\n",
    "vix.drop(\"Vol.\",axis=1,inplace=True)\n",
    "vix[\"Change %\"] = vix[\"Change %\"].str.replace(\"%\", \"\").astype(float)\n",
    "vix.head(),vix.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:40:48.561660Z",
     "iopub.status.busy": "2025-07-01T04:40:48.561475Z",
     "iopub.status.idle": "2025-07-01T04:40:48.591040Z",
     "shell.execute_reply": "2025-07-01T04:40:48.590235Z",
     "shell.execute_reply.started": "2025-07-01T04:40:48.561643Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = pd.merge(data, vix, on='Date', how='inner')\n",
    "data.drop(\"Date\",axis=1,inplace=True)\n",
    "print(data.head())\n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:41:24.270165Z",
     "iopub.status.busy": "2025-07-01T04:41:24.269814Z",
     "iopub.status.idle": "2025-07-01T04:43:12.591840Z",
     "shell.execute_reply": "2025-07-01T04:43:12.590919Z",
     "shell.execute_reply.started": "2025-07-01T04:41:24.270119Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Multiply, Activation\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Set Seed for Reproducibility\n",
    "# ---------------------------\n",
    "def set_random_seed(seed=20):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "set_random_seed(20)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load & Preprocess Data\n",
    "# ---------------------------\n",
    "# Make sure 'data' is already loaded (e.g., from a CSV)\n",
    "df = data.copy()\n",
    "\n",
    "# Scale all features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Get index of the \"Close\" column (target)\n",
    "close_index = df.columns.get_loc(\"Close\")\n",
    "\n",
    "# Create sequences for time series prediction\n",
    "def create_sequences(data, time_steps=15, target_index=0):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i : i + time_steps])                  # shape: (time_steps, features)\n",
    "        y.append(data[i + time_steps, target_index])        # single value: Close price\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences using all features to predict Close\n",
    "X, y = create_sequences(scaled_data, time_steps=15, target_index=close_index)\n",
    "\n",
    "# Train/val/test split\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.1)\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Build Model (unchanged)\n",
    "# ---------------------------\n",
    "def build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n",
    "    inputs = Input(shape=(seq_length, X_train.shape[2]))  # Updated dynamically based on feature count\n",
    "\n",
    "    # Branch 1: 15-day trend\n",
    "    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n",
    "    full_branch = Dense(64, activation='relu')(full_branch)\n",
    "    long_term_return = Dense(1, activation='tanh')(full_branch)\n",
    "    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n",
    "\n",
    "    # Branch 2: last 7 days\n",
    "    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)\n",
    "    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n",
    "    recent_branch = Dense(64, activation='relu')(recent_branch)\n",
    "    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n",
    "    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n",
    "\n",
    "    # Branch 3: last 1 day\n",
    "    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)\n",
    "    very_recent_branch = Flatten()(very_recent_input)\n",
    "    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n",
    "    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n",
    "    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n",
    "\n",
    "    # Gates\n",
    "    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)\n",
    "    gate_1 = Dense(1, activation='sigmoid')(short_term_return)\n",
    "\n",
    "    gated_mid_term_return = Multiply()([mid_term_return, gate_7])\n",
    "    gated_short_term_return = Multiply()([short_term_return, gate_1])\n",
    "\n",
    "    # Combine\n",
    "    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n",
    "    fused_returns = Dense(32, activation='relu')(fused_returns)\n",
    "    final_return = Dense(1, activation='tanh')(fused_returns)\n",
    "    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n",
    "\n",
    "    # Output layer (based on last close price)\n",
    "    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, close_index], axis=-1))(inputs)\n",
    "    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n",
    "\n",
    "    model = Model(inputs, final_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Build and fit the model\n",
    "model_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n",
    "model_attention.summary()\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\n",
    "history = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Evaluate the Model\n",
    "# ---------------------------\n",
    "y_pred = model_attention.predict(X_test)\n",
    "\n",
    "# Inverse transform only the Close column\n",
    "# So we need to reconstruct dummy full vectors with Close column at correct index\n",
    "def invert_close_only(pred_scaled, real_scaled, scaler, close_index):\n",
    "    dummy = np.zeros((len(pred_scaled), scaler.n_features_in_))\n",
    "    dummy[:, close_index] = pred_scaled.flatten()\n",
    "    y_pred_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "\n",
    "    dummy[:, close_index] = real_scaled.flatten()\n",
    "    y_test_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "    return y_pred_inv, y_test_inv\n",
    "\n",
    "y_pred_inv, y_test_inv = invert_close_only(y_pred, y_test.reshape(-1, 1), scaler, close_index)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "rmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\n",
    "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\n",
    "evs = explained_variance_score(y_test_inv, y_pred_inv)\n",
    "\n",
    "print(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test_inv, label='Actual')\n",
    "plt.plot(y_pred_inv, label='Predicted')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Paper 2\n",
    "* This paper does prediction on BSE SENSEX data\n",
    "* Data spans the dates of May 30, 2010, and February 9, 2018\n",
    "* It uses RMSE and MSE metric\n",
    "* Their best result is MSE-0.0021 RMSE-0.0438\n",
    "* Paper link:-https://ieeexplore.ieee.org/document/10397684"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:43:12.593190Z",
     "iopub.status.busy": "2025-07-01T04:43:12.592906Z",
     "iopub.status.idle": "2025-07-01T04:43:12.636797Z",
     "shell.execute_reply": "2025-07-01T04:43:12.635952Z",
     "shell.execute_reply.started": "2025-07-01T04:43:12.593167Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "vix = pd.read_csv(\"/kaggle/input/vix-data/vix.csv\")\n",
    "\n",
    "# Print initial number of rows\n",
    "initial_count = len(vix)\n",
    "print(f\"Initial number of rows: {initial_count}\")\n",
    "\n",
    "# Drop the \"Vol.\" column\n",
    "vix.drop(\"Vol.\", axis=1, inplace=True)\n",
    "\n",
    "# Convert \"Change %\" column to float after removing \"%\"\n",
    "vix[\"Change %\"] = vix[\"Change %\"].str.replace(\"%\", \"\").astype(float)\n",
    "\n",
    "# Convert \"Date\" column to datetime format\n",
    "vix[\"Date\"] = pd.to_datetime(vix[\"Date\"])\n",
    "\n",
    "# Filter data within the specified date range\n",
    "start_date = \"2010-05-30\"\n",
    "end_date = \"2018-02-09\"\n",
    "vix = vix[(vix[\"Date\"] >= start_date) & (vix[\"Date\"] <= end_date)]\n",
    "\n",
    "# Print final number of rows\n",
    "final_count = len(vix)\n",
    "print(f\"Final number of rows after filtering: {final_count}\")\n",
    "\n",
    "# Display results\n",
    "vix.head(), vix.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:43:12.638818Z",
     "iopub.status.busy": "2025-07-01T04:43:12.638581Z",
     "iopub.status.idle": "2025-07-01T04:43:12.702117Z",
     "shell.execute_reply": "2025-07-01T04:43:12.701483Z",
     "shell.execute_reply.started": "2025-07-01T04:43:12.638790Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Sensex dataset\n",
    "data = pd.read_csv(\"/kaggle/input/d/abirc8010/historical-india-stock-market/BSE Sensex 30 Historical Data.csv\")\n",
    "\n",
    "# Print initial number of rows\n",
    "initial_count = len(data)\n",
    "print(f\"Initial number of rows: {initial_count}\")\n",
    "\n",
    "# Reverse and reset index\n",
    "data = data[::-1].reset_index(drop=True)\n",
    "\n",
    "# Sort columns alphabetically and rename 'Price' to 'Close'\n",
    "data.sort_index(axis=1, ascending=True, inplace=True)\n",
    "data.rename(columns={'Price': 'Close'}, inplace=True)\n",
    "\n",
    "# Create a copy to avoid modifying the original data\n",
    "df = data.copy()\n",
    "\n",
    "# Convert financial columns to numeric\n",
    "for col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n",
    "    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n",
    "\n",
    "# Function to convert 'Vol.' column\n",
    "def convert_volume(vol):\n",
    "    if isinstance(vol, str):\n",
    "        vol = vol.replace(\",\", \"\")\n",
    "        if \"B\" in vol:\n",
    "            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n",
    "        elif \"M\" in vol:\n",
    "            return float(vol.replace(\"M\", \"\")) * 1_000_000\n",
    "        elif \"K\" in vol:\n",
    "            return float(vol.replace(\"K\", \"\")) * 1_000\n",
    "    return float(vol)\n",
    "\n",
    "df[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n",
    "\n",
    "# Convert 'Change %' column\n",
    "df[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n",
    "\n",
    "# Convert 'Date' column to datetime format, handling mixed formats\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "# Filter Sensex data within the specified date range\n",
    "start_date = \"2010-05-30\"\n",
    "end_date = \"2018-02-09\"\n",
    "df = df[(df[\"Date\"] >= start_date) & (df[\"Date\"] <= end_date)]\n",
    "\n",
    "# Print final number of rows\n",
    "final_count = len(df)\n",
    "print(f\"Final number of rows after filtering: {final_count}\")\n",
    "\n",
    "# Assign back to 'data'\n",
    "data = df\n",
    "\n",
    "# Merge with VIX dataset\n",
    "data = pd.merge(data, vix, on=\"Date\", how=\"inner\")\n",
    "data.drop([\"Date\"], axis=1, inplace=True)\n",
    "\n",
    "# Print number of rows after merging\n",
    "print(f\"Number of rows after merging: {len(data)}\")\n",
    "\n",
    "# Display results\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:43:12.703635Z",
     "iopub.status.busy": "2025-07-01T04:43:12.703354Z",
     "iopub.status.idle": "2025-07-01T04:43:59.852004Z",
     "shell.execute_reply": "2025-07-01T04:43:59.851241Z",
     "shell.execute_reply.started": "2025-07-01T04:43:12.703611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Multiply, Activation\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Set Seed for Reproducibility\n",
    "# ---------------------------\n",
    "def set_random_seed(seed=20):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "set_random_seed(20)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load & Preprocess Data\n",
    "# ---------------------------\n",
    "# Make sure 'data' is already loaded (e.g., from a CSV)\n",
    "df = data.copy()\n",
    "\n",
    "# Scale all features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Get index of the \"Close\" column (target)\n",
    "close_index = df.columns.get_loc(\"Close\")\n",
    "\n",
    "# Create sequences for time series prediction\n",
    "def create_sequences(data, time_steps=15, target_index=0):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i : i + time_steps])                  # shape: (time_steps, features)\n",
    "        y.append(data[i + time_steps, target_index])        # single value: Close price\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences using all features to predict Close\n",
    "X, y = create_sequences(scaled_data, time_steps=15, target_index=close_index)\n",
    "\n",
    "# Train/val/test split\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.1)\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Build Model (unchanged)\n",
    "# ---------------------------\n",
    "def build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n",
    "    inputs = Input(shape=(seq_length, X_train.shape[2]))  # Updated dynamically based on feature count\n",
    "\n",
    "    # Branch 1: 15-day trend\n",
    "    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n",
    "    full_branch = Dense(64, activation='relu')(full_branch)\n",
    "    long_term_return = Dense(1, activation='tanh')(full_branch)\n",
    "    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n",
    "\n",
    "    # Branch 2: last 7 days\n",
    "    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)\n",
    "    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n",
    "    recent_branch = Dense(64, activation='relu')(recent_branch)\n",
    "    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n",
    "    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n",
    "\n",
    "    # Branch 3: last 1 day\n",
    "    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)\n",
    "    very_recent_branch = Flatten()(very_recent_input)\n",
    "    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n",
    "    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n",
    "    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n",
    "\n",
    "    # Gates\n",
    "    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)\n",
    "    gate_1 = Dense(1, activation='sigmoid')(short_term_return)\n",
    "\n",
    "    gated_mid_term_return = Multiply()([mid_term_return, gate_7])\n",
    "    gated_short_term_return = Multiply()([short_term_return, gate_1])\n",
    "\n",
    "    # Combine\n",
    "    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n",
    "    fused_returns = Dense(32, activation='relu')(fused_returns)\n",
    "    final_return = Dense(1, activation='tanh')(fused_returns)\n",
    "    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n",
    "\n",
    "    # Output layer (based on last close price)\n",
    "    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, close_index], axis=-1))(inputs)\n",
    "    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n",
    "\n",
    "    model = Model(inputs, final_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Build and fit the model\n",
    "model_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n",
    "model_attention.summary()\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\n",
    "history = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Evaluate the Model\n",
    "# ---------------------------\n",
    "y_pred = model_attention.predict(X_test)\n",
    "\n",
    "# Inverse transform only the Close column\n",
    "# So we need to reconstruct dummy full vectors with Close column at correct index\n",
    "def invert_close_only(pred_scaled, real_scaled, scaler, close_index):\n",
    "    dummy = np.zeros((len(pred_scaled), scaler.n_features_in_))\n",
    "    dummy[:, close_index] = pred_scaled.flatten()\n",
    "    y_pred_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "\n",
    "    dummy[:, close_index] = real_scaled.flatten()\n",
    "    y_test_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "    return y_pred_inv, y_test_inv\n",
    "\n",
    "y_pred_inv, y_test_inv = invert_close_only(y_pred, y_test.reshape(-1, 1), scaler, close_index)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "rmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\n",
    "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\n",
    "evs = explained_variance_score(y_test_inv, y_pred_inv)\n",
    "\n",
    "print(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test_inv, label='Actual')\n",
    "plt.plot(y_pred_inv, label='Predicted')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trying without vix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:43:59.853071Z",
     "iopub.status.busy": "2025-07-01T04:43:59.852833Z",
     "iopub.status.idle": "2025-07-01T04:43:59.903240Z",
     "shell.execute_reply": "2025-07-01T04:43:59.902569Z",
     "shell.execute_reply.started": "2025-07-01T04:43:59.853037Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Sensex dataset\n",
    "data = pd.read_csv(\"/kaggle/input/d/abirc8010/historical-india-stock-market/BSE Sensex 30 Historical Data.csv\")\n",
    "\n",
    "# Print initial number of rows\n",
    "initial_count = len(data)\n",
    "print(f\"Initial number of rows: {initial_count}\")\n",
    "\n",
    "# Reverse and reset index\n",
    "data = data[::-1].reset_index(drop=True)\n",
    "\n",
    "# Sort columns alphabetically and rename 'Price' to 'Close'\n",
    "data.sort_index(axis=1, ascending=True, inplace=True)\n",
    "data.rename(columns={'Price': 'Close'}, inplace=True)\n",
    "\n",
    "# Create a copy to avoid modifying the original data\n",
    "df = data.copy()\n",
    "\n",
    "# Convert financial columns to numeric\n",
    "for col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n",
    "    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n",
    "\n",
    "# Function to convert 'Vol.' column\n",
    "def convert_volume(vol):\n",
    "    if isinstance(vol, str):\n",
    "        vol = vol.replace(\",\", \"\")\n",
    "        if \"B\" in vol:\n",
    "            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n",
    "        elif \"M\" in vol:\n",
    "            return float(vol.replace(\"M\", \"\")) * 1_000_000\n",
    "        elif \"K\" in vol:\n",
    "            return float(vol.replace(\"K\", \"\")) * 1_000\n",
    "    return float(vol)\n",
    "\n",
    "df[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n",
    "\n",
    "# Convert 'Change %' column\n",
    "df[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n",
    "\n",
    "# Convert 'Date' column to datetime format, handling mixed formats\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "# Filter Sensex data within the specified date range\n",
    "start_date = \"2010-05-30\"\n",
    "end_date = \"2018-02-09\"\n",
    "df = df[(df[\"Date\"] >= start_date) & (df[\"Date\"] <= end_date)]\n",
    "\n",
    "# Print final number of rows\n",
    "final_count = len(df)\n",
    "print(f\"Final number of rows after filtering: {final_count}\")\n",
    "\n",
    "# Assign back to 'data'\n",
    "data = df\n",
    "\n",
    "# Drop 'Date' column\n",
    "data.drop([\"Date\"], axis=1, inplace=True)\n",
    "\n",
    "# Print final dataset\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:47:44.905604Z",
     "iopub.status.busy": "2025-07-01T04:47:44.905261Z",
     "iopub.status.idle": "2025-07-01T04:49:58.043055Z",
     "shell.execute_reply": "2025-07-01T04:49:58.042320Z",
     "shell.execute_reply.started": "2025-07-01T04:47:44.905580Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Multiply, Activation\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Set Seed for Reproducibility\n",
    "# ---------------------------\n",
    "def set_random_seed(seed=20):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "set_random_seed(20)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load & Preprocess Data\n",
    "# ---------------------------\n",
    "# Make sure 'data' is already loaded (e.g., from a CSV)\n",
    "df = data.copy()\n",
    "\n",
    "# Scale all features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Get index of the \"Close\" column (target)\n",
    "close_index = df.columns.get_loc(\"Close\")\n",
    "\n",
    "# Create sequences for time series prediction\n",
    "def create_sequences(data, time_steps=15, target_index=0):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i : i + time_steps])                  # shape: (time_steps, features)\n",
    "        y.append(data[i + time_steps, target_index])        # single value: Close price\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences using all features to predict Close\n",
    "X, y = create_sequences(scaled_data, time_steps=15, target_index=close_index)\n",
    "\n",
    "# Train/val/test split\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.1)\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Build Model (unchanged)\n",
    "# ---------------------------\n",
    "def build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n",
    "    inputs = Input(shape=(seq_length, X_train.shape[2]))  # Updated dynamically based on feature count\n",
    "\n",
    "    # Branch 1: 15-day trend\n",
    "    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n",
    "    full_branch = Dense(64, activation='relu')(full_branch)\n",
    "    long_term_return = Dense(1, activation='tanh')(full_branch)\n",
    "    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n",
    "\n",
    "    # Branch 2: last 7 days\n",
    "    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)\n",
    "    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n",
    "    recent_branch = Dense(64, activation='relu')(recent_branch)\n",
    "    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n",
    "    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n",
    "\n",
    "    # Branch 3: last 1 day\n",
    "    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)\n",
    "    very_recent_branch = Flatten()(very_recent_input)\n",
    "    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n",
    "    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n",
    "    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n",
    "\n",
    "    # Gates\n",
    "    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)\n",
    "    gate_1 = Dense(1, activation='sigmoid')(short_term_return)\n",
    "\n",
    "    gated_mid_term_return = Multiply()([mid_term_return, gate_7])\n",
    "    gated_short_term_return = Multiply()([short_term_return, gate_1])\n",
    "\n",
    "    # Combine\n",
    "    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n",
    "    fused_returns = Dense(32, activation='relu')(fused_returns)\n",
    "    final_return = Dense(1, activation='tanh')(fused_returns)\n",
    "    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n",
    "\n",
    "    # Output layer (based on last close price)\n",
    "    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, close_index], axis=-1))(inputs)\n",
    "    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n",
    "\n",
    "    model = Model(inputs, final_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Build and fit the model\n",
    "model_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n",
    "model_attention.summary()\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\n",
    "history = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Evaluate the Model\n",
    "# ---------------------------\n",
    "y_pred = model_attention.predict(X_test)\n",
    "\n",
    "# Inverse transform only the Close column\n",
    "# So we need to reconstruct dummy full vectors with Close column at correct index\n",
    "def invert_close_only(pred_scaled, real_scaled, scaler, close_index):\n",
    "    dummy = np.zeros((len(pred_scaled), scaler.n_features_in_))\n",
    "    dummy[:, close_index] = pred_scaled.flatten()\n",
    "    y_pred_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "\n",
    "    dummy[:, close_index] = real_scaled.flatten()\n",
    "    y_test_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "    return y_pred_inv, y_test_inv\n",
    "\n",
    "y_pred_inv, y_test_inv = invert_close_only(y_pred, y_test.reshape(-1, 1), scaler, close_index)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "rmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\n",
    "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\n",
    "evs = explained_variance_score(y_test_inv, y_pred_inv)\n",
    "\n",
    "print(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test_inv, label='Actual')\n",
    "plt.plot(y_pred_inv, label='Predicted')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:53:04.737118Z",
     "iopub.status.busy": "2025-07-01T04:53:04.736772Z",
     "iopub.status.idle": "2025-07-01T04:53:04.748278Z",
     "shell.execute_reply": "2025-07-01T04:53:04.747382Z",
     "shell.execute_reply.started": "2025-07-01T04:53:04.737093Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "mse = mean_squared_error(y_test_inv, y_pred_inv)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "\n",
    "# Calculate the actual range of the test set\n",
    "actual_range = np.max(y_test_inv) - np.min(y_test_inv)\n",
    "\n",
    "# Compute Scaled Metrics\n",
    "mae_scaled = mae / actual_range\n",
    "mse_scaled = mse / (actual_range ** 2)\n",
    "rmse_scaled = rmse / actual_range\n",
    "\n",
    "# Explained Variance Score\n",
    "evs = explained_variance_score(y_test_inv, y_pred_inv) \n",
    "\n",
    "# Print the results\n",
    "print(f\"ğŸ“Š Final Results -\")\n",
    "print(f\"  RÂ²: {r2:.4f}\")\n",
    "print(f\"  MAE: {mae:.4f}, MAE Scaled: {mae_scaled:.4f}\")\n",
    "print(f\"  MSE: {mse:.4f}, MSE Scaled: {mse_scaled:.4f}\")\n",
    "print(f\"  RMSE: {rmse:.4f}, RMSE Scaled: {rmse_scaled:.4f}\")\n",
    "print(f\"  MAPE: {mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100:.2f}%\")\n",
    "print(f\"  EVS: {evs:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Paper 3\n",
    "* This paper does prediction on Nifty50 and Sensex\n",
    "* The data is collected daily from 2013 to 2022\n",
    "* The metric used is RMSE\n",
    "* Their result is NIFTY50:-RMSE - 170.843,SENSEX:-RMSE - 578.746\n",
    "* Paper link:-https://www.sciencedirect.com/science/article/pii/S1568494624005337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:53:04.749792Z",
     "iopub.status.busy": "2025-07-01T04:53:04.749581Z",
     "iopub.status.idle": "2025-07-01T04:53:04.804211Z",
     "shell.execute_reply": "2025-07-01T04:53:04.803421Z",
     "shell.execute_reply.started": "2025-07-01T04:53:04.749775Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vix = pd.read_csv(\"/kaggle/input/personal-dateset-vix/vix paper1.csv\")\n",
    "vix.drop(\"Vol.\",axis=1,inplace=True)\n",
    "vix[\"Change %\"] = vix[\"Change %\"].str.replace(\"%\", \"\").astype(float)\n",
    "vix.head(),vix.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:53:04.805887Z",
     "iopub.status.busy": "2025-07-01T04:53:04.805579Z",
     "iopub.status.idle": "2025-07-01T04:53:04.866430Z",
     "shell.execute_reply": "2025-07-01T04:53:04.865766Z",
     "shell.execute_reply.started": "2025-07-01T04:53:04.805865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/kaggle/input/personal-dataset/Sensex Paper1.csv\")\n",
    "data=data[::-1]\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.head()\n",
    "data.nunique()\n",
    "\n",
    "data.sort_index(axis=1,ascending=True)\n",
    "data.rename(columns={'Price': 'Close'}, inplace=True)\n",
    "df = data.copy()  # Ensure we don't modify the original dataset\n",
    "\n",
    "# Convert financial columns to numeric (remove commas)\n",
    "for col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n",
    "    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n",
    "\n",
    "# Function to convert 'Vol.' column\n",
    "def convert_volume(vol):\n",
    "    if isinstance(vol, str):  # Ensure it's a string before replacing\n",
    "        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n",
    "        if \"B\" in vol:\n",
    "            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n",
    "        elif \"M\" in vol:\n",
    "            return float(vol.replace(\"M\", \"\")) * 1_000_000\n",
    "        elif \"K\" in vol:\n",
    "            return float(vol.replace(\"K\", \"\")) * 1_000\n",
    "    return float(vol)  # Convert directly if already a number\n",
    "\n",
    "df[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n",
    "\n",
    "# Convert 'Change %' column (remove '%' and convert to float)\n",
    "df[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n",
    "\n",
    "# Print final DataFrame\n",
    "print(df.dtypes)\n",
    "print(df)\n",
    "\n",
    "# Assign back to 'data' (if needed)\n",
    "data = df\n",
    "\n",
    "data = pd.merge(data, vix, on='Date', how='inner')\n",
    "data.drop(['Date'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:53:04.886453Z",
     "iopub.status.busy": "2025-07-01T04:53:04.886243Z",
     "iopub.status.idle": "2025-07-01T04:54:42.861061Z",
     "shell.execute_reply": "2025-07-01T04:54:42.860234Z",
     "shell.execute_reply.started": "2025-07-01T04:53:04.886434Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Multiply, Activation\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Set Seed for Reproducibility\n",
    "# ---------------------------\n",
    "def set_random_seed(seed=20):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "set_random_seed(20)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load & Preprocess Data\n",
    "# ---------------------------\n",
    "# Make sure 'data' is already loaded (e.g., from a CSV)\n",
    "df = data.copy()\n",
    "\n",
    "# Scale all features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Get index of the \"Close\" column (target)\n",
    "close_index = df.columns.get_loc(\"Close\")\n",
    "\n",
    "# Create sequences for time series prediction\n",
    "def create_sequences(data, time_steps=15, target_index=0):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i : i + time_steps])                  # shape: (time_steps, features)\n",
    "        y.append(data[i + time_steps, target_index])        # single value: Close price\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences using all features to predict Close\n",
    "X, y = create_sequences(scaled_data, time_steps=15, target_index=close_index)\n",
    "\n",
    "# Train/val/test split\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.1)\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Build Model (unchanged)\n",
    "# ---------------------------\n",
    "def build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n",
    "    inputs = Input(shape=(seq_length, X_train.shape[2]))  # Updated dynamically based on feature count\n",
    "\n",
    "    # Branch 1: 15-day trend\n",
    "    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n",
    "    full_branch = Dense(64, activation='relu')(full_branch)\n",
    "    long_term_return = Dense(1, activation='tanh')(full_branch)\n",
    "    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n",
    "\n",
    "    # Branch 2: last 7 days\n",
    "    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)\n",
    "    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n",
    "    recent_branch = Dense(64, activation='relu')(recent_branch)\n",
    "    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n",
    "    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n",
    "\n",
    "    # Branch 3: last 1 day\n",
    "    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)\n",
    "    very_recent_branch = Flatten()(very_recent_input)\n",
    "    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n",
    "    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n",
    "    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n",
    "\n",
    "    # Gates\n",
    "    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)\n",
    "    gate_1 = Dense(1, activation='sigmoid')(short_term_return)\n",
    "\n",
    "    gated_mid_term_return = Multiply()([mid_term_return, gate_7])\n",
    "    gated_short_term_return = Multiply()([short_term_return, gate_1])\n",
    "\n",
    "    # Combine\n",
    "    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n",
    "    fused_returns = Dense(32, activation='relu')(fused_returns)\n",
    "    final_return = Dense(1, activation='tanh')(fused_returns)\n",
    "    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n",
    "\n",
    "    # Output layer (based on last close price)\n",
    "    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, close_index], axis=-1))(inputs)\n",
    "    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n",
    "\n",
    "    model = Model(inputs, final_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Build and fit the model\n",
    "model_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n",
    "model_attention.summary()\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\n",
    "history = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Evaluate the Model\n",
    "# ---------------------------\n",
    "y_pred = model_attention.predict(X_test)\n",
    "\n",
    "# Inverse transform only the Close column\n",
    "# So we need to reconstruct dummy full vectors with Close column at correct index\n",
    "def invert_close_only(pred_scaled, real_scaled, scaler, close_index):\n",
    "    dummy = np.zeros((len(pred_scaled), scaler.n_features_in_))\n",
    "    dummy[:, close_index] = pred_scaled.flatten()\n",
    "    y_pred_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "\n",
    "    dummy[:, close_index] = real_scaled.flatten()\n",
    "    y_test_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "    return y_pred_inv, y_test_inv\n",
    "\n",
    "y_pred_inv, y_test_inv = invert_close_only(y_pred, y_test.reshape(-1, 1), scaler, close_index)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "rmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\n",
    "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\n",
    "evs = explained_variance_score(y_test_inv, y_pred_inv)\n",
    "\n",
    "print(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test_inv, label='Actual')\n",
    "plt.plot(y_pred_inv, label='Predicted')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doing without vix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:54:42.862424Z",
     "iopub.status.busy": "2025-07-01T04:54:42.862185Z",
     "iopub.status.idle": "2025-07-01T04:54:42.909872Z",
     "shell.execute_reply": "2025-07-01T04:54:42.909226Z",
     "shell.execute_reply.started": "2025-07-01T04:54:42.862403Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/kaggle/input/personal-dataset/Sensex Paper1.csv\")\n",
    "data=data[::-1]\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.head()\n",
    "data.nunique()\n",
    "\n",
    "data.sort_index(axis=1,ascending=True)\n",
    "data.rename(columns={'Price': 'Close'}, inplace=True)\n",
    "df = data.copy()  # Ensure we don't modify the original dataset\n",
    "\n",
    "# Convert financial columns to numeric (remove commas)\n",
    "for col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n",
    "    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n",
    "\n",
    "# Function to convert 'Vol.' column\n",
    "def convert_volume(vol):\n",
    "    if isinstance(vol, str):  # Ensure it's a string before replacing\n",
    "        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n",
    "        if \"B\" in vol:\n",
    "            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n",
    "        elif \"M\" in vol:\n",
    "            return float(vol.replace(\"M\", \"\")) * 1_000_000\n",
    "        elif \"K\" in vol:\n",
    "            return float(vol.replace(\"K\", \"\")) * 1_000\n",
    "    return float(vol)  # Convert directly if already a number\n",
    "\n",
    "df[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n",
    "\n",
    "# Convert 'Change %' column (remove '%' and convert to float)\n",
    "df[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n",
    "\n",
    "# Print final DataFrame\n",
    "print(df.dtypes)\n",
    "print(df)\n",
    "\n",
    "# Assign back to 'data' (if needed)\n",
    "data = df\n",
    "\n",
    "#data = pd.merge(data, vix, on='Date', how='inner')\n",
    "data.drop(['Date'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:54:42.911486Z",
     "iopub.status.busy": "2025-07-01T04:54:42.911288Z",
     "iopub.status.idle": "2025-07-01T04:55:45.233381Z",
     "shell.execute_reply": "2025-07-01T04:55:45.232478Z",
     "shell.execute_reply.started": "2025-07-01T04:54:42.911469Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Multiply, Activation\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Set Seed for Reproducibility\n",
    "# ---------------------------\n",
    "def set_random_seed(seed=20):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "set_random_seed(20)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load & Preprocess Data\n",
    "# ---------------------------\n",
    "# Make sure 'data' is already loaded (e.g., from a CSV)\n",
    "df = data.copy()\n",
    "\n",
    "# Scale all features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Get index of the \"Close\" column (target)\n",
    "close_index = df.columns.get_loc(\"Close\")\n",
    "\n",
    "# Create sequences for time series prediction\n",
    "def create_sequences(data, time_steps=15, target_index=0):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i : i + time_steps])                  # shape: (time_steps, features)\n",
    "        y.append(data[i + time_steps, target_index])        # single value: Close price\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences using all features to predict Close\n",
    "X, y = create_sequences(scaled_data, time_steps=15, target_index=close_index)\n",
    "\n",
    "# Train/val/test split\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.1)\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Build Model (unchanged)\n",
    "# ---------------------------\n",
    "def build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n",
    "    inputs = Input(shape=(seq_length, X_train.shape[2]))  # Updated dynamically based on feature count\n",
    "\n",
    "    # Branch 1: 15-day trend\n",
    "    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n",
    "    full_branch = Dense(64, activation='relu')(full_branch)\n",
    "    long_term_return = Dense(1, activation='tanh')(full_branch)\n",
    "    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n",
    "\n",
    "    # Branch 2: last 7 days\n",
    "    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)\n",
    "    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n",
    "    recent_branch = Dense(64, activation='relu')(recent_branch)\n",
    "    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n",
    "    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n",
    "\n",
    "    # Branch 3: last 1 day\n",
    "    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)\n",
    "    very_recent_branch = Flatten()(very_recent_input)\n",
    "    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n",
    "    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n",
    "    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n",
    "\n",
    "    # Gates\n",
    "    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)\n",
    "    gate_1 = Dense(1, activation='sigmoid')(short_term_return)\n",
    "\n",
    "    gated_mid_term_return = Multiply()([mid_term_return, gate_7])\n",
    "    gated_short_term_return = Multiply()([short_term_return, gate_1])\n",
    "\n",
    "    # Combine\n",
    "    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n",
    "    fused_returns = Dense(32, activation='relu')(fused_returns)\n",
    "    final_return = Dense(1, activation='tanh')(fused_returns)\n",
    "    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n",
    "\n",
    "    # Output layer (based on last close price)\n",
    "    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, close_index], axis=-1))(inputs)\n",
    "    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n",
    "\n",
    "    model = Model(inputs, final_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Build and fit the model\n",
    "model_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n",
    "model_attention.summary()\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\n",
    "history = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Evaluate the Model\n",
    "# ---------------------------\n",
    "y_pred = model_attention.predict(X_test)\n",
    "\n",
    "# Inverse transform only the Close column\n",
    "# So we need to reconstruct dummy full vectors with Close column at correct index\n",
    "def invert_close_only(pred_scaled, real_scaled, scaler, close_index):\n",
    "    dummy = np.zeros((len(pred_scaled), scaler.n_features_in_))\n",
    "    dummy[:, close_index] = pred_scaled.flatten()\n",
    "    y_pred_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "\n",
    "    dummy[:, close_index] = real_scaled.flatten()\n",
    "    y_test_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "    return y_pred_inv, y_test_inv\n",
    "\n",
    "y_pred_inv, y_test_inv = invert_close_only(y_pred, y_test.reshape(-1, 1), scaler, close_index)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "rmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\n",
    "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\n",
    "evs = explained_variance_score(y_test_inv, y_pred_inv)\n",
    "\n",
    "print(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test_inv, label='Actual')\n",
    "plt.plot(y_pred_inv, label='Predicted')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Nifty50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:55:45.235050Z",
     "iopub.status.busy": "2025-07-01T04:55:45.234817Z",
     "iopub.status.idle": "2025-07-01T04:55:45.305857Z",
     "shell.execute_reply": "2025-07-01T04:55:45.305228Z",
     "shell.execute_reply.started": "2025-07-01T04:55:45.235030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/kaggle/input/personal-dataset/Nifty50 Paper1.csv\")\n",
    "data=data[::-1]\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.head()\n",
    "data.nunique()\n",
    "\n",
    "data.sort_index(axis=1,ascending=True)\n",
    "data.rename(columns={'Price': 'Close'}, inplace=True)\n",
    "df = data.copy()  # Ensure we don't modify the original dataset\n",
    "vix = pd.read_csv(\"/kaggle/input/personal-dateset-vix/vix paper1.csv\")\n",
    "vix.drop(\"Vol.\",axis=1,inplace=True)\n",
    "vix[\"Change %\"] = vix[\"Change %\"].str.replace(\"%\", \"\").astype(float)\n",
    "vix.head(),vix.dtypes\n",
    "# Convert financial columns to numeric (remove commas)\n",
    "for col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n",
    "    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n",
    "\n",
    "# Function to convert 'Vol.' column\n",
    "def convert_volume(vol):\n",
    "    if isinstance(vol, str):  # Ensure it's a string before replacing\n",
    "        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n",
    "        if \"B\" in vol:\n",
    "            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n",
    "        elif \"M\" in vol:\n",
    "            return float(vol.replace(\"M\", \"\")) * 1_000_000\n",
    "        elif \"K\" in vol:\n",
    "            return float(vol.replace(\"K\", \"\")) * 1_000\n",
    "    return float(vol)  # Convert directly if already a number\n",
    "\n",
    "df[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n",
    "\n",
    "# Convert 'Change %' column (remove '%' and convert to float)\n",
    "df[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n",
    "\n",
    "# Print final DataFrame\n",
    "print(df.dtypes)\n",
    "print(df)\n",
    "\n",
    "# Assign back to 'data' (if needed)\n",
    "data = df\n",
    "\n",
    "data = pd.merge(data, vix, on='Date', how='inner')\n",
    "data.drop(['Date'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:59:24.221715Z",
     "iopub.status.busy": "2025-07-01T04:59:24.221366Z",
     "iopub.status.idle": "2025-07-01T04:59:24.227396Z",
     "shell.execute_reply": "2025-07-01T04:59:24.226588Z",
     "shell.execute_reply.started": "2025-07-01T04:59:24.221685Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:59:26.138498Z",
     "iopub.status.busy": "2025-07-01T04:59:26.138206Z",
     "iopub.status.idle": "2025-07-01T05:00:16.272250Z",
     "shell.execute_reply": "2025-07-01T05:00:16.271407Z",
     "shell.execute_reply.started": "2025-07-01T04:59:26.138475Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Multiply, Activation\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Set Seed for Reproducibility\n",
    "# ---------------------------\n",
    "def set_random_seed(seed=20):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "set_random_seed(20)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load & Preprocess Data\n",
    "# ---------------------------\n",
    "# Make sure 'data' is already loaded (e.g., from a CSV)\n",
    "df = data.copy()\n",
    "\n",
    "# Scale all features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Get index of the \"Close\" column (target)\n",
    "close_index = df.columns.get_loc(\"Close\")\n",
    "\n",
    "# Create sequences for time series prediction\n",
    "def create_sequences(data, time_steps=15, target_index=0):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i : i + time_steps])                  # shape: (time_steps, features)\n",
    "        y.append(data[i + time_steps, target_index])        # single value: Close price\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences using all features to predict Close\n",
    "X, y = create_sequences(scaled_data, time_steps=15, target_index=close_index)\n",
    "\n",
    "# Train/val/test split\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.1)\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Build Model (unchanged)\n",
    "# ---------------------------\n",
    "def build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n",
    "    inputs = Input(shape=(seq_length, X_train.shape[2]))  # Updated dynamically based on feature count\n",
    "\n",
    "    # Branch 1: 15-day trend\n",
    "    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n",
    "    full_branch = Dense(64, activation='relu')(full_branch)\n",
    "    long_term_return = Dense(1, activation='tanh')(full_branch)\n",
    "    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n",
    "\n",
    "    # Branch 2: last 7 days\n",
    "    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)\n",
    "    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n",
    "    recent_branch = Dense(64, activation='relu')(recent_branch)\n",
    "    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n",
    "    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n",
    "\n",
    "    # Branch 3: last 1 day\n",
    "    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)\n",
    "    very_recent_branch = Flatten()(very_recent_input)\n",
    "    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n",
    "    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n",
    "    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n",
    "\n",
    "    # Gates\n",
    "    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)\n",
    "    gate_1 = Dense(1, activation='sigmoid')(short_term_return)\n",
    "\n",
    "    gated_mid_term_return = Multiply()([mid_term_return, gate_7])\n",
    "    gated_short_term_return = Multiply()([short_term_return, gate_1])\n",
    "\n",
    "    # Combine\n",
    "    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n",
    "    fused_returns = Dense(32, activation='relu')(fused_returns)\n",
    "    final_return = Dense(1, activation='tanh')(fused_returns)\n",
    "    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n",
    "\n",
    "    # Output layer (based on last close price)\n",
    "    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, close_index], axis=-1))(inputs)\n",
    "    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n",
    "\n",
    "    model = Model(inputs, final_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Build and fit the model\n",
    "model_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n",
    "model_attention.summary()\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\n",
    "history = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Evaluate the Model\n",
    "# ---------------------------\n",
    "y_pred = model_attention.predict(X_test)\n",
    "\n",
    "# Inverse transform only the Close column\n",
    "# So we need to reconstruct dummy full vectors with Close column at correct index\n",
    "def invert_close_only(pred_scaled, real_scaled, scaler, close_index):\n",
    "    dummy = np.zeros((len(pred_scaled), scaler.n_features_in_))\n",
    "    dummy[:, close_index] = pred_scaled.flatten()\n",
    "    y_pred_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "\n",
    "    dummy[:, close_index] = real_scaled.flatten()\n",
    "    y_test_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "    return y_pred_inv, y_test_inv\n",
    "\n",
    "y_pred_inv, y_test_inv = invert_close_only(y_pred, y_test.reshape(-1, 1), scaler, close_index)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "rmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\n",
    "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\n",
    "evs = explained_variance_score(y_test_inv, y_pred_inv)\n",
    "\n",
    "print(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test_inv, label='Actual')\n",
    "plt.plot(y_pred_inv, label='Predicted')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doing without vix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:02:12.691087Z",
     "iopub.status.busy": "2025-07-01T05:02:12.690785Z",
     "iopub.status.idle": "2025-07-01T05:02:12.746829Z",
     "shell.execute_reply": "2025-07-01T05:02:12.745988Z",
     "shell.execute_reply.started": "2025-07-01T05:02:12.691064Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/kaggle/input/personal-dataset/Nifty50 Paper1.csv\")\n",
    "data=data[::-1]\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.head()\n",
    "data.nunique()\n",
    "\n",
    "data.sort_index(axis=1,ascending=True)\n",
    "data.rename(columns={'Price': 'Close'}, inplace=True)\n",
    "df = data.copy()  # Ensure we don't modify the original dataset\n",
    "\n",
    "# Convert financial columns to numeric (remove commas)\n",
    "for col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n",
    "    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n",
    "\n",
    "# Function to convert 'Vol.' column\n",
    "def convert_volume(vol):\n",
    "    if isinstance(vol, str):  # Ensure it's a string before replacing\n",
    "        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n",
    "        if \"B\" in vol:\n",
    "            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n",
    "        elif \"M\" in vol:\n",
    "            return float(vol.replace(\"M\", \"\")) * 1_000_000\n",
    "        elif \"K\" in vol:\n",
    "            return float(vol.replace(\"K\", \"\")) * 1_000\n",
    "    return float(vol)  # Convert directly if already a number\n",
    "\n",
    "df[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n",
    "\n",
    "# Convert 'Change %' column (remove '%' and convert to float)\n",
    "df[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n",
    "\n",
    "# Print final DataFrame\n",
    "print(df.dtypes)\n",
    "print(df)\n",
    "\n",
    "# Assign back to 'data' (if needed)\n",
    "data = df\n",
    "\n",
    "#data = pd.merge(data, vix, on='Date', how='inner')\n",
    "data.drop(['Date'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:05:32.451216Z",
     "iopub.status.busy": "2025-07-01T05:05:32.450853Z",
     "iopub.status.idle": "2025-07-01T05:06:32.392832Z",
     "shell.execute_reply": "2025-07-01T05:06:32.392068Z",
     "shell.execute_reply.started": "2025-07-01T05:05:32.451178Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Multiply, Activation\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Set Seed for Reproducibility\n",
    "# ---------------------------\n",
    "def set_random_seed(seed=20):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "set_random_seed(20)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load & Preprocess Data\n",
    "# ---------------------------\n",
    "# Make sure 'data' is already loaded (e.g., from a CSV)\n",
    "data.dropna(axis=0,inplace=True)\n",
    "df = data.copy()\n",
    "\n",
    "# Scale all features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Get index of the \"Close\" column (target)\n",
    "close_index = df.columns.get_loc(\"Close\")\n",
    "\n",
    "# Create sequences for time series prediction\n",
    "def create_sequences(data, time_steps=15, target_index=0):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i : i + time_steps])                  # shape: (time_steps, features)\n",
    "        y.append(data[i + time_steps, target_index])        # single value: Close price\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences using all features to predict Close\n",
    "X, y = create_sequences(scaled_data, time_steps=15, target_index=close_index)\n",
    "\n",
    "# Train/val/test split\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.1)\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Build Model (unchanged)\n",
    "# ---------------------------\n",
    "def build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n",
    "    inputs = Input(shape=(seq_length, X_train.shape[2]))  # Updated dynamically based on feature count\n",
    "\n",
    "    # Branch 1: 15-day trend\n",
    "    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n",
    "    full_branch = Dense(64, activation='relu')(full_branch)\n",
    "    long_term_return = Dense(1, activation='tanh')(full_branch)\n",
    "    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n",
    "\n",
    "    # Branch 2: last 7 days\n",
    "    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)\n",
    "    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n",
    "    recent_branch = Dense(64, activation='relu')(recent_branch)\n",
    "    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n",
    "    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n",
    "\n",
    "    # Branch 3: last 1 day\n",
    "    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)\n",
    "    very_recent_branch = Flatten()(very_recent_input)\n",
    "    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n",
    "    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n",
    "    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n",
    "\n",
    "    # Gates\n",
    "    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)\n",
    "    gate_1 = Dense(1, activation='sigmoid')(short_term_return)\n",
    "\n",
    "    gated_mid_term_return = Multiply()([mid_term_return, gate_7])\n",
    "    gated_short_term_return = Multiply()([short_term_return, gate_1])\n",
    "\n",
    "    # Combine\n",
    "    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n",
    "    fused_returns = Dense(32, activation='relu')(fused_returns)\n",
    "    final_return = Dense(1, activation='tanh')(fused_returns)\n",
    "    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n",
    "\n",
    "    # Output layer (based on last close price)\n",
    "    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, close_index], axis=-1))(inputs)\n",
    "    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n",
    "\n",
    "    model = Model(inputs, final_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Build and fit the model\n",
    "model_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n",
    "model_attention.summary()\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\n",
    "history = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Evaluate the Model\n",
    "# ---------------------------\n",
    "y_pred = model_attention.predict(X_test)\n",
    "\n",
    "# Inverse transform only the Close column\n",
    "# So we need to reconstruct dummy full vectors with Close column at correct index\n",
    "def invert_close_only(pred_scaled, real_scaled, scaler, close_index):\n",
    "    dummy = np.zeros((len(pred_scaled), scaler.n_features_in_))\n",
    "    dummy[:, close_index] = pred_scaled.flatten()\n",
    "    y_pred_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "\n",
    "    dummy[:, close_index] = real_scaled.flatten()\n",
    "    y_test_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "    return y_pred_inv, y_test_inv\n",
    "\n",
    "y_pred_inv, y_test_inv = invert_close_only(y_pred, y_test.reshape(-1, 1), scaler, close_index)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "rmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\n",
    "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\n",
    "evs = explained_variance_score(y_test_inv, y_pred_inv)\n",
    "\n",
    "print(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test_inv, label='Actual')\n",
    "plt.plot(y_pred_inv, label='Predicted')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Paper 4\n",
    "* This paper does prediction on Nifty50\n",
    "* The metric used is RMSE\n",
    "* Their result is RMSE - 171.4\n",
    "* We are training the model on data from 1st January 2020 till 31st December 2023 and testing it on data from 1st January\n",
    "  2024 till 15th March 2024.\n",
    "* Paper link:-https://www.semanticscholar.org/paper/Closing-Price-Prediction-for-the-NIFTY-50-Index%3A-A-Singh-Shah/d0eca144e2cd8e86c57eca34e3d9f4943f3c45f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:06:32.394167Z",
     "iopub.status.busy": "2025-07-01T05:06:32.393883Z",
     "iopub.status.idle": "2025-07-01T05:06:32.420061Z",
     "shell.execute_reply": "2025-07-01T05:06:32.419369Z",
     "shell.execute_reply.started": "2025-07-01T05:06:32.394126Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"/kaggle/input/personal-dataset/Nifty 50 Train paper 2.csv\")\n",
    "data2=pd.read_csv(\"/kaggle/input/personal-dataset/Nifty 50 Test paper2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:06:32.422065Z",
     "iopub.status.busy": "2025-07-01T05:06:32.421858Z",
     "iopub.status.idle": "2025-07-01T05:06:32.431134Z",
     "shell.execute_reply": "2025-07-01T05:06:32.430398Z",
     "shell.execute_reply.started": "2025-07-01T05:06:32.422046Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data.head(),data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:06:32.432483Z",
     "iopub.status.busy": "2025-07-01T05:06:32.432301Z",
     "iopub.status.idle": "2025-07-01T05:06:32.471928Z",
     "shell.execute_reply": "2025-07-01T05:06:32.471225Z",
     "shell.execute_reply.started": "2025-07-01T05:06:32.432466Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data=data[::-1]\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    print(data.head())\n",
    "    data.nunique()\n",
    "\n",
    "    data.sort_index(axis=1,ascending=True)\n",
    "    df=data\n",
    "    print(df)\n",
    "    data.rename(columns={'Price': 'Close'}, inplace=True)\n",
    "    print(data.head()),print(data.dtypes)\n",
    "    df = data.copy()  # Ensure we don't modify the original dataset\n",
    "\n",
    "# Convert financial columns to numeric (remove commas)\n",
    "    for col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n",
    "        df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n",
    "\n",
    "# Function to convert 'Vol.' column\n",
    "    def convert_volume(vol):\n",
    "        if isinstance(vol, str):  # Ensure it's a string before replacing\n",
    "            vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n",
    "            if \"B\" in vol:\n",
    "                return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n",
    "            elif \"M\" in vol:\n",
    "                return float(vol.replace(\"M\", \"\")) * 1_000_000\n",
    "            elif \"K\" in vol:\n",
    "                return float(vol.replace(\"K\", \"\")) * 1_000\n",
    "        return float(vol)  # Convert directly if already a number\n",
    "\n",
    "    df[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n",
    "\n",
    "# Convert 'Change %' column (remove '%' and convert to float)\n",
    "    df[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n",
    "\n",
    "# Print final DataFrame\n",
    "    print(df.dtypes)\n",
    "    print(df)\n",
    "\n",
    "# Assign back to 'data' (if needed)\n",
    "    data = df\n",
    "    return data\n",
    "    #data=df\n",
    "#data.drop(\"Date\",axis=1,inplace=True)\n",
    "\n",
    "data=preprocess(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:06:32.472973Z",
     "iopub.status.busy": "2025-07-01T05:06:32.472746Z",
     "iopub.status.idle": "2025-07-01T05:06:32.508868Z",
     "shell.execute_reply": "2025-07-01T05:06:32.508075Z",
     "shell.execute_reply.started": "2025-07-01T05:06:32.472955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data2=preprocess(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:06:32.510049Z",
     "iopub.status.busy": "2025-07-01T05:06:32.509791Z",
     "iopub.status.idle": "2025-07-01T05:06:32.532215Z",
     "shell.execute_reply": "2025-07-01T05:06:32.531448Z",
     "shell.execute_reply.started": "2025-07-01T05:06:32.510025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vix = pd.read_csv(\"/kaggle/input/vix-paper2/vix paper 2.csv\")\n",
    "vix.drop(\"Vol.\",axis=1,inplace=True)\n",
    "vix[\"Change %\"] = vix[\"Change %\"].str.replace(\"%\", \"\").astype(float)\n",
    "vix.head(),vix.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:06:32.533288Z",
     "iopub.status.busy": "2025-07-01T05:06:32.532996Z",
     "iopub.status.idle": "2025-07-01T05:06:32.546199Z",
     "shell.execute_reply": "2025-07-01T05:06:32.545560Z",
     "shell.execute_reply.started": "2025-07-01T05:06:32.533259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = pd.merge(data, vix, on='Date', how='inner')\n",
    "data.drop(\"Date\",axis=1,inplace=True)\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:06:32.547295Z",
     "iopub.status.busy": "2025-07-01T05:06:32.546962Z",
     "iopub.status.idle": "2025-07-01T05:06:32.564335Z",
     "shell.execute_reply": "2025-07-01T05:06:32.563596Z",
     "shell.execute_reply.started": "2025-07-01T05:06:32.547266Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data2 = pd.merge(data2, vix, on='Date', how='inner')\n",
    "data2.drop(\"Date\",axis=1,inplace=True)\n",
    "print(data2.head())\n",
    "print(len(data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:06:32.566960Z",
     "iopub.status.busy": "2025-07-01T05:06:32.566736Z",
     "iopub.status.idle": "2025-07-01T05:07:19.843173Z",
     "shell.execute_reply": "2025-07-01T05:07:19.842390Z",
     "shell.execute_reply.started": "2025-07-01T05:06:32.566941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Multiply, Activation\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Set Seed for Reproducibility\n",
    "# ---------------------------\n",
    "def set_random_seed(seed=20):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "set_random_seed(20)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load & Preprocess Data\n",
    "# ---------------------------\n",
    "# Make sure 'data' is already loaded (e.g., from a CSV)\n",
    "data.dropna(axis=0,inplace=True)\n",
    "df = data.copy()\n",
    "\n",
    "# Scale all features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Get index of the \"Close\" column (target)\n",
    "close_index = df.columns.get_loc(\"Close\")\n",
    "\n",
    "# Create sequences for time series prediction\n",
    "def create_sequences(data, time_steps=15, target_index=0):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i : i + time_steps])                  # shape: (time_steps, features)\n",
    "        y.append(data[i + time_steps, target_index])        # single value: Close price\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences using all features to predict Close\n",
    "X, y = create_sequences(scaled_data, time_steps=15, target_index=close_index)\n",
    "\n",
    "# Train/val/test split\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.1)\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Build Model (unchanged)\n",
    "# ---------------------------\n",
    "def build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n",
    "    inputs = Input(shape=(seq_length, X_train.shape[2]))  # Updated dynamically based on feature count\n",
    "\n",
    "    # Branch 1: 15-day trend\n",
    "    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n",
    "    full_branch = Dense(64, activation='relu')(full_branch)\n",
    "    long_term_return = Dense(1, activation='tanh')(full_branch)\n",
    "    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n",
    "\n",
    "    # Branch 2: last 7 days\n",
    "    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)\n",
    "    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n",
    "    recent_branch = Dense(64, activation='relu')(recent_branch)\n",
    "    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n",
    "    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n",
    "\n",
    "    # Branch 3: last 1 day\n",
    "    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)\n",
    "    very_recent_branch = Flatten()(very_recent_input)\n",
    "    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n",
    "    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n",
    "    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n",
    "\n",
    "    # Gates\n",
    "    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)\n",
    "    gate_1 = Dense(1, activation='sigmoid')(short_term_return)\n",
    "\n",
    "    gated_mid_term_return = Multiply()([mid_term_return, gate_7])\n",
    "    gated_short_term_return = Multiply()([short_term_return, gate_1])\n",
    "\n",
    "    # Combine\n",
    "    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n",
    "    fused_returns = Dense(32, activation='relu')(fused_returns)\n",
    "    final_return = Dense(1, activation='tanh')(fused_returns)\n",
    "    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n",
    "\n",
    "    # Output layer (based on last close price)\n",
    "    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, close_index], axis=-1))(inputs)\n",
    "    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n",
    "\n",
    "    model = Model(inputs, final_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Build and fit the model\n",
    "model_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n",
    "model_attention.summary()\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\n",
    "history = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Evaluate the Model\n",
    "# ---------------------------\n",
    "y_pred = model_attention.predict(X_test)\n",
    "\n",
    "# Inverse transform only the Close column\n",
    "# So we need to reconstruct dummy full vectors with Close column at correct index\n",
    "def invert_close_only(pred_scaled, real_scaled, scaler, close_index):\n",
    "    dummy = np.zeros((len(pred_scaled), scaler.n_features_in_))\n",
    "    dummy[:, close_index] = pred_scaled.flatten()\n",
    "    y_pred_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "\n",
    "    dummy[:, close_index] = real_scaled.flatten()\n",
    "    y_test_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "    return y_pred_inv, y_test_inv\n",
    "\n",
    "y_pred_inv, y_test_inv = invert_close_only(y_pred, y_test.reshape(-1, 1), scaler, close_index)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "rmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\n",
    "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\n",
    "evs = explained_variance_score(y_test_inv, y_pred_inv)\n",
    "\n",
    "print(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test_inv, label='Actual')\n",
    "plt.plot(y_pred_inv, label='Predicted')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doing without vix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:07:19.844824Z",
     "iopub.status.busy": "2025-07-01T05:07:19.844607Z",
     "iopub.status.idle": "2025-07-01T05:07:19.901913Z",
     "shell.execute_reply": "2025-07-01T05:07:19.900993Z",
     "shell.execute_reply.started": "2025-07-01T05:07:19.844798Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/kaggle/input/personal-dataset/Nifty 50 Train paper 2.csv\")\n",
    "data2=pd.read_csv(\"/kaggle/input/personal-dataset/Nifty 50 Test paper2.csv\")\n",
    "data=preprocess(data)\n",
    "data2=preprocess(data2)\n",
    "data2.drop(\"Date\",axis=1,inplace=True)\n",
    "data.drop(\"Date\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:09:37.414760Z",
     "iopub.status.busy": "2025-07-01T05:09:37.414428Z",
     "iopub.status.idle": "2025-07-01T05:10:03.460939Z",
     "shell.execute_reply": "2025-07-01T05:10:03.460106Z",
     "shell.execute_reply.started": "2025-07-01T05:09:37.414732Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Multiply, Activation\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Set Seed for Reproducibility\n",
    "# ---------------------------\n",
    "def set_random_seed(seed=20):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "set_random_seed(20)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load & Preprocess Data\n",
    "# ---------------------------\n",
    "# Make sure 'data' is already loaded (e.g., from a CSV)\n",
    "data.dropna(axis=0,inplace=True)\n",
    "df = data.copy()\n",
    "\n",
    "# Scale all features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Get index of the \"Close\" column (target)\n",
    "close_index = df.columns.get_loc(\"Close\")\n",
    "\n",
    "# Create sequences for time series prediction\n",
    "def create_sequences(data, time_steps=15, target_index=0):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i : i + time_steps])                  # shape: (time_steps, features)\n",
    "        y.append(data[i + time_steps, target_index])        # single value: Close price\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences using all features to predict Close\n",
    "X, y = create_sequences(scaled_data, time_steps=15, target_index=close_index)\n",
    "\n",
    "# Train/val/test split\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.1)\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Build Model (unchanged)\n",
    "# ---------------------------\n",
    "def build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n",
    "    inputs = Input(shape=(seq_length, X_train.shape[2]))  # Updated dynamically based on feature count\n",
    "\n",
    "    # Branch 1: 15-day trend\n",
    "    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n",
    "    full_branch = Dense(64, activation='relu')(full_branch)\n",
    "    long_term_return = Dense(1, activation='tanh')(full_branch)\n",
    "    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n",
    "\n",
    "    # Branch 2: last 7 days\n",
    "    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)\n",
    "    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n",
    "    recent_branch = Dense(64, activation='relu')(recent_branch)\n",
    "    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n",
    "    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n",
    "\n",
    "    # Branch 3: last 1 day\n",
    "    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)\n",
    "    very_recent_branch = Flatten()(very_recent_input)\n",
    "    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n",
    "    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n",
    "    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n",
    "\n",
    "    # Gates\n",
    "    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)\n",
    "    gate_1 = Dense(1, activation='sigmoid')(short_term_return)\n",
    "\n",
    "    gated_mid_term_return = Multiply()([mid_term_return, gate_7])\n",
    "    gated_short_term_return = Multiply()([short_term_return, gate_1])\n",
    "\n",
    "    # Combine\n",
    "    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n",
    "    fused_returns = Dense(32, activation='relu')(fused_returns)\n",
    "    final_return = Dense(1, activation='tanh')(fused_returns)\n",
    "    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n",
    "\n",
    "    # Output layer (based on last close price)\n",
    "    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, close_index], axis=-1))(inputs)\n",
    "    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n",
    "\n",
    "    model = Model(inputs, final_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Build and fit the model\n",
    "model_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n",
    "model_attention.summary()\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\n",
    "history = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Evaluate the Model\n",
    "# ---------------------------\n",
    "y_pred = model_attention.predict(X_test)\n",
    "\n",
    "# Inverse transform only the Close column\n",
    "# So we need to reconstruct dummy full vectors with Close column at correct index\n",
    "def invert_close_only(pred_scaled, real_scaled, scaler, close_index):\n",
    "    dummy = np.zeros((len(pred_scaled), scaler.n_features_in_))\n",
    "    dummy[:, close_index] = pred_scaled.flatten()\n",
    "    y_pred_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "\n",
    "    dummy[:, close_index] = real_scaled.flatten()\n",
    "    y_test_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "    return y_pred_inv, y_test_inv\n",
    "\n",
    "y_pred_inv, y_test_inv = invert_close_only(y_pred, y_test.reshape(-1, 1), scaler, close_index)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "rmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\n",
    "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\n",
    "evs = explained_variance_score(y_test_inv, y_pred_inv)\n",
    "\n",
    "print(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test_inv, label='Actual')\n",
    "plt.plot(y_pred_inv, label='Predicted')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Paper 5\n",
    "* This paper does prediction on Nifty50 and BSE Sensex\n",
    "* The ranges are:-\n",
    "* ## BSE( Data )\n",
    " 24/02/2015-27/11/2022\n",
    "\n",
    "* ## NSE(Data)\n",
    " 27/10/2006-27/11/2022\n",
    "* The metric used are RMSE,MSE,MAE\n",
    "* Their best results are:-\n",
    "* ## BSE 15 Day:\n",
    "MAE Scaled: 0.0522\n",
    " MSE Scaled: 0.0515\n",
    " RMSE Scaled: 0.1456\n",
    "* ## NSE 15 Day:\n",
    "MAE Scaled: 0.0712\n",
    " MSE Scaled: 0.0521\n",
    "  RMSE Scaled: 0.1365\n",
    "* Paper link:-https://ieeexplore.ieee.org/document/10481618"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensex (without vix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:10:03.462569Z",
     "iopub.status.busy": "2025-07-01T05:10:03.462252Z",
     "iopub.status.idle": "2025-07-01T05:10:03.518076Z",
     "shell.execute_reply": "2025-07-01T05:10:03.517457Z",
     "shell.execute_reply.started": "2025-07-01T05:10:03.462543Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data=pd.read_csv(\"/kaggle/input/paper-5-deepstock-dataset/BSE Sensex 30 Historical Data ( paper - 5 ).csv\")\n",
    "data=data[::-1]\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.head()\n",
    "data.nunique()\n",
    "\n",
    "data.sort_index(axis=1,ascending=True)\n",
    "data.rename(columns={'Price': 'Close'}, inplace=True)\n",
    "df = data.copy()  # Ensure we don't modify the original dataset\n",
    "\n",
    "# Convert financial columns to numeric (remove commas)\n",
    "for col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n",
    "    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n",
    "\n",
    "# Function to convert 'Vol.' column\n",
    "def convert_volume(vol):\n",
    "    if isinstance(vol, str):  # Ensure it's a string before replacing\n",
    "        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n",
    "        if \"B\" in vol:\n",
    "            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n",
    "        elif \"M\" in vol:\n",
    "            return float(vol.replace(\"M\", \"\")) * 1_000_000\n",
    "        elif \"K\" in vol:\n",
    "            return float(vol.replace(\"K\", \"\")) * 1_000\n",
    "    return float(vol)  # Convert directly if already a number\n",
    "\n",
    "df[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n",
    "\n",
    "# Convert 'Change %' column (remove '%' and convert to float)\n",
    "df[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n",
    "\n",
    "# Print final DataFrame\n",
    "print(df.dtypes)\n",
    "print(df)\n",
    "\n",
    "# Assign back to 'data' (if needed)\n",
    "data = df\n",
    "\n",
    "#data = pd.merge(data, vix, on='Date', how='inner')\n",
    "data.drop(['Date'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:10:03.519600Z",
     "iopub.status.busy": "2025-07-01T05:10:03.519384Z",
     "iopub.status.idle": "2025-07-01T05:10:48.082232Z",
     "shell.execute_reply": "2025-07-01T05:10:48.081380Z",
     "shell.execute_reply.started": "2025-07-01T05:10:03.519580Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Multiply, Activation\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Set Seed for Reproducibility\n",
    "# ---------------------------\n",
    "def set_random_seed(seed=20):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "set_random_seed(20)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load & Preprocess Data\n",
    "# ---------------------------\n",
    "# Make sure 'data' is already loaded (e.g., from a CSV)\n",
    "data.dropna(axis=0,inplace=True)\n",
    "df = data.copy()\n",
    "\n",
    "# Scale all features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Get index of the \"Close\" column (target)\n",
    "close_index = df.columns.get_loc(\"Close\")\n",
    "\n",
    "# Create sequences for time series prediction\n",
    "def create_sequences(data, time_steps=15, target_index=0):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i : i + time_steps])                  # shape: (time_steps, features)\n",
    "        y.append(data[i + time_steps, target_index])        # single value: Close price\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences using all features to predict Close\n",
    "X, y = create_sequences(scaled_data, time_steps=15, target_index=close_index)\n",
    "\n",
    "# Train/val/test split\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.1)\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Build Model (unchanged)\n",
    "# ---------------------------\n",
    "def build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n",
    "    inputs = Input(shape=(seq_length, X_train.shape[2]))  # Updated dynamically based on feature count\n",
    "\n",
    "    # Branch 1: 15-day trend\n",
    "    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n",
    "    full_branch = Dense(64, activation='relu')(full_branch)\n",
    "    long_term_return = Dense(1, activation='tanh')(full_branch)\n",
    "    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n",
    "\n",
    "    # Branch 2: last 7 days\n",
    "    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)\n",
    "    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n",
    "    recent_branch = Dense(64, activation='relu')(recent_branch)\n",
    "    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n",
    "    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n",
    "\n",
    "    # Branch 3: last 1 day\n",
    "    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)\n",
    "    very_recent_branch = Flatten()(very_recent_input)\n",
    "    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n",
    "    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n",
    "    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n",
    "\n",
    "    # Gates\n",
    "    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)\n",
    "    gate_1 = Dense(1, activation='sigmoid')(short_term_return)\n",
    "\n",
    "    gated_mid_term_return = Multiply()([mid_term_return, gate_7])\n",
    "    gated_short_term_return = Multiply()([short_term_return, gate_1])\n",
    "\n",
    "    # Combine\n",
    "    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n",
    "    fused_returns = Dense(32, activation='relu')(fused_returns)\n",
    "    final_return = Dense(1, activation='tanh')(fused_returns)\n",
    "    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n",
    "\n",
    "    # Output layer (based on last close price)\n",
    "    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, close_index], axis=-1))(inputs)\n",
    "    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n",
    "\n",
    "    model = Model(inputs, final_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Build and fit the model\n",
    "model_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n",
    "model_attention.summary()\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\n",
    "history = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Evaluate the Model\n",
    "# ---------------------------\n",
    "y_pred = model_attention.predict(X_test)\n",
    "\n",
    "# Inverse transform only the Close column\n",
    "# So we need to reconstruct dummy full vectors with Close column at correct index\n",
    "def invert_close_only(pred_scaled, real_scaled, scaler, close_index):\n",
    "    dummy = np.zeros((len(pred_scaled), scaler.n_features_in_))\n",
    "    dummy[:, close_index] = pred_scaled.flatten()\n",
    "    y_pred_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "\n",
    "    dummy[:, close_index] = real_scaled.flatten()\n",
    "    y_test_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "    return y_pred_inv, y_test_inv\n",
    "\n",
    "y_pred_inv, y_test_inv = invert_close_only(y_pred, y_test.reshape(-1, 1), scaler, close_index)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "rmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\n",
    "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\n",
    "evs = explained_variance_score(y_test_inv, y_pred_inv)\n",
    "\n",
    "print(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test_inv, label='Actual')\n",
    "plt.plot(y_pred_inv, label='Predicted')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensex with vix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:10:48.083623Z",
     "iopub.status.busy": "2025-07-01T05:10:48.083391Z",
     "iopub.status.idle": "2025-07-01T05:10:48.156537Z",
     "shell.execute_reply": "2025-07-01T05:10:48.155841Z",
     "shell.execute_reply.started": "2025-07-01T05:10:48.083603Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vix = pd.read_csv(\"/kaggle/input/paper-5-deepstock-dataset/India VIX Historical Data ( paper 5 ).csv\")\n",
    "vix.drop(\"Vol.\",axis=1,inplace=True)\n",
    "vix[\"Change %\"] = vix[\"Change %\"].str.replace(\"%\", \"\").astype(float)\n",
    "vix.head(),vix.dtypes\n",
    "data=pd.read_csv(\"/kaggle/input/paper-5-deepstock-dataset/BSE Sensex 30 Historical Data ( paper - 5 ).csv\")\n",
    "data=data[::-1]\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.head()\n",
    "data.nunique()\n",
    "\n",
    "data.sort_index(axis=1,ascending=True)\n",
    "data.rename(columns={'Price': 'Close'}, inplace=True)\n",
    "df = data.copy()  # Ensure we don't modify the original dataset\n",
    "\n",
    "# Convert financial columns to numeric (remove commas)\n",
    "for col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n",
    "    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n",
    "\n",
    "# Function to convert 'Vol.' column\n",
    "def convert_volume(vol):\n",
    "    if isinstance(vol, str):  # Ensure it's a string before replacing\n",
    "        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n",
    "        if \"B\" in vol:\n",
    "            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n",
    "        elif \"M\" in vol:\n",
    "            return float(vol.replace(\"M\", \"\")) * 1_000_000\n",
    "        elif \"K\" in vol:\n",
    "            return float(vol.replace(\"K\", \"\")) * 1_000\n",
    "    return float(vol)  # Convert directly if already a number\n",
    "\n",
    "df[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n",
    "\n",
    "# Convert 'Change %' column (remove '%' and convert to float)\n",
    "df[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n",
    "\n",
    "# Print final DataFrame\n",
    "print(df.dtypes)\n",
    "print(df)\n",
    "\n",
    "# Assign back to 'data' (if needed)\n",
    "data = df\n",
    "\n",
    "data = pd.merge(data, vix, on='Date', how='inner')\n",
    "data.drop(['Date'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:10:48.157578Z",
     "iopub.status.busy": "2025-07-01T05:10:48.157300Z",
     "iopub.status.idle": "2025-07-01T05:11:32.221783Z",
     "shell.execute_reply": "2025-07-01T05:11:32.220962Z",
     "shell.execute_reply.started": "2025-07-01T05:10:48.157544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Multiply, Activation\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Set Seed for Reproducibility\n",
    "# ---------------------------\n",
    "def set_random_seed(seed=20):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "set_random_seed(20)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load & Preprocess Data\n",
    "# ---------------------------\n",
    "# Make sure 'data' is already loaded (e.g., from a CSV)\n",
    "data.dropna(axis=0,inplace=True)\n",
    "df = data.copy()\n",
    "\n",
    "# Scale all features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Get index of the \"Close\" column (target)\n",
    "close_index = df.columns.get_loc(\"Close\")\n",
    "\n",
    "# Create sequences for time series prediction\n",
    "def create_sequences(data, time_steps=15, target_index=0):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i : i + time_steps])                  # shape: (time_steps, features)\n",
    "        y.append(data[i + time_steps, target_index])        # single value: Close price\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences using all features to predict Close\n",
    "X, y = create_sequences(scaled_data, time_steps=15, target_index=close_index)\n",
    "\n",
    "# Train/val/test split\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.1)\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Build Model (unchanged)\n",
    "# ---------------------------\n",
    "def build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n",
    "    inputs = Input(shape=(seq_length, X_train.shape[2]))  # Updated dynamically based on feature count\n",
    "\n",
    "    # Branch 1: 15-day trend\n",
    "    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n",
    "    full_branch = Dense(64, activation='relu')(full_branch)\n",
    "    long_term_return = Dense(1, activation='tanh')(full_branch)\n",
    "    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n",
    "\n",
    "    # Branch 2: last 7 days\n",
    "    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)\n",
    "    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n",
    "    recent_branch = Dense(64, activation='relu')(recent_branch)\n",
    "    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n",
    "    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n",
    "\n",
    "    # Branch 3: last 1 day\n",
    "    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)\n",
    "    very_recent_branch = Flatten()(very_recent_input)\n",
    "    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n",
    "    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n",
    "    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n",
    "\n",
    "    # Gates\n",
    "    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)\n",
    "    gate_1 = Dense(1, activation='sigmoid')(short_term_return)\n",
    "\n",
    "    gated_mid_term_return = Multiply()([mid_term_return, gate_7])\n",
    "    gated_short_term_return = Multiply()([short_term_return, gate_1])\n",
    "\n",
    "    # Combine\n",
    "    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n",
    "    fused_returns = Dense(32, activation='relu')(fused_returns)\n",
    "    final_return = Dense(1, activation='tanh')(fused_returns)\n",
    "    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n",
    "\n",
    "    # Output layer (based on last close price)\n",
    "    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, close_index], axis=-1))(inputs)\n",
    "    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n",
    "\n",
    "    model = Model(inputs, final_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Build and fit the model\n",
    "model_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n",
    "model_attention.summary()\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\n",
    "history = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Evaluate the Model\n",
    "# ---------------------------\n",
    "y_pred = model_attention.predict(X_test)\n",
    "\n",
    "# Inverse transform only the Close column\n",
    "# So we need to reconstruct dummy full vectors with Close column at correct index\n",
    "def invert_close_only(pred_scaled, real_scaled, scaler, close_index):\n",
    "    dummy = np.zeros((len(pred_scaled), scaler.n_features_in_))\n",
    "    dummy[:, close_index] = pred_scaled.flatten()\n",
    "    y_pred_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "\n",
    "    dummy[:, close_index] = real_scaled.flatten()\n",
    "    y_test_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "    return y_pred_inv, y_test_inv\n",
    "\n",
    "y_pred_inv, y_test_inv = invert_close_only(y_pred, y_test.reshape(-1, 1), scaler, close_index)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "rmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\n",
    "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\n",
    "evs = explained_variance_score(y_test_inv, y_pred_inv)\n",
    "\n",
    "print(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test_inv, label='Actual')\n",
    "plt.plot(y_pred_inv, label='Predicted')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nifty50 with vix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:11:32.222797Z",
     "iopub.status.busy": "2025-07-01T05:11:32.222568Z",
     "iopub.status.idle": "2025-07-01T05:11:32.246444Z",
     "shell.execute_reply": "2025-07-01T05:11:32.245656Z",
     "shell.execute_reply.started": "2025-07-01T05:11:32.222773Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vix = pd.read_csv(\"/kaggle/input/paper-5-deepstock-dataset/India VIX Historical Data ( paper 5 ).csv\")\n",
    "vix.drop(\"Vol.\",axis=1,inplace=True)\n",
    "vix[\"Change %\"] = vix[\"Change %\"].str.replace(\"%\", \"\").astype(float)\n",
    "vix.head(),vix.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:11:32.247579Z",
     "iopub.status.busy": "2025-07-01T05:11:32.247303Z",
     "iopub.status.idle": "2025-07-01T05:11:32.324008Z",
     "shell.execute_reply": "2025-07-01T05:11:32.323167Z",
     "shell.execute_reply.started": "2025-07-01T05:11:32.247547Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/kaggle/input/paper-5-deepstock-dataset/Nifty 50 Historical Data - paper( 5 ).csv\")\n",
    "data=data[::-1]\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.head()\n",
    "data.nunique()\n",
    "\n",
    "data.sort_index(axis=1,ascending=True)\n",
    "data.rename(columns={'Price': 'Close'}, inplace=True)\n",
    "df = data.copy()  # Ensure we don't modify the original dataset\n",
    "\n",
    "# Convert financial columns to numeric (remove commas)\n",
    "for col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n",
    "    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n",
    "\n",
    "# Function to convert 'Vol.' column\n",
    "def convert_volume(vol):\n",
    "    if isinstance(vol, str):  # Ensure it's a string before replacing\n",
    "        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n",
    "        if \"B\" in vol:\n",
    "            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n",
    "        elif \"M\" in vol:\n",
    "            return float(vol.replace(\"M\", \"\")) * 1_000_000\n",
    "        elif \"K\" in vol:\n",
    "            return float(vol.replace(\"K\", \"\")) * 1_000\n",
    "    return float(vol)  # Convert directly if already a number\n",
    "\n",
    "df[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n",
    "\n",
    "# Convert 'Change %' column (remove '%' and convert to float)\n",
    "df[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n",
    "\n",
    "# Print final DataFrame\n",
    "print(df.dtypes)\n",
    "print(df)\n",
    "\n",
    "# Assign back to 'data' (if needed)\n",
    "data = df\n",
    "\n",
    "data = pd.merge(data, vix, on='Date', how='inner')\n",
    "data.drop(['Date'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:11:32.326552Z",
     "iopub.status.busy": "2025-07-01T05:11:32.326355Z",
     "iopub.status.idle": "2025-07-01T05:12:59.827011Z",
     "shell.execute_reply": "2025-07-01T05:12:59.826211Z",
     "shell.execute_reply.started": "2025-07-01T05:11:32.326534Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Multiply, Activation\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Set Seed for Reproducibility\n",
    "# ---------------------------\n",
    "def set_random_seed(seed=20):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "set_random_seed(20)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load & Preprocess Data\n",
    "# ---------------------------\n",
    "# Make sure 'data' is already loaded (e.g., from a CSV)\n",
    "data.dropna(axis=0,inplace=True)\n",
    "df = data.copy()\n",
    "\n",
    "# Scale all features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Get index of the \"Close\" column (target)\n",
    "close_index = df.columns.get_loc(\"Close\")\n",
    "\n",
    "# Create sequences for time series prediction\n",
    "def create_sequences(data, time_steps=15, target_index=0):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i : i + time_steps])                  # shape: (time_steps, features)\n",
    "        y.append(data[i + time_steps, target_index])        # single value: Close price\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences using all features to predict Close\n",
    "X, y = create_sequences(scaled_data, time_steps=15, target_index=close_index)\n",
    "\n",
    "# Train/val/test split\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.1)\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Build Model (unchanged)\n",
    "# ---------------------------\n",
    "def build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n",
    "    inputs = Input(shape=(seq_length, X_train.shape[2]))  # Updated dynamically based on feature count\n",
    "\n",
    "    # Branch 1: 15-day trend\n",
    "    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n",
    "    full_branch = Dense(64, activation='relu')(full_branch)\n",
    "    long_term_return = Dense(1, activation='tanh')(full_branch)\n",
    "    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n",
    "\n",
    "    # Branch 2: last 7 days\n",
    "    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)\n",
    "    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n",
    "    recent_branch = Dense(64, activation='relu')(recent_branch)\n",
    "    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n",
    "    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n",
    "\n",
    "    # Branch 3: last 1 day\n",
    "    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)\n",
    "    very_recent_branch = Flatten()(very_recent_input)\n",
    "    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n",
    "    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n",
    "    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n",
    "\n",
    "    # Gates\n",
    "    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)\n",
    "    gate_1 = Dense(1, activation='sigmoid')(short_term_return)\n",
    "\n",
    "    gated_mid_term_return = Multiply()([mid_term_return, gate_7])\n",
    "    gated_short_term_return = Multiply()([short_term_return, gate_1])\n",
    "\n",
    "    # Combine\n",
    "    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n",
    "    fused_returns = Dense(32, activation='relu')(fused_returns)\n",
    "    final_return = Dense(1, activation='tanh')(fused_returns)\n",
    "    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n",
    "\n",
    "    # Output layer (based on last close price)\n",
    "    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, close_index], axis=-1))(inputs)\n",
    "    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n",
    "\n",
    "    model = Model(inputs, final_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Build and fit the model\n",
    "model_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n",
    "model_attention.summary()\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\n",
    "history = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Evaluate the Model\n",
    "# ---------------------------\n",
    "y_pred = model_attention.predict(X_test)\n",
    "\n",
    "# Inverse transform only the Close column\n",
    "# So we need to reconstruct dummy full vectors with Close column at correct index\n",
    "def invert_close_only(pred_scaled, real_scaled, scaler, close_index):\n",
    "    dummy = np.zeros((len(pred_scaled), scaler.n_features_in_))\n",
    "    dummy[:, close_index] = pred_scaled.flatten()\n",
    "    y_pred_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "\n",
    "    dummy[:, close_index] = real_scaled.flatten()\n",
    "    y_test_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "    return y_pred_inv, y_test_inv\n",
    "\n",
    "y_pred_inv, y_test_inv = invert_close_only(y_pred, y_test.reshape(-1, 1), scaler, close_index)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "rmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\n",
    "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\n",
    "evs = explained_variance_score(y_test_inv, y_pred_inv)\n",
    "\n",
    "print(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test_inv, label='Actual')\n",
    "plt.plot(y_pred_inv, label='Predicted')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nifty50 without vix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:12:59.828559Z",
     "iopub.status.busy": "2025-07-01T05:12:59.828336Z",
     "iopub.status.idle": "2025-07-01T05:12:59.884093Z",
     "shell.execute_reply": "2025-07-01T05:12:59.883459Z",
     "shell.execute_reply.started": "2025-07-01T05:12:59.828539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/kaggle/input/paper-5-deepstock-dataset/Nifty 50 Historical Data - paper( 5 ).csv\")\n",
    "data=data[::-1]\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.head()\n",
    "data.nunique()\n",
    "\n",
    "data.sort_index(axis=1,ascending=True)\n",
    "data.rename(columns={'Price': 'Close'}, inplace=True)\n",
    "df = data.copy()  # Ensure we don't modify the original dataset\n",
    "\n",
    "# Convert financial columns to numeric (remove commas)\n",
    "for col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n",
    "    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n",
    "\n",
    "# Function to convert 'Vol.' column\n",
    "def convert_volume(vol):\n",
    "    if isinstance(vol, str):  # Ensure it's a string before replacing\n",
    "        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n",
    "        if \"B\" in vol:\n",
    "            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n",
    "        elif \"M\" in vol:\n",
    "            return float(vol.replace(\"M\", \"\")) * 1_000_000\n",
    "        elif \"K\" in vol:\n",
    "            return float(vol.replace(\"K\", \"\")) * 1_000\n",
    "    return float(vol)  # Convert directly if already a number\n",
    "\n",
    "df[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n",
    "\n",
    "# Convert 'Change %' column (remove '%' and convert to float)\n",
    "df[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n",
    "\n",
    "# Print final DataFrame\n",
    "print(df.dtypes)\n",
    "print(df)\n",
    "\n",
    "# Assign back to 'data' (if needed)\n",
    "data = df\n",
    "\n",
    "#data = pd.merge(data, vix, on='Date', how='inner')\n",
    "data.drop(['Date'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:12:59.885035Z",
     "iopub.status.busy": "2025-07-01T05:12:59.884840Z",
     "iopub.status.idle": "2025-07-01T05:14:06.493105Z",
     "shell.execute_reply": "2025-07-01T05:14:06.492169Z",
     "shell.execute_reply.started": "2025-07-01T05:12:59.885018Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Multiply, Activation\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Set Seed for Reproducibility\n",
    "# ---------------------------\n",
    "def set_random_seed(seed=20):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "set_random_seed(20)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load & Preprocess Data\n",
    "# ---------------------------\n",
    "# Make sure 'data' is already loaded (e.g., from a CSV)\n",
    "data.dropna(axis=0,inplace=True)\n",
    "df = data.copy()\n",
    "\n",
    "# Scale all features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Get index of the \"Close\" column (target)\n",
    "close_index = df.columns.get_loc(\"Close\")\n",
    "\n",
    "# Create sequences for time series prediction\n",
    "def create_sequences(data, time_steps=15, target_index=0):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i : i + time_steps])                  # shape: (time_steps, features)\n",
    "        y.append(data[i + time_steps, target_index])        # single value: Close price\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences using all features to predict Close\n",
    "X, y = create_sequences(scaled_data, time_steps=15, target_index=close_index)\n",
    "\n",
    "# Train/val/test split\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.1)\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Build Model (unchanged)\n",
    "# ---------------------------\n",
    "def build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n",
    "    inputs = Input(shape=(seq_length, X_train.shape[2]))  # Updated dynamically based on feature count\n",
    "\n",
    "    # Branch 1: 15-day trend\n",
    "    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n",
    "    full_branch = Dense(64, activation='relu')(full_branch)\n",
    "    long_term_return = Dense(1, activation='tanh')(full_branch)\n",
    "    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n",
    "\n",
    "    # Branch 2: last 7 days\n",
    "    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)\n",
    "    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n",
    "    recent_branch = Dense(64, activation='relu')(recent_branch)\n",
    "    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n",
    "    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n",
    "\n",
    "    # Branch 3: last 1 day\n",
    "    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)\n",
    "    very_recent_branch = Flatten()(very_recent_input)\n",
    "    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n",
    "    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n",
    "    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n",
    "\n",
    "    # Gates\n",
    "    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)\n",
    "    gate_1 = Dense(1, activation='sigmoid')(short_term_return)\n",
    "\n",
    "    gated_mid_term_return = Multiply()([mid_term_return, gate_7])\n",
    "    gated_short_term_return = Multiply()([short_term_return, gate_1])\n",
    "\n",
    "    # Combine\n",
    "    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n",
    "    fused_returns = Dense(32, activation='relu')(fused_returns)\n",
    "    final_return = Dense(1, activation='tanh')(fused_returns)\n",
    "    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n",
    "\n",
    "    # Output layer (based on last close price)\n",
    "    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, close_index], axis=-1))(inputs)\n",
    "    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n",
    "\n",
    "    model = Model(inputs, final_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Build and fit the model\n",
    "model_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n",
    "model_attention.summary()\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\n",
    "history = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Evaluate the Model\n",
    "# ---------------------------\n",
    "y_pred = model_attention.predict(X_test)\n",
    "\n",
    "# Inverse transform only the Close column\n",
    "# So we need to reconstruct dummy full vectors with Close column at correct index\n",
    "def invert_close_only(pred_scaled, real_scaled, scaler, close_index):\n",
    "    dummy = np.zeros((len(pred_scaled), scaler.n_features_in_))\n",
    "    dummy[:, close_index] = pred_scaled.flatten()\n",
    "    y_pred_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "\n",
    "    dummy[:, close_index] = real_scaled.flatten()\n",
    "    y_test_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "    return y_pred_inv, y_test_inv\n",
    "\n",
    "y_pred_inv, y_test_inv = invert_close_only(y_pred, y_test.reshape(-1, 1), scaler, close_index)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "rmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\n",
    "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\n",
    "evs = explained_variance_score(y_test_inv, y_pred_inv)\n",
    "\n",
    "print(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test_inv, label='Actual')\n",
    "plt.plot(y_pred_inv, label='Predicted')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUR RANGE: SENSEX (15-3-2008 to 15-3-2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:14:06.494297Z",
     "iopub.status.busy": "2025-07-01T05:14:06.493981Z",
     "iopub.status.idle": "2025-07-01T05:14:06.597432Z",
     "shell.execute_reply": "2025-07-01T05:14:06.596730Z",
     "shell.execute_reply.started": "2025-07-01T05:14:06.494265Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vix = pd.read_csv(\"/kaggle/input/our-data-15-3-2008-to-15-3-2024/India VIX Historical Data (3).csv\")\n",
    "vix.drop(\"Vol.\",axis=1,inplace=True)\n",
    "vix[\"Change %\"] = vix[\"Change %\"].str.replace(\"%\", \"\").astype(float)\n",
    "vix.head(),vix.dtypes\n",
    "data=pd.read_csv(\"/kaggle/input/our-data-15-3-2008-to-15-3-2024/BSE Sensex 30 Historical Data (3).csv\")\n",
    "data=data[::-1]\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.head()\n",
    "data.nunique()\n",
    "\n",
    "data.sort_index(axis=1,ascending=True)\n",
    "data.rename(columns={'Price': 'Close'}, inplace=True)\n",
    "df = data.copy()  # Ensure we don't modify the original dataset\n",
    "\n",
    "# Convert financial columns to numeric (remove commas)\n",
    "for col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n",
    "    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n",
    "\n",
    "# Function to convert 'Vol.' column\n",
    "def convert_volume(vol):\n",
    "    if isinstance(vol, str):  # Ensure it's a string before replacing\n",
    "        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n",
    "        if \"B\" in vol:\n",
    "            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n",
    "        elif \"M\" in vol:\n",
    "            return float(vol.replace(\"M\", \"\")) * 1_000_000\n",
    "        elif \"K\" in vol:\n",
    "            return float(vol.replace(\"K\", \"\")) * 1_000\n",
    "    return float(vol)  # Convert directly if already a number\n",
    "\n",
    "df[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n",
    "\n",
    "# Convert 'Change %' column (remove '%' and convert to float)\n",
    "df[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n",
    "\n",
    "# Print final DataFrame\n",
    "print(df.dtypes)\n",
    "print(df)\n",
    "\n",
    "# Assign back to 'data' (if needed)\n",
    "data = df\n",
    "\n",
    "data = pd.merge(data, vix, on='Date', how='inner')\n",
    "data.drop(['Date'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:14:06.598433Z",
     "iopub.status.busy": "2025-07-01T05:14:06.598195Z",
     "iopub.status.idle": "2025-07-01T05:16:17.314130Z",
     "shell.execute_reply": "2025-07-01T05:16:17.313163Z",
     "shell.execute_reply.started": "2025-07-01T05:14:06.598400Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Multiply, Activation\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Set Seed for Reproducibility\n",
    "# ---------------------------\n",
    "def set_random_seed(seed=20):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "set_random_seed(20)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load & Preprocess Data\n",
    "# ---------------------------\n",
    "# Make sure 'data' is already loaded (e.g., from a CSV)\n",
    "data.dropna(axis=0,inplace=True)\n",
    "df = data.copy()\n",
    "\n",
    "# Scale all features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Get index of the \"Close\" column (target)\n",
    "close_index = df.columns.get_loc(\"Close\")\n",
    "\n",
    "# Create sequences for time series prediction\n",
    "def create_sequences(data, time_steps=15, target_index=0):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i : i + time_steps])                  # shape: (time_steps, features)\n",
    "        y.append(data[i + time_steps, target_index])        # single value: Close price\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences using all features to predict Close\n",
    "X, y = create_sequences(scaled_data, time_steps=15, target_index=close_index)\n",
    "\n",
    "# Train/val/test split\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.1)\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Build Model (unchanged)\n",
    "# ---------------------------\n",
    "def build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n",
    "    inputs = Input(shape=(seq_length, X_train.shape[2]))  # Updated dynamically based on feature count\n",
    "\n",
    "    # Branch 1: 15-day trend\n",
    "    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n",
    "    full_branch = Dense(64, activation='relu')(full_branch)\n",
    "    long_term_return = Dense(1, activation='tanh')(full_branch)\n",
    "    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n",
    "\n",
    "    # Branch 2: last 7 days\n",
    "    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)\n",
    "    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n",
    "    recent_branch = Dense(64, activation='relu')(recent_branch)\n",
    "    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n",
    "    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n",
    "\n",
    "    # Branch 3: last 1 day\n",
    "    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)\n",
    "    very_recent_branch = Flatten()(very_recent_input)\n",
    "    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n",
    "    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n",
    "    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n",
    "\n",
    "    # Gates\n",
    "    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)\n",
    "    gate_1 = Dense(1, activation='sigmoid')(short_term_return)\n",
    "\n",
    "    gated_mid_term_return = Multiply()([mid_term_return, gate_7])\n",
    "    gated_short_term_return = Multiply()([short_term_return, gate_1])\n",
    "\n",
    "    # Combine\n",
    "    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n",
    "    fused_returns = Dense(32, activation='relu')(fused_returns)\n",
    "    final_return = Dense(1, activation='tanh')(fused_returns)\n",
    "    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n",
    "\n",
    "    # Output layer (based on last close price)\n",
    "    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, close_index], axis=-1))(inputs)\n",
    "    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n",
    "\n",
    "    model = Model(inputs, final_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Build and fit the model\n",
    "model_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n",
    "model_attention.summary()\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\n",
    "history = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Evaluate the Model\n",
    "# ---------------------------\n",
    "y_pred = model_attention.predict(X_test)\n",
    "\n",
    "# Inverse transform only the Close column\n",
    "# So we need to reconstruct dummy full vectors with Close column at correct index\n",
    "def invert_close_only(pred_scaled, real_scaled, scaler, close_index):\n",
    "    dummy = np.zeros((len(pred_scaled), scaler.n_features_in_))\n",
    "    dummy[:, close_index] = pred_scaled.flatten()\n",
    "    y_pred_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "\n",
    "    dummy[:, close_index] = real_scaled.flatten()\n",
    "    y_test_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "    return y_pred_inv, y_test_inv\n",
    "\n",
    "y_pred_inv, y_test_inv = invert_close_only(y_pred, y_test.reshape(-1, 1), scaler, close_index)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "rmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\n",
    "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\n",
    "evs = explained_variance_score(y_test_inv, y_pred_inv)\n",
    "\n",
    "print(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test_inv, label='Actual')\n",
    "plt.plot(y_pred_inv, label='Predicted')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUR RANGE: NIFTY50 (15-3-2008 to 15-3-2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:16:17.315384Z",
     "iopub.status.busy": "2025-07-01T05:16:17.315083Z",
     "iopub.status.idle": "2025-07-01T05:16:17.405037Z",
     "shell.execute_reply": "2025-07-01T05:16:17.404392Z",
     "shell.execute_reply.started": "2025-07-01T05:16:17.315362Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vix = pd.read_csv(\"/kaggle/input/our-data-15-3-2008-to-15-3-2024/India VIX Historical Data (3).csv\")\n",
    "vix.drop(\"Vol.\",axis=1,inplace=True)\n",
    "vix[\"Change %\"] = vix[\"Change %\"].str.replace(\"%\", \"\").astype(float)\n",
    "vix.head(),vix.dtypes\n",
    "data=pd.read_csv(\"/kaggle/input/our-data-15-3-2008-to-15-3-2024/Nifty 50 Historical Data (2).csv\")\n",
    "data=data[::-1]\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.head()\n",
    "data.nunique()\n",
    "\n",
    "data.sort_index(axis=1,ascending=True)\n",
    "data.rename(columns={'Price': 'Close'}, inplace=True)\n",
    "df = data.copy()  # Ensure we don't modify the original dataset\n",
    "\n",
    "# Convert financial columns to numeric (remove commas)\n",
    "for col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n",
    "    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n",
    "\n",
    "# Function to convert 'Vol.' column\n",
    "def convert_volume(vol):\n",
    "    if isinstance(vol, str):  # Ensure it's a string before replacing\n",
    "        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n",
    "        if \"B\" in vol:\n",
    "            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n",
    "        elif \"M\" in vol:\n",
    "            return float(vol.replace(\"M\", \"\")) * 1_000_000\n",
    "        elif \"K\" in vol:\n",
    "            return float(vol.replace(\"K\", \"\")) * 1_000\n",
    "    return float(vol)  # Convert directly if already a number\n",
    "\n",
    "df[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n",
    "\n",
    "# Convert 'Change %' column (remove '%' and convert to float)\n",
    "df[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n",
    "\n",
    "# Print final DataFrame\n",
    "print(df.dtypes)\n",
    "print(df)\n",
    "\n",
    "# Assign back to 'data' (if needed)\n",
    "data = df\n",
    "\n",
    "data = pd.merge(data, vix, on='Date', how='inner')\n",
    "data.drop(['Date'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:16:17.406336Z",
     "iopub.status.busy": "2025-07-01T05:16:17.405942Z",
     "iopub.status.idle": "2025-07-01T05:17:40.509742Z",
     "shell.execute_reply": "2025-07-01T05:17:40.508870Z",
     "shell.execute_reply.started": "2025-07-01T05:16:17.406300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Multiply, Activation\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Set Seed for Reproducibility\n",
    "# ---------------------------\n",
    "def set_random_seed(seed=20):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "set_random_seed(20)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load & Preprocess Data\n",
    "# ---------------------------\n",
    "# Make sure 'data' is already loaded (e.g., from a CSV)\n",
    "data.dropna(axis=0,inplace=True)\n",
    "df = data.copy()\n",
    "\n",
    "# Scale all features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Get index of the \"Close\" column (target)\n",
    "close_index = df.columns.get_loc(\"Close\")\n",
    "\n",
    "# Create sequences for time series prediction\n",
    "def create_sequences(data, time_steps=15, target_index=0):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i : i + time_steps])                  # shape: (time_steps, features)\n",
    "        y.append(data[i + time_steps, target_index])        # single value: Close price\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences using all features to predict Close\n",
    "X, y = create_sequences(scaled_data, time_steps=15, target_index=close_index)\n",
    "\n",
    "# Train/val/test split\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.1)\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Build Model (unchanged)\n",
    "# ---------------------------\n",
    "def build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n",
    "    inputs = Input(shape=(seq_length, X_train.shape[2]))  # Updated dynamically based on feature count\n",
    "\n",
    "    # Branch 1: 15-day trend\n",
    "    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n",
    "    full_branch = Dense(64, activation='relu')(full_branch)\n",
    "    long_term_return = Dense(1, activation='tanh')(full_branch)\n",
    "    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n",
    "\n",
    "    # Branch 2: last 7 days\n",
    "    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)\n",
    "    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n",
    "    recent_branch = Dense(64, activation='relu')(recent_branch)\n",
    "    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n",
    "    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n",
    "\n",
    "    # Branch 3: last 1 day\n",
    "    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)\n",
    "    very_recent_branch = Flatten()(very_recent_input)\n",
    "    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n",
    "    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n",
    "    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n",
    "\n",
    "    # Gates\n",
    "    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)\n",
    "    gate_1 = Dense(1, activation='sigmoid')(short_term_return)\n",
    "\n",
    "    gated_mid_term_return = Multiply()([mid_term_return, gate_7])\n",
    "    gated_short_term_return = Multiply()([short_term_return, gate_1])\n",
    "\n",
    "    # Combine\n",
    "    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n",
    "    fused_returns = Dense(32, activation='relu')(fused_returns)\n",
    "    final_return = Dense(1, activation='tanh')(fused_returns)\n",
    "    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n",
    "\n",
    "    # Output layer (based on last close price)\n",
    "    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, close_index], axis=-1))(inputs)\n",
    "    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n",
    "\n",
    "    model = Model(inputs, final_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Build and fit the model\n",
    "model_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n",
    "model_attention.summary()\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\n",
    "history = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Evaluate the Model\n",
    "# ---------------------------\n",
    "y_pred = model_attention.predict(X_test)\n",
    "\n",
    "# Inverse transform only the Close column\n",
    "# So we need to reconstruct dummy full vectors with Close column at correct index\n",
    "def invert_close_only(pred_scaled, real_scaled, scaler, close_index):\n",
    "    dummy = np.zeros((len(pred_scaled), scaler.n_features_in_))\n",
    "    dummy[:, close_index] = pred_scaled.flatten()\n",
    "    y_pred_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "\n",
    "    dummy[:, close_index] = real_scaled.flatten()\n",
    "    y_test_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "    return y_pred_inv, y_test_inv\n",
    "\n",
    "y_pred_inv, y_test_inv = invert_close_only(y_pred, y_test.reshape(-1, 1), scaler, close_index)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "rmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\n",
    "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\n",
    "evs = explained_variance_score(y_test_inv, y_pred_inv)\n",
    "\n",
    "print(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test_inv, label='Actual')\n",
    "plt.plot(y_pred_inv, label='Predicted')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOING FOR SP500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:17:40.510941Z",
     "iopub.status.busy": "2025-07-01T05:17:40.510652Z",
     "iopub.status.idle": "2025-07-01T05:17:40.538393Z",
     "shell.execute_reply": "2025-07-01T05:17:40.537573Z",
     "shell.execute_reply.started": "2025-07-01T05:17:40.510910Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"/kaggle/input/personal-dateset-vix/SP 500 (2).csv\")\n",
    "\n",
    "# Ensure the required columns exist\n",
    "if \"Price\" in df.columns and \"Open\" in df.columns:\n",
    "    # Calculate the percentage change\n",
    "    df[\"Change %\"] = ((df[\"Price\"] - df[\"Open\"]) / df[\"Price\"]) * 100\n",
    "\n",
    "    # Save the updated CSV file\n",
    "    # df.to_csv(\"SP 500 (2).csv\", index=False)\n",
    "\n",
    "    # Display first few rows\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"Error: CSV file must contain 'Price' and 'Open' columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:17:40.539506Z",
     "iopub.status.busy": "2025-07-01T05:17:40.539254Z",
     "iopub.status.idle": "2025-07-01T05:17:40.569092Z",
     "shell.execute_reply": "2025-07-01T05:17:40.568316Z",
     "shell.execute_reply.started": "2025-07-01T05:17:40.539475Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "vix = pd.read_csv(\"/kaggle/input/personal-dateset-vix/SP 500 VIX.csv\")\n",
    "vix.drop(\"Vol.\",axis=1,inplace=True)\n",
    "def convert_volume(val):\n",
    "    if pd.isna(val) or val == '-':  # Handle NaN or '-' cases\n",
    "        return np.nan\n",
    "    val = val.strip().replace(',', '')  # Remove spaces & commas\n",
    "    if val[-1] == 'K': return float(val[:-1]) * 1e3\n",
    "    if val[-1] == 'M': return float(val[:-1]) * 1e6\n",
    "    if val[-1] == 'B': return float(val[:-1]) * 1e9\n",
    "    return float(val)\n",
    "\n",
    "#vix[\"Vol.\"] = vix[\"Vol.\"].apply(convert_volume)\n",
    "vix.rename(columns={'Open':'Vopen','High':'Vhigh','Low':'Vlow','Change %':'Vchange%'}, inplace=True)\n",
    "vix[\"Vchange%\"] = vix[\"Vchange%\"].str.replace(\"%\", \"\").astype(float)\n",
    "vix.head(),vix.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:17:40.570248Z",
     "iopub.status.busy": "2025-07-01T05:17:40.569948Z",
     "iopub.status.idle": "2025-07-01T05:17:40.630795Z",
     "shell.execute_reply": "2025-07-01T05:17:40.630000Z",
     "shell.execute_reply.started": "2025-07-01T05:17:40.570219Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data=df\n",
    "data=data[::-1]\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.head()\n",
    "data.nunique()\n",
    "\n",
    "data.sort_index(axis=1,ascending=True)\n",
    "data.rename(columns={'Price': 'Close'}, inplace=True)\n",
    "df = data.copy()  # Ensure we don't modify the original dataset\n",
    "\n",
    "# Convert financial columns to numeric (remove commas)\n",
    "for col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n",
    "    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n",
    "\n",
    "# Function to convert 'Vol.' column\n",
    "#df.drop(\"Vol.\",axis=1,inplace=True)\n",
    "\n",
    "# Convert 'Change %' column (remove '%' and convert to float)\n",
    "df[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n",
    "\n",
    "# Print final DataFrame\n",
    "print(df.dtypes)\n",
    "print(df)\n",
    "\n",
    "# Assign back to 'data' (if needed)\n",
    "data = df\n",
    "data['Date'] = pd.to_datetime(data['Date'], format=\"%d-%m-%Y\")\n",
    "\n",
    "# For 'vix', the date format is \"MM/DD/YYYY\"\n",
    "vix['Date'] = pd.to_datetime(vix['Date'], format=\"%m/%d/%Y\")\n",
    "data = pd.merge(data, vix, on='Date', how='inner')\n",
    "row=data[data[\"Vol.\"].isna()]\n",
    "data.drop(['Date'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:17:40.631982Z",
     "iopub.status.busy": "2025-07-01T05:17:40.631691Z",
     "iopub.status.idle": "2025-07-01T05:21:22.171638Z",
     "shell.execute_reply": "2025-07-01T05:21:22.170910Z",
     "shell.execute_reply.started": "2025-07-01T05:17:40.631953Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Multiply, Activation\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Set Seed for Reproducibility\n",
    "# ---------------------------\n",
    "def set_random_seed(seed=20):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "set_random_seed(20)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load & Preprocess Data\n",
    "# ---------------------------\n",
    "# Make sure 'data' is already loaded (e.g., from a CSV)\n",
    "data.dropna(axis=0,inplace=True)\n",
    "df = data.copy()\n",
    "\n",
    "# Scale all features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Get index of the \"Close\" column (target)\n",
    "close_index = df.columns.get_loc(\"Close\")\n",
    "\n",
    "# Create sequences for time series prediction\n",
    "def create_sequences(data, time_steps=15, target_index=0):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i : i + time_steps])                  # shape: (time_steps, features)\n",
    "        y.append(data[i + time_steps, target_index])        # single value: Close price\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences using all features to predict Close\n",
    "X, y = create_sequences(scaled_data, time_steps=15, target_index=close_index)\n",
    "\n",
    "# Train/val/test split\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.1)\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Build Model (unchanged)\n",
    "# ---------------------------\n",
    "def build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n",
    "    inputs = Input(shape=(seq_length, X_train.shape[2]))  # Updated dynamically based on feature count\n",
    "\n",
    "    # Branch 1: 15-day trend\n",
    "    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n",
    "    full_branch = Dense(64, activation='relu')(full_branch)\n",
    "    long_term_return = Dense(1, activation='tanh')(full_branch)\n",
    "    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n",
    "\n",
    "    # Branch 2: last 7 days\n",
    "    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)\n",
    "    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n",
    "    recent_branch = Dense(64, activation='relu')(recent_branch)\n",
    "    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n",
    "    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n",
    "\n",
    "    # Branch 3: last 1 day\n",
    "    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)\n",
    "    very_recent_branch = Flatten()(very_recent_input)\n",
    "    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n",
    "    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n",
    "    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n",
    "\n",
    "    # Gates\n",
    "    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)\n",
    "    gate_1 = Dense(1, activation='sigmoid')(short_term_return)\n",
    "\n",
    "    gated_mid_term_return = Multiply()([mid_term_return, gate_7])\n",
    "    gated_short_term_return = Multiply()([short_term_return, gate_1])\n",
    "\n",
    "    # Combine\n",
    "    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n",
    "    fused_returns = Dense(32, activation='relu')(fused_returns)\n",
    "    final_return = Dense(1, activation='tanh')(fused_returns)\n",
    "    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n",
    "\n",
    "    # Output layer (based on last close price)\n",
    "    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, close_index], axis=-1))(inputs)\n",
    "    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n",
    "\n",
    "    model = Model(inputs, final_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Build and fit the model\n",
    "model_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n",
    "model_attention.summary()\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\n",
    "history = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Evaluate the Model\n",
    "# ---------------------------\n",
    "y_pred = model_attention.predict(X_test)\n",
    "\n",
    "# Inverse transform only the Close column\n",
    "# So we need to reconstruct dummy full vectors with Close column at correct index\n",
    "def invert_close_only(pred_scaled, real_scaled, scaler, close_index):\n",
    "    dummy = np.zeros((len(pred_scaled), scaler.n_features_in_))\n",
    "    dummy[:, close_index] = pred_scaled.flatten()\n",
    "    y_pred_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "\n",
    "    dummy[:, close_index] = real_scaled.flatten()\n",
    "    y_test_inv = scaler.inverse_transform(dummy)[:, close_index]\n",
    "    return y_pred_inv, y_test_inv\n",
    "\n",
    "y_pred_inv, y_test_inv = invert_close_only(y_pred, y_test.reshape(-1, 1), scaler, close_index)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "rmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\n",
    "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\n",
    "evs = explained_variance_score(y_test_inv, y_pred_inv)\n",
    "\n",
    "print(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test_inv, label='Actual')\n",
    "plt.plot(y_pred_inv, label='Predicted')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:21:22.172758Z",
     "iopub.status.busy": "2025-07-01T05:21:22.172455Z",
     "iopub.status.idle": "2025-07-01T05:21:22.345682Z",
     "shell.execute_reply": "2025-07-01T05:21:22.344785Z",
     "shell.execute_reply.started": "2025-07-01T05:21:22.172727Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(y_test_inv, label='Actual')\n",
    "plt.plot(y_pred_inv, label='Predicted')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6591013,
     "sourceId": 10644584,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6608730,
     "sourceId": 10670161,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6617585,
     "sourceId": 10681944,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6631576,
     "sourceId": 10701073,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6631289,
     "sourceId": 10708519,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6646244,
     "sourceId": 10721662,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6790193,
     "sourceId": 10921766,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6790221,
     "sourceId": 10921810,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6631332,
     "sourceId": 10992211,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
