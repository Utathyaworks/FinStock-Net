{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10644584,"sourceType":"datasetVersion","datasetId":6591013},{"sourceId":10670161,"sourceType":"datasetVersion","datasetId":6608730},{"sourceId":10681944,"sourceType":"datasetVersion","datasetId":6617585},{"sourceId":10701073,"sourceType":"datasetVersion","datasetId":6631576},{"sourceId":10708519,"sourceType":"datasetVersion","datasetId":6631289},{"sourceId":10721662,"sourceType":"datasetVersion","datasetId":6646244},{"sourceId":10921766,"sourceType":"datasetVersion","datasetId":6790193},{"sourceId":10921810,"sourceType":"datasetVersion","datasetId":6790221},{"sourceId":10992211,"sourceType":"datasetVersion","datasetId":6631332}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T10:58:07.932938Z","iopub.execute_input":"2025-03-11T10:58:07.933176Z","iopub.status.idle":"2025-03-11T10:58:09.326212Z","shell.execute_reply.started":"2025-03-11T10:58:07.933153Z","shell.execute_reply":"2025-03-11T10:58:09.325106Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Working with Paper 1\n* This Paper does Prediction on BSE Index data\n* The date of their data ranges from 3rd April 1979 to 13th January 2022\n* It uses MAE metric\n* The best result quoted by them is MAE:402.140232\n* Link to paper:-https://scholar.google.com/citations?view_op=view_citation&hl=en&user=yO6rbN8AAAAJ&citation_for_view=yO6rbN8AAAAJ:ULOm3_A8WrAC","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/personal-dataset/sensex_paper3.csv\")\n#data=data[::-1]\ndata.reset_index(drop=True, inplace=True)\ndata.nunique()\ndata.head(),data.dtypes,data.isnull().sum()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:02:46.409878Z","iopub.execute_input":"2025-03-01T10:02:46.410223Z","iopub.status.idle":"2025-03-01T10:02:46.486288Z","shell.execute_reply.started":"2025-03-01T10:02:46.410201Z","shell.execute_reply":"2025-03-01T10:02:46.485501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Load CSV\ndf = pd.read_csv(\"/kaggle/input/personal-dataset/sensex_paper3.csv\")  # Replace with actual filename\n\n# Strip spaces and convert Change % properly\ndf = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n\n# Convert numeric columns (except 'Vol.') to float\nnumeric_cols = [\"Close\", \"Open\", \"High\", \"Low\"]\nfor col in numeric_cols:\n    df[col] = pd.to_numeric(df[col].str.replace(',', '', regex=True), errors='coerce')\ndef clean_change(x):\n    if isinstance(x, str):\n        # Remove the '%' symbol (only at the end, if present)\n        if x.endswith('%'):\n            x = x[:-1]\n        return x.strip()\n    return x\n\ndf[\"Change %\"] = df[\"Change %\"].apply(clean_change)\ndf[\"Change %\"] = pd.to_numeric(df[\"Change %\"], errors='coerce')\n\n\n# Convert 'Vol.' column while keeping important data\ndef convert_volume(val):\n    if pd.isna(val) or val == '-':  # Handle NaN or '-' cases\n        return np.nan\n    val = val.strip().replace(',', '')  # Remove spaces & commas\n    if val[-1] == 'K': return float(val[:-1]) * 1e3\n    if val[-1] == 'M': return float(val[:-1]) * 1e6\n    if val[-1] == 'B': return float(val[:-1]) * 1e9\n    return float(val)\n\ndf[\"Vol.\"] = df[\"Vol.\"].apply(convert_volume)\n\n# Drop Date column\ndf.drop(columns=[\"Date\",\"Vol.\",\"Change %\"], inplace=True)\n\n# Forward fill missing 'Vol.' values to retain important data\n#df[\"Vol.\"].fillna(method='ffill', inplace=True)\n\n# Drop rows where 'Close' is still NaN (essential for LSTM training)\ndf.dropna(subset=[\"Close\"], inplace=True)\n\n# Final check\nprint(df.info())  # Ensure all columns are numeric\nprint(df.head())  # Verify cleaned data\ndata=df\ndata.isnull().sum(),data.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:02:46.488612Z","iopub.execute_input":"2025-03-01T10:02:46.488837Z","iopub.status.idle":"2025-03-01T10:02:46.61325Z","shell.execute_reply.started":"2025-03-01T10:02:46.488818Z","shell.execute_reply":"2025-03-01T10:02:46.612475Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport tensorflow as tf\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# ---------------------------\n# 1. Set Seed for Reproducibility\n# ---------------------------\ndef set_random_seed(seed=20):\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    tf.config.experimental.enable_op_determinism()\n\nset_random_seed(20)\n\n# ---------------------------\n# 2. Load & Preprocess Data\n# ---------------------------\n# For demonstration, we use a basic univariate \"Close\" series.\n# Ensure that your DataFrame 'data' contains at least a \"Close\" column.\ndf = data[[\"Close\"]].copy()\nscaler = MinMaxScaler()\ndf[\"Close\"] = scaler.fit_transform(df)\n\n# ---------------------------\n# 3. Create 15-day Sequences\n# ---------------------------\ndef create_sequences(data, time_steps=15):\n    X, y = [], []\n    for i in range(len(data) - time_steps):\n        X.append(data[i : i + time_steps])\n        y.append(data[i + time_steps])\n    return np.array(X), np.array(y)\n\nX, y = create_sequences(df[\"Close\"].values, time_steps=15)\n\n# Split data: 70% train, 10% validation, 20% test.\ntrain_size = int(len(X) * 0.7)\nval_size = int(len(X) * 0.1)\nX_train, y_train = X[:train_size], y[:train_size]\nX_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\nX_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n\n# Reshape each sample to (time_steps, 1)\nX_train = X_train.reshape(-1, 15, 1)\nX_val   = X_val.reshape(-1, 15, 1)\nX_test  = X_test.reshape(-1, 15, 1)\n\n# ---------------------------\n# 4. Build Multi-Scale Hybrid Model\n# ---------------------------\nfrom tensorflow.keras.layers import Multiply, Activation\n\ndef build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n    inputs = Input(shape=(seq_length, 1))\n\n    # *Branch 1: Full 15-Day Analysis (Long-Term Trends)*\n    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n    full_branch = Dense(64, activation='relu')(full_branch)\n    long_term_return = Dense(1, activation='tanh')(full_branch)\n    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n\n    # *Branch 2: Recent 7-Day Analysis (Mid-Term Trends)*\n    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)  # Take the last 7 days\n    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n    recent_branch = Dense(64, activation='relu')(recent_branch)\n    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n\n    # *Branch 3: Very Recent 1-Day Analysis (Short-Term Trends)*\n    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)  # Take the last day's price\n    very_recent_branch = Flatten()(very_recent_input)\n    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n\n    # *Gating Mechanism (Forget Gate)*\n    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)  # Gate for 7-day impact\n    gate_1 = Dense(1, activation='sigmoid')(short_term_return)  # Gate for 1-day impact\n\n    gated_mid_term_return = Multiply()([mid_term_return, gate_7])  # 7-day result scaled by gate\n    gated_short_term_return = Multiply()([short_term_return, gate_1])  # 1-day result scaled by gate\n\n    # *Combine Long-Term, Gated Mid-Term, and Gated Short-Term Predictions*\n    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n    fused_returns = Dense(32, activation='relu')(fused_returns)\n    final_return = Dense(1, activation='tanh')(fused_returns)\n    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n\n    # *Use the last day's price for the final output calculation*\n    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, 0], axis=-1))(inputs)\n    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n\n    model = Model(inputs, final_output)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n\n    return model\n\n# *Build the Modified Model with Gating*\n# model_gated = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n# model_gated.summary()\n\n# *Build the Modified Model with Attention-Based Residuals*\nmodel_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\nmodel_attention.summary()\nearly_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\nhistory = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n\n# ---------------------------\n# 7. Evaluate the Model\n# ---------------------------\ny_pred = model_attention.predict(X_test)\ny_pred_inv = scaler.inverse_transform(y_pred)\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n\nr2 = r2_score(y_test_inv, y_pred_inv)\nrmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\nmae = mean_absolute_error(y_test_inv, y_pred_inv)\nmape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\nevs = explained_variance_score(y_test_inv, y_pred_inv)\n\nprint(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n\nplt.figure(figsize=(10,5))\nplt.plot(y_test_inv, label='Actual')\nplt.plot(y_pred_inv, label='Predicted')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Close Price\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:02:46.614682Z","iopub.execute_input":"2025-03-01T10:02:46.614941Z","iopub.status.idle":"2025-03-01T10:06:02.83021Z","shell.execute_reply.started":"2025-03-01T10:02:46.614921Z","shell.execute_reply":"2025-03-01T10:06:02.829277Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* Now we try to add another parameter,namely the India VIX\n* However the VIX index ranges from Only 4th March 2008,so we have to run this model on a subset of the data to check our results\n* the data we try our model on thus ranges from 04-03-2008 to 30-04-2021 ","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/historical-india-stock-market/BSE Sensex 30 Historical Data.csv\")\ndata=data[::-1]\ndata.reset_index(drop=True, inplace=True)\ndata.head()\ndata.nunique()\n\ndata.sort_index(axis=1,ascending=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True)\ndf = data.copy()  # Ensure we don't modify the original dataset\n\n# Convert financial columns to numeric (remove commas)\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\ndef convert_volume(vol):\n    if isinstance(vol, str):  # Ensure it's a string before replacing\n        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n        if \"B\" in vol:\n            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n        elif \"M\" in vol:\n            return float(vol.replace(\"M\", \"\")) * 1_000_000\n        elif \"K\" in vol:\n            return float(vol.replace(\"K\", \"\")) * 1_000\n    return float(vol)  # Convert directly if already a number\n\ndf[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n\n# Convert 'Change %' column (remove '%' and convert to float)\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Print final DataFrame\nprint(df.dtypes)\nprint(df)\n\n# Assign back to 'data' (if needed)\ndata = df\n\n#data = pd.merge(data, vix, on='Date', how='inner')\n#data.drop(['Date'], axis=1, inplace=True)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:06:02.831064Z","iopub.execute_input":"2025-03-01T10:06:02.831546Z","iopub.status.idle":"2025-03-01T10:06:02.900932Z","shell.execute_reply.started":"2025-03-01T10:06:02.831521Z","shell.execute_reply":"2025-03-01T10:06:02.900086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vix = pd.read_csv(\"/kaggle/input/vix-data/vix.csv\")\nvix.drop(\"Vol.\",axis=1,inplace=True)\nvix[\"Change %\"] = vix[\"Change %\"].str.replace(\"%\", \"\").astype(float)\nvix.head(),vix.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:06:02.901787Z","iopub.execute_input":"2025-03-01T10:06:02.902072Z","iopub.status.idle":"2025-03-01T10:06:02.924338Z","shell.execute_reply.started":"2025-03-01T10:06:02.902038Z","shell.execute_reply":"2025-03-01T10:06:02.923669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.merge(data, vix, on='Date', how='inner')\ndata.drop(\"Date\",axis=1,inplace=True)\nprint(data.head())\nprint(len(data))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:06:02.925098Z","iopub.execute_input":"2025-03-01T10:06:02.925311Z","iopub.status.idle":"2025-03-01T10:06:02.945109Z","shell.execute_reply.started":"2025-03-01T10:06:02.925291Z","shell.execute_reply":"2025-03-01T10:06:02.944261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport tensorflow as tf\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# ---------------------------\n# 1. Set Seed for Reproducibility\n# ---------------------------\ndef set_random_seed(seed=20):\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    tf.config.experimental.enable_op_determinism()\n\nset_random_seed(20)\n\n# ---------------------------\n# 2. Load & Preprocess Data\n# ---------------------------\n# For demonstration, we use a basic univariate \"Close\" series.\n# Ensure that your DataFrame 'data' contains at least a \"Close\" column.\ndf = data[[\"Close\"]].copy()\nscaler = MinMaxScaler()\ndf[\"Close\"] = scaler.fit_transform(df)\n\n# ---------------------------\n# 3. Create 15-day Sequences\n# ---------------------------\ndef create_sequences(data, time_steps=15):\n    X, y = [], []\n    for i in range(len(data) - time_steps):\n        X.append(data[i : i + time_steps])\n        y.append(data[i + time_steps])\n    return np.array(X), np.array(y)\n\nX, y = create_sequences(df[\"Close\"].values, time_steps=15)\n\n# Split data: 70% train, 10% validation, 20% test.\ntrain_size = int(len(X) * 0.7)\nval_size = int(len(X) * 0.1)\nX_train, y_train = X[:train_size], y[:train_size]\nX_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\nX_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n\n# Reshape each sample to (time_steps, 1)\nX_train = X_train.reshape(-1, 15, 1)\nX_val   = X_val.reshape(-1, 15, 1)\nX_test  = X_test.reshape(-1, 15, 1)\n\n# ---------------------------\n# 4. Build Multi-Scale Hybrid Model\n# ---------------------------\nfrom tensorflow.keras.layers import Multiply, Activation\n\ndef build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n    inputs = Input(shape=(seq_length, 1))\n\n    # *Branch 1: Full 15-Day Analysis (Long-Term Trends)*\n    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n    full_branch = Dense(64, activation='relu')(full_branch)\n    long_term_return = Dense(1, activation='tanh')(full_branch)\n    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n\n    # *Branch 2: Recent 7-Day Analysis (Mid-Term Trends)*\n    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)  # Take the last 7 days\n    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n    recent_branch = Dense(64, activation='relu')(recent_branch)\n    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n\n    # *Branch 3: Very Recent 1-Day Analysis (Short-Term Trends)*\n    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)  # Take the last day's price\n    very_recent_branch = Flatten()(very_recent_input)\n    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n\n    # *Gating Mechanism (Forget Gate)*\n    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)  # Gate for 7-day impact\n    gate_1 = Dense(1, activation='sigmoid')(short_term_return)  # Gate for 1-day impact\n\n    gated_mid_term_return = Multiply()([mid_term_return, gate_7])  # 7-day result scaled by gate\n    gated_short_term_return = Multiply()([short_term_return, gate_1])  # 1-day result scaled by gate\n\n    # *Combine Long-Term, Gated Mid-Term, and Gated Short-Term Predictions*\n    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n    fused_returns = Dense(32, activation='relu')(fused_returns)\n    final_return = Dense(1, activation='tanh')(fused_returns)\n    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n\n    # *Use the last day's price for the final output calculation*\n    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, 0], axis=-1))(inputs)\n    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n\n    model = Model(inputs, final_output)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n\n    return model\n\n# *Build the Modified Model with Gating*\n# model_gated = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n# model_gated.summary()\n\n# *Build the Modified Model with Attention-Based Residuals*\nmodel_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\nmodel_attention.summary()\nearly_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\nhistory = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n\n# ---------------------------\n# 7. Evaluate the Model\n# ---------------------------\ny_pred = model_attention.predict(X_test)\ny_pred_inv = scaler.inverse_transform(y_pred)\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n\nr2 = r2_score(y_test_inv, y_pred_inv)\nrmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\nmae = mean_absolute_error(y_test_inv, y_pred_inv)\nmape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\nevs = explained_variance_score(y_test_inv, y_pred_inv)\n\nprint(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n\nplt.figure(figsize=(10,5))\nplt.plot(y_test_inv, label='Actual')\nplt.plot(y_pred_inv, label='Predicted')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Close Price\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:06:02.946105Z","iopub.execute_input":"2025-03-01T10:06:02.946394Z","iopub.status.idle":"2025-03-01T10:07:41.200862Z","shell.execute_reply.started":"2025-03-01T10:06:02.946359Z","shell.execute_reply":"2025-03-01T10:07:41.200167Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Working with Paper 2\n* This paper does prediction on BSE SENSEX data\n* Data spans the dates of May 30, 2010, and February 9, 2018\n* It uses RMSE and MSE metric\n* Their best result is MSE-0.0021 RMSE-0.0438\n* Paper link:-https://ieeexplore.ieee.org/document/10397684","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the dataset\nvix = pd.read_csv(\"/kaggle/input/vix-data/vix.csv\")\n\n# Print initial number of rows\ninitial_count = len(vix)\nprint(f\"Initial number of rows: {initial_count}\")\n\n# Drop the \"Vol.\" column\nvix.drop(\"Vol.\", axis=1, inplace=True)\n\n# Convert \"Change %\" column to float after removing \"%\"\nvix[\"Change %\"] = vix[\"Change %\"].str.replace(\"%\", \"\").astype(float)\n\n# Convert \"Date\" column to datetime format\nvix[\"Date\"] = pd.to_datetime(vix[\"Date\"])\n\n# Filter data within the specified date range\nstart_date = \"2010-05-30\"\nend_date = \"2018-02-09\"\nvix = vix[(vix[\"Date\"] >= start_date) & (vix[\"Date\"] <= end_date)]\n\n# Print final number of rows\nfinal_count = len(vix)\nprint(f\"Final number of rows after filtering: {final_count}\")\n\n# Display results\nvix.head(), vix.dtypes\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:07:41.201402Z","iopub.execute_input":"2025-03-01T10:07:41.201627Z","iopub.status.idle":"2025-03-01T10:07:41.239486Z","shell.execute_reply.started":"2025-03-01T10:07:41.201606Z","shell.execute_reply":"2025-03-01T10:07:41.238687Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load Sensex dataset\ndata = pd.read_csv(\"/kaggle/input/d/abirc8010/historical-india-stock-market/BSE Sensex 30 Historical Data.csv\")\n\n# Print initial number of rows\ninitial_count = len(data)\nprint(f\"Initial number of rows: {initial_count}\")\n\n# Reverse and reset index\ndata = data[::-1].reset_index(drop=True)\n\n# Sort columns alphabetically and rename 'Price' to 'Close'\ndata.sort_index(axis=1, ascending=True, inplace=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True)\n\n# Create a copy to avoid modifying the original data\ndf = data.copy()\n\n# Convert financial columns to numeric\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\ndef convert_volume(vol):\n    if isinstance(vol, str):\n        vol = vol.replace(\",\", \"\")\n        if \"B\" in vol:\n            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n        elif \"M\" in vol:\n            return float(vol.replace(\"M\", \"\")) * 1_000_000\n        elif \"K\" in vol:\n            return float(vol.replace(\"K\", \"\")) * 1_000\n    return float(vol)\n\ndf[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n\n# Convert 'Change %' column\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Convert 'Date' column to datetime format, handling mixed formats\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"], dayfirst=True, errors=\"coerce\")\n\n# Filter Sensex data within the specified date range\nstart_date = \"2010-05-30\"\nend_date = \"2018-02-09\"\ndf = df[(df[\"Date\"] >= start_date) & (df[\"Date\"] <= end_date)]\n\n# Print final number of rows\nfinal_count = len(df)\nprint(f\"Final number of rows after filtering: {final_count}\")\n\n# Assign back to 'data'\ndata = df\n\n# Merge with VIX dataset\ndata = pd.merge(data, vix, on=\"Date\", how=\"inner\")\ndata.drop([\"Date\"], axis=1, inplace=True)\n\n# Print number of rows after merging\nprint(f\"Number of rows after merging: {len(data)}\")\n\n# Display results\nprint(data.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:07:41.240214Z","iopub.execute_input":"2025-03-01T10:07:41.24043Z","iopub.status.idle":"2025-03-01T10:07:41.308807Z","shell.execute_reply.started":"2025-03-01T10:07:41.240411Z","shell.execute_reply":"2025-03-01T10:07:41.308032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport tensorflow as tf\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# ---------------------------\n# 1. Set Seed for Reproducibility\n# ---------------------------\ndef set_random_seed(seed=20):\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    tf.config.experimental.enable_op_determinism()\n\nset_random_seed(20)\n\n# ---------------------------\n# 2. Load & Preprocess Data\n# ---------------------------\n# For demonstration, we use a basic univariate \"Close\" series.\n# Ensure that your DataFrame 'data' contains at least a \"Close\" column.\ndf = data[[\"Close\"]].copy()\nscaler = MinMaxScaler()\ndf[\"Close\"] = scaler.fit_transform(df)\n\n# ---------------------------\n# 3. Create 15-day Sequences\n# ---------------------------\ndef create_sequences(data, time_steps=15):\n    X, y = [], []\n    for i in range(len(data) - time_steps):\n        X.append(data[i : i + time_steps])\n        y.append(data[i + time_steps])\n    return np.array(X), np.array(y)\n\nX, y = create_sequences(df[\"Close\"].values, time_steps=15)\n\n# Split data: 70% train, 10% validation, 20% test.\ntrain_size = int(len(X) * 0.7)\nval_size = int(len(X) * 0.1)\nX_train, y_train = X[:train_size], y[:train_size]\nX_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\nX_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n\n# Reshape each sample to (time_steps, 1)\nX_train = X_train.reshape(-1, 15, 1)\nX_val   = X_val.reshape(-1, 15, 1)\nX_test  = X_test.reshape(-1, 15, 1)\n\n# ---------------------------\n# 4. Build Multi-Scale Hybrid Model\n# ---------------------------\nfrom tensorflow.keras.layers import Multiply, Activation\n\ndef build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n    inputs = Input(shape=(seq_length, 1))\n\n    # *Branch 1: Full 15-Day Analysis (Long-Term Trends)*\n    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n    full_branch = Dense(64, activation='relu')(full_branch)\n    long_term_return = Dense(1, activation='tanh')(full_branch)\n    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n\n    # *Branch 2: Recent 7-Day Analysis (Mid-Term Trends)*\n    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)  # Take the last 7 days\n    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n    recent_branch = Dense(64, activation='relu')(recent_branch)\n    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n\n    # *Branch 3: Very Recent 1-Day Analysis (Short-Term Trends)*\n    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)  # Take the last day's price\n    very_recent_branch = Flatten()(very_recent_input)\n    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n\n    # *Gating Mechanism (Forget Gate)*\n    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)  # Gate for 7-day impact\n    gate_1 = Dense(1, activation='sigmoid')(short_term_return)  # Gate for 1-day impact\n\n    gated_mid_term_return = Multiply()([mid_term_return, gate_7])  # 7-day result scaled by gate\n    gated_short_term_return = Multiply()([short_term_return, gate_1])  # 1-day result scaled by gate\n\n    # *Combine Long-Term, Gated Mid-Term, and Gated Short-Term Predictions*\n    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n    fused_returns = Dense(32, activation='relu')(fused_returns)\n    final_return = Dense(1, activation='tanh')(fused_returns)\n    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n\n    # *Use the last day's price for the final output calculation*\n    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, 0], axis=-1))(inputs)\n    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n\n    model = Model(inputs, final_output)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n\n    return model\n\n# *Build the Modified Model with Gating*\n# model_gated = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n# model_gated.summary()\n\n# *Build the Modified Model with Attention-Based Residuals*\nmodel_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\nmodel_attention.summary()\nearly_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\nhistory = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n\n# ---------------------------\n# 7. Evaluate the Model\n# ---------------------------\ny_pred = model_attention.predict(X_test)\ny_pred_inv = scaler.inverse_transform(y_pred)\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n\nr2 = r2_score(y_test_inv, y_pred_inv)\nrmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\nmae = mean_absolute_error(y_test_inv, y_pred_inv)\nmape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\nevs = explained_variance_score(y_test_inv, y_pred_inv)\n\nprint(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n\nplt.figure(figsize=(10,5))\nplt.plot(y_test_inv, label='Actual')\nplt.plot(y_pred_inv, label='Predicted')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Close Price\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:07:41.3097Z","iopub.execute_input":"2025-03-01T10:07:41.309938Z","iopub.status.idle":"2025-03-01T10:08:21.561101Z","shell.execute_reply.started":"2025-03-01T10:07:41.309918Z","shell.execute_reply":"2025-03-01T10:08:21.560186Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## trying without vix","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load Sensex dataset\ndata = pd.read_csv(\"/kaggle/input/d/abirc8010/historical-india-stock-market/BSE Sensex 30 Historical Data.csv\")\n\n# Print initial number of rows\ninitial_count = len(data)\nprint(f\"Initial number of rows: {initial_count}\")\n\n# Reverse and reset index\ndata = data[::-1].reset_index(drop=True)\n\n# Sort columns alphabetically and rename 'Price' to 'Close'\ndata.sort_index(axis=1, ascending=True, inplace=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True)\n\n# Create a copy to avoid modifying the original data\ndf = data.copy()\n\n# Convert financial columns to numeric\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\ndef convert_volume(vol):\n    if isinstance(vol, str):\n        vol = vol.replace(\",\", \"\")\n        if \"B\" in vol:\n            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n        elif \"M\" in vol:\n            return float(vol.replace(\"M\", \"\")) * 1_000_000\n        elif \"K\" in vol:\n            return float(vol.replace(\"K\", \"\")) * 1_000\n    return float(vol)\n\ndf[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n\n# Convert 'Change %' column\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Convert 'Date' column to datetime format, handling mixed formats\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"], dayfirst=True, errors=\"coerce\")\n\n# Filter Sensex data within the specified date range\nstart_date = \"2010-05-30\"\nend_date = \"2018-02-09\"\ndf = df[(df[\"Date\"] >= start_date) & (df[\"Date\"] <= end_date)]\n\n# Print final number of rows\nfinal_count = len(df)\nprint(f\"Final number of rows after filtering: {final_count}\")\n\n# Assign back to 'data'\ndata = df\n\n# Drop 'Date' column\ndata.drop([\"Date\"], axis=1, inplace=True)\n\n# Print final dataset\nprint(data.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:08:21.562063Z","iopub.execute_input":"2025-03-01T10:08:21.562387Z","iopub.status.idle":"2025-03-01T10:08:21.626391Z","shell.execute_reply.started":"2025-03-01T10:08:21.562355Z","shell.execute_reply":"2025-03-01T10:08:21.625607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport tensorflow as tf\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# ---------------------------\n# 1. Set Seed for Reproducibility\n# ---------------------------\ndef set_random_seed(seed=20):\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    tf.config.experimental.enable_op_determinism()\n\nset_random_seed(20)\n\n# ---------------------------\n# 2. Load & Preprocess Data\n# ---------------------------\n# For demonstration, we use a basic univariate \"Close\" series.\n# Ensure that your DataFrame 'data' contains at least a \"Close\" column.\ndf = data[[\"Close\"]].copy()\nscaler = MinMaxScaler()\ndf[\"Close\"] = scaler.fit_transform(df)\n\n# ---------------------------\n# 3. Create 15-day Sequences\n# ---------------------------\ndef create_sequences(data, time_steps=15):\n    X, y = [], []\n    for i in range(len(data) - time_steps):\n        X.append(data[i : i + time_steps])\n        y.append(data[i + time_steps])\n    return np.array(X), np.array(y)\n\nX, y = create_sequences(df[\"Close\"].values, time_steps=15)\n\n# Split data: 70% train, 10% validation, 20% test.\ntrain_size = int(len(X) * 0.7)\nval_size = int(len(X) * 0.1)\nX_train, y_train = X[:train_size], y[:train_size]\nX_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\nX_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n\n# Reshape each sample to (time_steps, 1)\nX_train = X_train.reshape(-1, 15, 1)\nX_val   = X_val.reshape(-1, 15, 1)\nX_test  = X_test.reshape(-1, 15, 1)\n\n# ---------------------------\n# 4. Build Multi-Scale Hybrid Model\n# ---------------------------\nfrom tensorflow.keras.layers import Multiply, Activation\n\ndef build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n    inputs = Input(shape=(seq_length, 1))\n\n    # *Branch 1: Full 15-Day Analysis (Long-Term Trends)*\n    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n    full_branch = Dense(64, activation='relu')(full_branch)\n    long_term_return = Dense(1, activation='tanh')(full_branch)\n    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n\n    # *Branch 2: Recent 7-Day Analysis (Mid-Term Trends)*\n    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)  # Take the last 7 days\n    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n    recent_branch = Dense(64, activation='relu')(recent_branch)\n    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n\n    # *Branch 3: Very Recent 1-Day Analysis (Short-Term Trends)*\n    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)  # Take the last day's price\n    very_recent_branch = Flatten()(very_recent_input)\n    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n\n    # *Gating Mechanism (Forget Gate)*\n    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)  # Gate for 7-day impact\n    gate_1 = Dense(1, activation='sigmoid')(short_term_return)  # Gate for 1-day impact\n\n    gated_mid_term_return = Multiply()([mid_term_return, gate_7])  # 7-day result scaled by gate\n    gated_short_term_return = Multiply()([short_term_return, gate_1])  # 1-day result scaled by gate\n\n    # *Combine Long-Term, Gated Mid-Term, and Gated Short-Term Predictions*\n    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n    fused_returns = Dense(32, activation='relu')(fused_returns)\n    final_return = Dense(1, activation='tanh')(fused_returns)\n    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n\n    # *Use the last day's price for the final output calculation*\n    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, 0], axis=-1))(inputs)\n    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n\n    model = Model(inputs, final_output)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n\n    return model\n\n# *Build the Modified Model with Gating*\n# model_gated = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n# model_gated.summary()\n\n# *Build the Modified Model with Attention-Based Residuals*\nmodel_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\nmodel_attention.summary()\nearly_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\nhistory = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n\n# ---------------------------\n# 7. Evaluate the Model\n# ---------------------------\ny_pred = model_attention.predict(X_test)\ny_pred_inv = scaler.inverse_transform(y_pred)\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n\nr2 = r2_score(y_test_inv, y_pred_inv)\nrmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\nmae = mean_absolute_error(y_test_inv, y_pred_inv)\nmape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\nevs = explained_variance_score(y_test_inv, y_pred_inv)\n\nprint(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n\nplt.figure(figsize=(10,5))\nplt.plot(y_test_inv, label='Actual')\nplt.plot(y_pred_inv, label='Predicted')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Close Price\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:08:21.627379Z","iopub.execute_input":"2025-03-01T10:08:21.627686Z","iopub.status.idle":"2025-03-01T10:10:26.565869Z","shell.execute_reply.started":"2025-03-01T10:08:21.627655Z","shell.execute_reply":"2025-03-01T10:10:26.564968Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"r2 = r2_score(y_test_inv, y_pred_inv)\nmae = mean_absolute_error(y_test_inv, y_pred_inv)\nmse = mean_squared_error(y_test_inv, y_pred_inv)\nrmse = np.sqrt(mse)  # Root Mean Squared Error\n\n# Calculate the actual range of the test set\nactual_range = np.max(y_test_inv) - np.min(y_test_inv)\n\n# Compute Scaled Metrics\nmae_scaled = mae / actual_range\nmse_scaled = mse / (actual_range ** 2)\nrmse_scaled = rmse / actual_range\n\n# Explained Variance Score\nevs = explained_variance_score(y_test_inv, y_pred_inv) \n\n# Print the results\nprint(f\"ğŸ“Š Final Results -\")\nprint(f\"  RÂ²: {r2:.4f}\")\nprint(f\"  MAE: {mae:.4f}, MAE Scaled: {mae_scaled:.4f}\")\nprint(f\"  MSE: {mse:.4f}, MSE Scaled: {mse_scaled:.4f}\")\nprint(f\"  RMSE: {rmse:.4f}, RMSE Scaled: {rmse_scaled:.4f}\")\nprint(f\"  MAPE: {mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100:.2f}%\")\nprint(f\"  EVS: {evs:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:10:26.566928Z","iopub.execute_input":"2025-03-01T10:10:26.567204Z","iopub.status.idle":"2025-03-01T10:10:26.577343Z","shell.execute_reply.started":"2025-03-01T10:10:26.567182Z","shell.execute_reply":"2025-03-01T10:10:26.576643Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Working with Paper 3\n* This paper does prediction on Nifty50 and Sensex\n* The data is collected daily from 2013 to 2022\n* The metric used is RMSE\n* Their result is NIFTY50:-RMSE - 170.843,SENSEX:-RMSE - 578.746\n* Paper link:-https://www.sciencedirect.com/science/article/pii/S1568494624005337","metadata":{}},{"cell_type":"code","source":"vix = pd.read_csv(\"/kaggle/input/personal-dateset-vix/vix paper1.csv\")\nvix.drop(\"Vol.\",axis=1,inplace=True)\nvix[\"Change %\"] = vix[\"Change %\"].str.replace(\"%\", \"\").astype(float)\nvix.head(),vix.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:10:26.57819Z","iopub.execute_input":"2025-03-01T10:10:26.578462Z","iopub.status.idle":"2025-03-01T10:10:26.621312Z","shell.execute_reply.started":"2025-03-01T10:10:26.57843Z","shell.execute_reply":"2025-03-01T10:10:26.620519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/personal-dataset/Sensex Paper1.csv\")\ndata=data[::-1]\ndata.reset_index(drop=True, inplace=True)\ndata.head()\ndata.nunique()\n\ndata.sort_index(axis=1,ascending=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True)\ndf = data.copy()  # Ensure we don't modify the original dataset\n\n# Convert financial columns to numeric (remove commas)\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\ndef convert_volume(vol):\n    if isinstance(vol, str):  # Ensure it's a string before replacing\n        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n        if \"B\" in vol:\n            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n        elif \"M\" in vol:\n            return float(vol.replace(\"M\", \"\")) * 1_000_000\n        elif \"K\" in vol:\n            return float(vol.replace(\"K\", \"\")) * 1_000\n    return float(vol)  # Convert directly if already a number\n\ndf[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n\n# Convert 'Change %' column (remove '%' and convert to float)\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Print final DataFrame\nprint(df.dtypes)\nprint(df)\n\n# Assign back to 'data' (if needed)\ndata = df\n\ndata = pd.merge(data, vix, on='Date', how='inner')\ndata.drop(['Date'], axis=1, inplace=True)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:10:26.6222Z","iopub.execute_input":"2025-03-01T10:10:26.622507Z","iopub.status.idle":"2025-03-01T10:10:26.715061Z","shell.execute_reply.started":"2025-03-01T10:10:26.622476Z","shell.execute_reply":"2025-03-01T10:10:26.714412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport tensorflow as tf\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# ---------------------------\n# 1. Set Seed for Reproducibility\n# ---------------------------\ndef set_random_seed(seed=20):\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    tf.config.experimental.enable_op_determinism()\n\nset_random_seed(20)\n\n# ---------------------------\n# 2. Load & Preprocess Data\n# ---------------------------\n# For demonstration, we use a basic univariate \"Close\" series.\n# Ensure that your DataFrame 'data' contains at least a \"Close\" column.\ndf = data[[\"Close\"]].copy()\nscaler = MinMaxScaler()\ndf[\"Close\"] = scaler.fit_transform(df)\n\n# ---------------------------\n# 3. Create 15-day Sequences\n# ---------------------------\ndef create_sequences(data, time_steps=15):\n    X, y = [], []\n    for i in range(len(data) - time_steps):\n        X.append(data[i : i + time_steps])\n        y.append(data[i + time_steps])\n    return np.array(X), np.array(y)\n\nX, y = create_sequences(df[\"Close\"].values, time_steps=15)\n\n# Split data: 70% train, 10% validation, 20% test.\ntrain_size = int(len(X) * 0.7)\nval_size = int(len(X) * 0.1)\nX_train, y_train = X[:train_size], y[:train_size]\nX_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\nX_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n\n# Reshape each sample to (time_steps, 1)\nX_train = X_train.reshape(-1, 15, 1)\nX_val   = X_val.reshape(-1, 15, 1)\nX_test  = X_test.reshape(-1, 15, 1)\n\n# ---------------------------\n# 4. Build Multi-Scale Hybrid Model\n# ---------------------------\nfrom tensorflow.keras.layers import Multiply, Activation\n\ndef build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n    inputs = Input(shape=(seq_length, 1))\n\n    # *Branch 1: Full 15-Day Analysis (Long-Term Trends)*\n    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n    full_branch = Dense(64, activation='relu')(full_branch)\n    long_term_return = Dense(1, activation='tanh')(full_branch)\n    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n\n    # *Branch 2: Recent 7-Day Analysis (Mid-Term Trends)*\n    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)  # Take the last 7 days\n    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n    recent_branch = Dense(64, activation='relu')(recent_branch)\n    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n\n    # *Branch 3: Very Recent 1-Day Analysis (Short-Term Trends)*\n    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)  # Take the last day's price\n    very_recent_branch = Flatten()(very_recent_input)\n    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n\n    # *Gating Mechanism (Forget Gate)*\n    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)  # Gate for 7-day impact\n    gate_1 = Dense(1, activation='sigmoid')(short_term_return)  # Gate for 1-day impact\n\n    gated_mid_term_return = Multiply()([mid_term_return, gate_7])  # 7-day result scaled by gate\n    gated_short_term_return = Multiply()([short_term_return, gate_1])  # 1-day result scaled by gate\n\n    # *Combine Long-Term, Gated Mid-Term, and Gated Short-Term Predictions*\n    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n    fused_returns = Dense(32, activation='relu')(fused_returns)\n    final_return = Dense(1, activation='tanh')(fused_returns)\n    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n\n    # *Use the last day's price for the final output calculation*\n    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, 0], axis=-1))(inputs)\n    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n\n    model = Model(inputs, final_output)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n\n    return model\n\n# *Build the Modified Model with Gating*\n# model_gated = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n# model_gated.summary()\n\n# *Build the Modified Model with Attention-Based Residuals*\nmodel_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\nmodel_attention.summary()\nearly_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\nhistory = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n\n# ---------------------------\n# 7. Evaluate the Model\n# ---------------------------\ny_pred = model_attention.predict(X_test)\ny_pred_inv = scaler.inverse_transform(y_pred)\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n\nr2 = r2_score(y_test_inv, y_pred_inv)\nrmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\nmae = mean_absolute_error(y_test_inv, y_pred_inv)\nmape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\nevs = explained_variance_score(y_test_inv, y_pred_inv)\n\nprint(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n\nplt.figure(figsize=(10,5))\nplt.plot(y_test_inv, label='Actual')\nplt.plot(y_pred_inv, label='Predicted')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Close Price\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:10:26.718305Z","iopub.execute_input":"2025-03-01T10:10:26.718503Z","iopub.status.idle":"2025-03-01T10:11:29.261147Z","shell.execute_reply.started":"2025-03-01T10:10:26.718486Z","shell.execute_reply":"2025-03-01T10:11:29.260275Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## doing without vix","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/personal-dataset/Sensex Paper1.csv\")\ndata=data[::-1]\ndata.reset_index(drop=True, inplace=True)\ndata.head()\ndata.nunique()\n\ndata.sort_index(axis=1,ascending=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True)\ndf = data.copy()  # Ensure we don't modify the original dataset\n\n# Convert financial columns to numeric (remove commas)\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\ndef convert_volume(vol):\n    if isinstance(vol, str):  # Ensure it's a string before replacing\n        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n        if \"B\" in vol:\n            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n        elif \"M\" in vol:\n            return float(vol.replace(\"M\", \"\")) * 1_000_000\n        elif \"K\" in vol:\n            return float(vol.replace(\"K\", \"\")) * 1_000\n    return float(vol)  # Convert directly if already a number\n\ndf[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n\n# Convert 'Change %' column (remove '%' and convert to float)\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Print final DataFrame\nprint(df.dtypes)\nprint(df)\n\n# Assign back to 'data' (if needed)\ndata = df\n\n#data = pd.merge(data, vix, on='Date', how='inner')\ndata.drop(['Date'], axis=1, inplace=True)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:11:29.262739Z","iopub.execute_input":"2025-03-01T10:11:29.26297Z","iopub.status.idle":"2025-03-01T10:11:29.307406Z","shell.execute_reply.started":"2025-03-01T10:11:29.26295Z","shell.execute_reply":"2025-03-01T10:11:29.30676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport tensorflow as tf\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# ---------------------------\n# 1. Set Seed for Reproducibility\n# ---------------------------\ndef set_random_seed(seed=20):\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    tf.config.experimental.enable_op_determinism()\n\nset_random_seed(20)\n\n# ---------------------------\n# 2. Load & Preprocess Data\n# ---------------------------\n# For demonstration, we use a basic univariate \"Close\" series.\n# Ensure that your DataFrame 'data' contains at least a \"Close\" column.\ndf = data[[\"Close\"]].copy()\nscaler = MinMaxScaler()\ndf[\"Close\"] = scaler.fit_transform(df)\n\n# ---------------------------\n# 3. Create 15-day Sequences\n# ---------------------------\ndef create_sequences(data, time_steps=15):\n    X, y = [], []\n    for i in range(len(data) - time_steps):\n        X.append(data[i : i + time_steps])\n        y.append(data[i + time_steps])\n    return np.array(X), np.array(y)\n\nX, y = create_sequences(df[\"Close\"].values, time_steps=15)\n\n# Split data: 70% train, 10% validation, 20% test.\ntrain_size = int(len(X) * 0.7)\nval_size = int(len(X) * 0.1)\nX_train, y_train = X[:train_size], y[:train_size]\nX_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\nX_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n\n# Reshape each sample to (time_steps, 1)\nX_train = X_train.reshape(-1, 15, 1)\nX_val   = X_val.reshape(-1, 15, 1)\nX_test  = X_test.reshape(-1, 15, 1)\n\n# ---------------------------\n# 4. Build Multi-Scale Hybrid Model\n# ---------------------------\nfrom tensorflow.keras.layers import Multiply, Activation\n\ndef build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n    inputs = Input(shape=(seq_length, 1))\n\n    # *Branch 1: Full 15-Day Analysis (Long-Term Trends)*\n    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n    full_branch = Dense(64, activation='relu')(full_branch)\n    long_term_return = Dense(1, activation='tanh')(full_branch)\n    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n\n    # *Branch 2: Recent 7-Day Analysis (Mid-Term Trends)*\n    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)  # Take the last 7 days\n    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n    recent_branch = Dense(64, activation='relu')(recent_branch)\n    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n\n    # *Branch 3: Very Recent 1-Day Analysis (Short-Term Trends)*\n    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)  # Take the last day's price\n    very_recent_branch = Flatten()(very_recent_input)\n    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n\n    # *Gating Mechanism (Forget Gate)*\n    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)  # Gate for 7-day impact\n    gate_1 = Dense(1, activation='sigmoid')(short_term_return)  # Gate for 1-day impact\n\n    gated_mid_term_return = Multiply()([mid_term_return, gate_7])  # 7-day result scaled by gate\n    gated_short_term_return = Multiply()([short_term_return, gate_1])  # 1-day result scaled by gate\n\n    # *Combine Long-Term, Gated Mid-Term, and Gated Short-Term Predictions*\n    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n    fused_returns = Dense(32, activation='relu')(fused_returns)\n    final_return = Dense(1, activation='tanh')(fused_returns)\n    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n\n    # *Use the last day's price for the final output calculation*\n    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, 0], axis=-1))(inputs)\n    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n\n    model = Model(inputs, final_output)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n\n    return model\n\n# *Build the Modified Model with Gating*\n# model_gated = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n# model_gated.summary()\n\n# *Build the Modified Model with Attention-Based Residuals*\nmodel_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\nmodel_attention.summary()\nearly_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\nhistory = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n\n# ---------------------------\n# 7. Evaluate the Model\n# ---------------------------\ny_pred = model_attention.predict(X_test)\ny_pred_inv = scaler.inverse_transform(y_pred)\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n\nr2 = r2_score(y_test_inv, y_pred_inv)\nrmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\nmae = mean_absolute_error(y_test_inv, y_pred_inv)\nmape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\nevs = explained_variance_score(y_test_inv, y_pred_inv)\n\nprint(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n\nplt.figure(figsize=(10,5))\nplt.plot(y_test_inv, label='Actual')\nplt.plot(y_pred_inv, label='Predicted')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Close Price\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:11:29.308131Z","iopub.execute_input":"2025-03-01T10:11:29.308351Z","iopub.status.idle":"2025-03-01T10:12:23.626784Z","shell.execute_reply.started":"2025-03-01T10:11:29.30832Z","shell.execute_reply":"2025-03-01T10:12:23.625947Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Working with Nifty50","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/personal-dataset/Nifty50 Paper1.csv\")\ndata=data[::-1]\ndata.reset_index(drop=True, inplace=True)\ndata.head()\ndata.nunique()\n\ndata.sort_index(axis=1,ascending=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True)\ndf = data.copy()  # Ensure we don't modify the original dataset\nvix = pd.read_csv(\"/kaggle/input/personal-dateset-vix/vix paper1.csv\")\nvix.drop(\"Vol.\",axis=1,inplace=True)\nvix[\"Change %\"] = vix[\"Change %\"].str.replace(\"%\", \"\").astype(float)\nvix.head(),vix.dtypes\n# Convert financial columns to numeric (remove commas)\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\ndef convert_volume(vol):\n    if isinstance(vol, str):  # Ensure it's a string before replacing\n        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n        if \"B\" in vol:\n            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n        elif \"M\" in vol:\n            return float(vol.replace(\"M\", \"\")) * 1_000_000\n        elif \"K\" in vol:\n            return float(vol.replace(\"K\", \"\")) * 1_000\n    return float(vol)  # Convert directly if already a number\n\ndf[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n\n# Convert 'Change %' column (remove '%' and convert to float)\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Print final DataFrame\nprint(df.dtypes)\nprint(df)\n\n# Assign back to 'data' (if needed)\ndata = df\n\ndata = pd.merge(data, vix, on='Date', how='inner')\ndata.drop(['Date'], axis=1, inplace=True)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:12:23.627699Z","iopub.execute_input":"2025-03-01T10:12:23.628047Z","iopub.status.idle":"2025-03-01T10:12:23.692821Z","shell.execute_reply.started":"2025-03-01T10:12:23.628014Z","shell.execute_reply":"2025-03-01T10:12:23.692174Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport tensorflow as tf\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# ---------------------------\n# 1. Set Seed for Reproducibility\n# ---------------------------\ndef set_random_seed(seed=20):\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    tf.config.experimental.enable_op_determinism()\n\nset_random_seed(20)\n\n# ---------------------------\n# 2. Load & Preprocess Data\n# ---------------------------\n# For demonstration, we use a basic univariate \"Close\" series.\n# Ensure that your DataFrame 'data' contains at least a \"Close\" column.\ndf = data[[\"Close\"]].copy()\nscaler = MinMaxScaler()\ndf[\"Close\"] = scaler.fit_transform(df)\n\n# ---------------------------\n# 3. Create 15-day Sequences\n# ---------------------------\ndef create_sequences(data, time_steps=15):\n    X, y = [], []\n    for i in range(len(data) - time_steps):\n        X.append(data[i : i + time_steps])\n        y.append(data[i + time_steps])\n    return np.array(X), np.array(y)\n\nX, y = create_sequences(df[\"Close\"].values, time_steps=15)\n\n# Split data: 70% train, 10% validation, 20% test.\ntrain_size = int(len(X) * 0.7)\nval_size = int(len(X) * 0.1)\nX_train, y_train = X[:train_size], y[:train_size]\nX_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\nX_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n\n# Reshape each sample to (time_steps, 1)\nX_train = X_train.reshape(-1, 15, 1)\nX_val   = X_val.reshape(-1, 15, 1)\nX_test  = X_test.reshape(-1, 15, 1)\n\n# ---------------------------\n# 4. Build Multi-Scale Hybrid Model\n# ---------------------------\nfrom tensorflow.keras.layers import Multiply, Activation\n\ndef build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n    inputs = Input(shape=(seq_length, 1))\n\n    # *Branch 1: Full 15-Day Analysis (Long-Term Trends)*\n    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n    full_branch = Dense(64, activation='relu')(full_branch)\n    long_term_return = Dense(1, activation='tanh')(full_branch)\n    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n\n    # *Branch 2: Recent 7-Day Analysis (Mid-Term Trends)*\n    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)  # Take the last 7 days\n    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n    recent_branch = Dense(64, activation='relu')(recent_branch)\n    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n\n    # *Branch 3: Very Recent 1-Day Analysis (Short-Term Trends)*\n    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)  # Take the last day's price\n    very_recent_branch = Flatten()(very_recent_input)\n    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n\n    # *Gating Mechanism (Forget Gate)*\n    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)  # Gate for 7-day impact\n    gate_1 = Dense(1, activation='sigmoid')(short_term_return)  # Gate for 1-day impact\n\n    gated_mid_term_return = Multiply()([mid_term_return, gate_7])  # 7-day result scaled by gate\n    gated_short_term_return = Multiply()([short_term_return, gate_1])  # 1-day result scaled by gate\n\n    # *Combine Long-Term, Gated Mid-Term, and Gated Short-Term Predictions*\n    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n    fused_returns = Dense(32, activation='relu')(fused_returns)\n    final_return = Dense(1, activation='tanh')(fused_returns)\n    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n\n    # *Use the last day's price for the final output calculation*\n    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, 0], axis=-1))(inputs)\n    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n\n    model = Model(inputs, final_output)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n\n    return model\n\n# *Build the Modified Model with Gating*\n# model_gated = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n# model_gated.summary()\n\n# *Build the Modified Model with Attention-Based Residuals*\nmodel_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\nmodel_attention.summary()\nearly_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\nhistory = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n\n# ---------------------------\n# 7. Evaluate the Model\n# ---------------------------\ny_pred = model_attention.predict(X_test)\ny_pred_inv = scaler.inverse_transform(y_pred)\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n\nr2 = r2_score(y_test_inv, y_pred_inv)\nrmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\nmae = mean_absolute_error(y_test_inv, y_pred_inv)\nmape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\nevs = explained_variance_score(y_test_inv, y_pred_inv)\n\nprint(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n\nplt.figure(figsize=(10,5))\nplt.plot(y_test_inv, label='Actual')\nplt.plot(y_pred_inv, label='Predicted')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Close Price\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:12:23.693503Z","iopub.execute_input":"2025-03-01T10:12:23.693726Z","iopub.status.idle":"2025-03-01T10:13:14.212258Z","shell.execute_reply.started":"2025-03-01T10:12:23.693694Z","shell.execute_reply":"2025-03-01T10:13:14.211385Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## doing without vix","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/personal-dataset/Nifty50 Paper1.csv\")\ndata=data[::-1]\ndata.reset_index(drop=True, inplace=True)\ndata.head()\ndata.nunique()\n\ndata.sort_index(axis=1,ascending=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True)\ndf = data.copy()  # Ensure we don't modify the original dataset\n\n# Convert financial columns to numeric (remove commas)\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\ndef convert_volume(vol):\n    if isinstance(vol, str):  # Ensure it's a string before replacing\n        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n        if \"B\" in vol:\n            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n        elif \"M\" in vol:\n            return float(vol.replace(\"M\", \"\")) * 1_000_000\n        elif \"K\" in vol:\n            return float(vol.replace(\"K\", \"\")) * 1_000\n    return float(vol)  # Convert directly if already a number\n\ndf[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n\n# Convert 'Change %' column (remove '%' and convert to float)\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Print final DataFrame\nprint(df.dtypes)\nprint(df)\n\n# Assign back to 'data' (if needed)\ndata = df\n\n#data = pd.merge(data, vix, on='Date', how='inner')\ndata.drop(['Date'], axis=1, inplace=True)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:13:14.213215Z","iopub.execute_input":"2025-03-01T10:13:14.213558Z","iopub.status.idle":"2025-03-01T10:13:14.257872Z","shell.execute_reply.started":"2025-03-01T10:13:14.213525Z","shell.execute_reply":"2025-03-01T10:13:14.257068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport tensorflow as tf\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# ---------------------------\n# 1. Set Seed for Reproducibility\n# ---------------------------\ndef set_random_seed(seed=20):\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    tf.config.experimental.enable_op_determinism()\n\nset_random_seed(20)\n\n# ---------------------------\n# 2. Load & Preprocess Data\n# ---------------------------\n# For demonstration, we use a basic univariate \"Close\" series.\n# Ensure that your DataFrame 'data' contains at least a \"Close\" column.\ndf = data[[\"Close\"]].copy()\nscaler = MinMaxScaler()\ndf[\"Close\"] = scaler.fit_transform(df)\n\n# ---------------------------\n# 3. Create 15-day Sequences\n# ---------------------------\ndef create_sequences(data, time_steps=15):\n    X, y = [], []\n    for i in range(len(data) - time_steps):\n        X.append(data[i : i + time_steps])\n        y.append(data[i + time_steps])\n    return np.array(X), np.array(y)\n\nX, y = create_sequences(df[\"Close\"].values, time_steps=15)\n\n# Split data: 70% train, 10% validation, 20% test.\ntrain_size = int(len(X) * 0.7)\nval_size = int(len(X) * 0.1)\nX_train, y_train = X[:train_size], y[:train_size]\nX_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\nX_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n\n# Reshape each sample to (time_steps, 1)\nX_train = X_train.reshape(-1, 15, 1)\nX_val   = X_val.reshape(-1, 15, 1)\nX_test  = X_test.reshape(-1, 15, 1)\n\n# ---------------------------\n# 4. Build Multi-Scale Hybrid Model\n# ---------------------------\nfrom tensorflow.keras.layers import Multiply, Activation\n\ndef build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n    inputs = Input(shape=(seq_length, 1))\n\n    # *Branch 1: Full 15-Day Analysis (Long-Term Trends)*\n    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n    full_branch = Dense(64, activation='relu')(full_branch)\n    long_term_return = Dense(1, activation='tanh')(full_branch)\n    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n\n    # *Branch 2: Recent 7-Day Analysis (Mid-Term Trends)*\n    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)  # Take the last 7 days\n    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n    recent_branch = Dense(64, activation='relu')(recent_branch)\n    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n\n    # *Branch 3: Very Recent 1-Day Analysis (Short-Term Trends)*\n    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)  # Take the last day's price\n    very_recent_branch = Flatten()(very_recent_input)\n    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n\n    # *Gating Mechanism (Forget Gate)*\n    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)  # Gate for 7-day impact\n    gate_1 = Dense(1, activation='sigmoid')(short_term_return)  # Gate for 1-day impact\n\n    gated_mid_term_return = Multiply()([mid_term_return, gate_7])  # 7-day result scaled by gate\n    gated_short_term_return = Multiply()([short_term_return, gate_1])  # 1-day result scaled by gate\n\n    # *Combine Long-Term, Gated Mid-Term, and Gated Short-Term Predictions*\n    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n    fused_returns = Dense(32, activation='relu')(fused_returns)\n    final_return = Dense(1, activation='tanh')(fused_returns)\n    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n\n    # *Use the last day's price for the final output calculation*\n    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, 0], axis=-1))(inputs)\n    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n\n    model = Model(inputs, final_output)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n\n    return model\n\n# *Build the Modified Model with Gating*\n# model_gated = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n# model_gated.summary()\n\n# *Build the Modified Model with Attention-Based Residuals*\nmodel_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\nmodel_attention.summary()\nearly_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\nhistory = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n\n# ---------------------------\n# 7. Evaluate the Model\n# ---------------------------\ny_pred = model_attention.predict(X_test)\ny_pred_inv = scaler.inverse_transform(y_pred)\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n\nr2 = r2_score(y_test_inv, y_pred_inv)\nrmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\nmae = mean_absolute_error(y_test_inv, y_pred_inv)\nmape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\nevs = explained_variance_score(y_test_inv, y_pred_inv)\n\nprint(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n\nplt.figure(figsize=(10,5))\nplt.plot(y_test_inv, label='Actual')\nplt.plot(y_pred_inv, label='Predicted')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Close Price\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:13:14.258623Z","iopub.execute_input":"2025-03-01T10:13:14.258821Z","iopub.status.idle":"2025-03-01T10:14:26.067543Z","shell.execute_reply.started":"2025-03-01T10:13:14.258803Z","shell.execute_reply":"2025-03-01T10:14:26.066765Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Working with Paper 4\n* This paper does prediction on Nifty50\n* The metric used is RMSE\n* Their result is RMSE - 171.4\n* We are training the model on data from 1st January 2020 till 31st December 2023 and testing it on data from 1st January\n  2024 till 15th March 2024.\n* Paper link:-https://www.semanticscholar.org/paper/Closing-Price-Prediction-for-the-NIFTY-50-Index%3A-A-Singh-Shah/d0eca144e2cd8e86c57eca34e3d9f4943f3c45f2","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndata=pd.read_csv(\"/kaggle/input/personal-dataset/Nifty 50 Train paper 2.csv\")\ndata2=pd.read_csv(\"/kaggle/input/personal-dataset/Nifty 50 Test paper2.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:14:26.068242Z","iopub.execute_input":"2025-03-01T10:14:26.068464Z","iopub.status.idle":"2025-03-01T10:14:26.087911Z","shell.execute_reply.started":"2025-03-01T10:14:26.068431Z","shell.execute_reply":"2025-03-01T10:14:26.087254Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.head(),data2.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:14:26.088796Z","iopub.execute_input":"2025-03-01T10:14:26.089158Z","iopub.status.idle":"2025-03-01T10:14:26.098067Z","shell.execute_reply.started":"2025-03-01T10:14:26.089115Z","shell.execute_reply":"2025-03-01T10:14:26.097313Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess(data):\n    data=data[::-1]\n    data.reset_index(drop=True, inplace=True)\n    print(data.head())\n    data.nunique()\n\n    data.sort_index(axis=1,ascending=True)\n    df=data\n    print(df)\n    data.rename(columns={'Price': 'Close'}, inplace=True)\n    print(data.head()),print(data.dtypes)\n    df = data.copy()  # Ensure we don't modify the original dataset\n\n# Convert financial columns to numeric (remove commas)\n    for col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n        df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\n    def convert_volume(vol):\n        if isinstance(vol, str):  # Ensure it's a string before replacing\n            vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n            if \"B\" in vol:\n                return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n            elif \"M\" in vol:\n                return float(vol.replace(\"M\", \"\")) * 1_000_000\n            elif \"K\" in vol:\n                return float(vol.replace(\"K\", \"\")) * 1_000\n        return float(vol)  # Convert directly if already a number\n\n    df[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n\n# Convert 'Change %' column (remove '%' and convert to float)\n    df[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Print final DataFrame\n    print(df.dtypes)\n    print(df)\n\n# Assign back to 'data' (if needed)\n    data = df\n    return data\n    #data=df\n#data.drop(\"Date\",axis=1,inplace=True)\n\ndata=preprocess(data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:14:26.098736Z","iopub.execute_input":"2025-03-01T10:14:26.099026Z","iopub.status.idle":"2025-03-01T10:14:26.141733Z","shell.execute_reply.started":"2025-03-01T10:14:26.099004Z","shell.execute_reply":"2025-03-01T10:14:26.141148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data2=preprocess(data2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:14:26.142385Z","iopub.execute_input":"2025-03-01T10:14:26.142618Z","iopub.status.idle":"2025-03-01T10:14:26.174319Z","shell.execute_reply.started":"2025-03-01T10:14:26.142587Z","shell.execute_reply":"2025-03-01T10:14:26.17351Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vix = pd.read_csv(\"/kaggle/input/vix-paper2/vix paper 2.csv\")\nvix.drop(\"Vol.\",axis=1,inplace=True)\nvix[\"Change %\"] = vix[\"Change %\"].str.replace(\"%\", \"\").astype(float)\nvix.head(),vix.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:14:26.175157Z","iopub.execute_input":"2025-03-01T10:14:26.175452Z","iopub.status.idle":"2025-03-01T10:14:26.193164Z","shell.execute_reply.started":"2025-03-01T10:14:26.175423Z","shell.execute_reply":"2025-03-01T10:14:26.192301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.merge(data, vix, on='Date', how='inner')\ndata.drop(\"Date\",axis=1,inplace=True)\nprint(data.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:14:26.193857Z","iopub.execute_input":"2025-03-01T10:14:26.194197Z","iopub.status.idle":"2025-03-01T10:14:26.209427Z","shell.execute_reply.started":"2025-03-01T10:14:26.194152Z","shell.execute_reply":"2025-03-01T10:14:26.20879Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data2 = pd.merge(data2, vix, on='Date', how='inner')\ndata2.drop(\"Date\",axis=1,inplace=True)\nprint(data2.head())\nprint(len(data2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:14:26.210066Z","iopub.execute_input":"2025-03-01T10:14:26.210308Z","iopub.status.idle":"2025-03-01T10:14:26.232092Z","shell.execute_reply.started":"2025-03-01T10:14:26.210277Z","shell.execute_reply":"2025-03-01T10:14:26.231234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport tensorflow as tf\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# ---------------------------\n# 1. Set Seed for Reproducibility\n# ---------------------------\ndef set_random_seed(seed=20):\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    tf.config.experimental.enable_op_determinism()\n\nset_random_seed(20)\n\n# ---------------------------\n# 2. Load & Preprocess Data\n# ---------------------------\n# For demonstration, we use a basic univariate \"Close\" series.\n# Ensure that your DataFrame 'data' contains at least a \"Close\" column.\ndf = data[[\"Close\"]].copy()\nscaler = MinMaxScaler()\ndf[\"Close\"] = scaler.fit_transform(df)\n\n# ---------------------------\n# 3. Create 15-day Sequences\n# ---------------------------\ndef create_sequences(data, time_steps=15):\n    X, y = [], []\n    for i in range(len(data) - time_steps):\n        X.append(data[i : i + time_steps])\n        y.append(data[i + time_steps])\n    return np.array(X), np.array(y)\n\nX, y = create_sequences(df[\"Close\"].values, time_steps=15)\n\n# Split data: 70% train, 10% validation, 20% test.\ntrain_size = int(len(X) * 0.7)\nval_size = int(len(X) * 0.1)\nX_train, y_train = X[:train_size], y[:train_size]\nX_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\nX_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n\n# Reshape each sample to (time_steps, 1)\nX_train = X_train.reshape(-1, 15, 1)\nX_val   = X_val.reshape(-1, 15, 1)\nX_test  = X_test.reshape(-1, 15, 1)\n\n# ---------------------------\n# 4. Build Multi-Scale Hybrid Model\n# ---------------------------\nfrom tensorflow.keras.layers import Multiply, Activation\n\ndef build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n    inputs = Input(shape=(seq_length, 1))\n\n    # *Branch 1: Full 15-Day Analysis (Long-Term Trends)*\n    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n    full_branch = Dense(64, activation='relu')(full_branch)\n    long_term_return = Dense(1, activation='tanh')(full_branch)\n    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n\n    # *Branch 2: Recent 7-Day Analysis (Mid-Term Trends)*\n    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)  # Take the last 7 days\n    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n    recent_branch = Dense(64, activation='relu')(recent_branch)\n    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n\n    # *Branch 3: Very Recent 1-Day Analysis (Short-Term Trends)*\n    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)  # Take the last day's price\n    very_recent_branch = Flatten()(very_recent_input)\n    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n\n    # *Gating Mechanism (Forget Gate)*\n    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)  # Gate for 7-day impact\n    gate_1 = Dense(1, activation='sigmoid')(short_term_return)  # Gate for 1-day impact\n\n    gated_mid_term_return = Multiply()([mid_term_return, gate_7])  # 7-day result scaled by gate\n    gated_short_term_return = Multiply()([short_term_return, gate_1])  # 1-day result scaled by gate\n\n    # *Combine Long-Term, Gated Mid-Term, and Gated Short-Term Predictions*\n    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n    fused_returns = Dense(32, activation='relu')(fused_returns)\n    final_return = Dense(1, activation='tanh')(fused_returns)\n    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n\n    # *Use the last day's price for the final output calculation*\n    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, 0], axis=-1))(inputs)\n    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n\n    model = Model(inputs, final_output)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n\n    return model\n\n# *Build the Modified Model with Gating*\n# model_gated = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n# model_gated.summary()\n\n# *Build the Modified Model with Attention-Based Residuals*\nmodel_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\nmodel_attention.summary()\nearly_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\nhistory = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n\n# ---------------------------\n# 7. Evaluate the Model\n# ---------------------------\ny_pred = model_attention.predict(X_test)\ny_pred_inv = scaler.inverse_transform(y_pred)\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n\nr2 = r2_score(y_test_inv, y_pred_inv)\nrmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\nmae = mean_absolute_error(y_test_inv, y_pred_inv)\nmape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\nevs = explained_variance_score(y_test_inv, y_pred_inv)\n\nprint(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n\nplt.figure(figsize=(10,5))\nplt.plot(y_test_inv, label='Actual')\nplt.plot(y_pred_inv, label='Predicted')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Close Price\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:14:26.232924Z","iopub.execute_input":"2025-03-01T10:14:26.233226Z","iopub.status.idle":"2025-03-01T10:14:57.19487Z","shell.execute_reply.started":"2025-03-01T10:14:26.233205Z","shell.execute_reply":"2025-03-01T10:14:57.194045Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## doing without vix","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/personal-dataset/Nifty 50 Train paper 2.csv\")\ndata2=pd.read_csv(\"/kaggle/input/personal-dataset/Nifty 50 Test paper2.csv\")\ndata=preprocess(data)\ndata2=preprocess(data2)\ndata2.drop(\"Date\",axis=1,inplace=True)\ndata.drop(\"Date\",axis=1,inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:14:57.195627Z","iopub.execute_input":"2025-03-01T10:14:57.195835Z","iopub.status.idle":"2025-03-01T10:14:57.256641Z","shell.execute_reply.started":"2025-03-01T10:14:57.195816Z","shell.execute_reply":"2025-03-01T10:14:57.256011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport tensorflow as tf\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# ---------------------------\n# 1. Set Seed for Reproducibility\n# ---------------------------\ndef set_random_seed(seed=20):\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    tf.config.experimental.enable_op_determinism()\n\nset_random_seed(20)\n\n# ---------------------------\n# 2. Load & Preprocess Data\n# ---------------------------\n# For demonstration, we use a basic univariate \"Close\" series.\n# Ensure that your DataFrame 'data' contains at least a \"Close\" column.\ndf = data[[\"Close\"]].copy()\nscaler = MinMaxScaler()\ndf[\"Close\"] = scaler.fit_transform(df)\n\n# ---------------------------\n# 3. Create 15-day Sequences\n# ---------------------------\ndef create_sequences(data, time_steps=15):\n    X, y = [], []\n    for i in range(len(data) - time_steps):\n        X.append(data[i : i + time_steps])\n        y.append(data[i + time_steps])\n    return np.array(X), np.array(y)\n\nX, y = create_sequences(df[\"Close\"].values, time_steps=15)\n\n# Split data: 70% train, 10% validation, 20% test.\ntrain_size = int(len(X) * 0.7)\nval_size = int(len(X) * 0.1)\nX_train, y_train = X[:train_size], y[:train_size]\nX_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\nX_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n\n# Reshape each sample to (time_steps, 1)\nX_train = X_train.reshape(-1, 15, 1)\nX_val   = X_val.reshape(-1, 15, 1)\nX_test  = X_test.reshape(-1, 15, 1)\n\n# ---------------------------\n# 4. Build Multi-Scale Hybrid Model\n# ---------------------------\nfrom tensorflow.keras.layers import Multiply, Activation\n\ndef build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n    inputs = Input(shape=(seq_length, 1))\n\n    # *Branch 1: Full 15-Day Analysis (Long-Term Trends)*\n    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n    full_branch = Dense(64, activation='relu')(full_branch)\n    long_term_return = Dense(1, activation='tanh')(full_branch)\n    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n\n    # *Branch 2: Recent 7-Day Analysis (Mid-Term Trends)*\n    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)  # Take the last 7 days\n    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n    recent_branch = Dense(64, activation='relu')(recent_branch)\n    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n\n    # *Branch 3: Very Recent 1-Day Analysis (Short-Term Trends)*\n    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)  # Take the last day's price\n    very_recent_branch = Flatten()(very_recent_input)\n    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n\n    # *Gating Mechanism (Forget Gate)*\n    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)  # Gate for 7-day impact\n    gate_1 = Dense(1, activation='sigmoid')(short_term_return)  # Gate for 1-day impact\n\n    gated_mid_term_return = Multiply()([mid_term_return, gate_7])  # 7-day result scaled by gate\n    gated_short_term_return = Multiply()([short_term_return, gate_1])  # 1-day result scaled by gate\n\n    # *Combine Long-Term, Gated Mid-Term, and Gated Short-Term Predictions*\n    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n    fused_returns = Dense(32, activation='relu')(fused_returns)\n    final_return = Dense(1, activation='tanh')(fused_returns)\n    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n\n    # *Use the last day's price for the final output calculation*\n    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, 0], axis=-1))(inputs)\n    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n\n    model = Model(inputs, final_output)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n\n    return model\n\n# *Build the Modified Model with Gating*\n# model_gated = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n# model_gated.summary()\n\n# *Build the Modified Model with Attention-Based Residuals*\nmodel_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\nmodel_attention.summary()\nearly_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\nhistory = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n\n# ---------------------------\n# 7. Evaluate the Model\n# ---------------------------\ny_pred = model_attention.predict(X_test)\ny_pred_inv = scaler.inverse_transform(y_pred)\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n\nr2 = r2_score(y_test_inv, y_pred_inv)\nrmse = mean_squared_error(y_test_inv, y_pred_inv, squared=False)\nmae = mean_absolute_error(y_test_inv, y_pred_inv)\nmape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\nevs = explained_variance_score(y_test_inv, y_pred_inv)\n\nprint(f\"ğŸ“Š Final Results - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%, EVS: {evs:.4f}\")\n\nplt.figure(figsize=(10,5))\nplt.plot(y_test_inv, label='Actual')\nplt.plot(y_pred_inv, label='Predicted')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Close Price\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:14:57.257463Z","iopub.execute_input":"2025-03-01T10:14:57.257676Z","iopub.status.idle":"2025-03-01T10:15:45.336945Z","shell.execute_reply.started":"2025-03-01T10:14:57.257657Z","shell.execute_reply":"2025-03-01T10:15:45.336167Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Working with Paper 5\n* This paper does prediction on Nifty50 and BSE Sensex\n* The ranges are:-\n* ## BSE( Data )\n 24/02/2015-27/11/2022\n\n* ## NSE(Data)\n 27/10/2006-27/11/2022\n* The metric used are RMSE,MSE,MAE\n* Their best results are:-\n* ## BSE 15 Day:\nMAE Scaled: 0.0522\n MSE Scaled: 0.0515\n RMSE Scaled: 0.1456\n* ## NSE 15 Day:\nMAE Scaled: 0.0712\n MSE Scaled: 0.0521\n  RMSE Scaled: 0.1365\n* Paper link:-https://ieeexplore.ieee.org/document/10481618","metadata":{}},{"cell_type":"markdown","source":"### Sensex (without vix)","metadata":{}},{"cell_type":"code","source":"\ndata=pd.read_csv(\"/kaggle/input/paper-5-deepstock-dataset/BSE Sensex 30 Historical Data ( paper - 5 ).csv\")\ndata=data[::-1]\ndata.reset_index(drop=True, inplace=True)\ndata.head()\ndata.nunique()\n\ndata.sort_index(axis=1,ascending=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True)\ndf = data.copy()  # Ensure we don't modify the original dataset\n\n# Convert financial columns to numeric (remove commas)\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\ndef convert_volume(vol):\n    if isinstance(vol, str):  # Ensure it's a string before replacing\n        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n        if \"B\" in vol:\n            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n        elif \"M\" in vol:\n            return float(vol.replace(\"M\", \"\")) * 1_000_000\n        elif \"K\" in vol:\n            return float(vol.replace(\"K\", \"\")) * 1_000\n    return float(vol)  # Convert directly if already a number\n\ndf[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n\n# Convert 'Change %' column (remove '%' and convert to float)\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Print final DataFrame\nprint(df.dtypes)\nprint(df)\n\n# Assign back to 'data' (if needed)\ndata = df\n\n#data = pd.merge(data, vix, on='Date', how='inner')\ndata.drop(['Date'], axis=1, inplace=True)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:15:45.337692Z","iopub.execute_input":"2025-03-01T10:15:45.337901Z","iopub.status.idle":"2025-03-01T10:15:45.385138Z","shell.execute_reply.started":"2025-03-01T10:15:45.337883Z","shell.execute_reply":"2025-03-01T10:15:45.384391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport tensorflow as tf\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# ---------------------------\n# 1. Set Seed for Reproducibility\n# ---------------------------\ndef set_random_seed(seed=20):\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    tf.config.experimental.enable_op_determinism()\n\nset_random_seed(20)\n\n# ---------------------------\n# 2. Load & Preprocess Data\n# ---------------------------\n# For demonstration, we use a basic univariate \"Close\" series.\n# Ensure that your DataFrame 'data' contains at least a \"Close\" column.\ndf = data[[\"Close\"]].copy()\nscaler = MinMaxScaler()\ndf[\"Close\"] = scaler.fit_transform(df)\n\n# ---------------------------\n# 3. Create 15-day Sequences\n# ---------------------------\ndef create_sequences(data, time_steps=15):\n    X, y = [], []\n    for i in range(len(data) - time_steps):\n        X.append(data[i : i + time_steps])\n        y.append(data[i + time_steps])\n    return np.array(X), np.array(y)\n\nX, y = create_sequences(df[\"Close\"].values, time_steps=15)\n\n# Split data: 70% train, 10% validation, 20% test.\ntrain_size = int(len(X) * 0.7)\nval_size = int(len(X) * 0.1)\nX_train, y_train = X[:train_size], y[:train_size]\nX_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\nX_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n\n# Reshape each sample to (time_steps, 1)\nX_train = X_train.reshape(-1, 15, 1)\nX_val   = X_val.reshape(-1, 15, 1)\nX_test  = X_test.reshape(-1, 15, 1)\n\n# ---------------------------\n# 4. Build Multi-Scale Hybrid Model\n# ---------------------------\nfrom tensorflow.keras.layers import Multiply, Activation\n\ndef build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n    inputs = Input(shape=(seq_length, 1))\n\n    # *Branch 1: Full 15-Day Analysis (Long-Term Trends)*\n    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n    full_branch = Dense(64, activation='relu')(full_branch)\n    long_term_return = Dense(1, activation='tanh')(full_branch)\n    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n\n    # *Branch 2: Recent 7-Day Analysis (Mid-Term Trends)*\n    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)  # Take the last 7 days\n    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n    recent_branch = Dense(64, activation='relu')(recent_branch)\n    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n\n    # *Branch 3: Very Recent 1-Day Analysis (Short-Term Trends)*\n    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)  # Take the last day's price\n    very_recent_branch = Flatten()(very_recent_input)\n    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n\n    # *Gating Mechanism (Forget Gate)*\n    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)  # Gate for 7-day impact\n    gate_1 = Dense(1, activation='sigmoid')(short_term_return)  # Gate for 1-day impact\n\n    gated_mid_term_return = Multiply()([mid_term_return, gate_7])  # 7-day result scaled by gate\n    gated_short_term_return = Multiply()([short_term_return, gate_1])  # 1-day result scaled by gate\n\n    # *Combine Long-Term, Gated Mid-Term, and Gated Short-Term Predictions*\n    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n    fused_returns = Dense(32, activation='relu')(fused_returns)\n    final_return = Dense(1, activation='tanh')(fused_returns)\n    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n\n    # *Use the last day's price for the final output calculation*\n    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, 0], axis=-1))(inputs)\n    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n\n    model = Model(inputs, final_output)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n\n    return model\n\n# *Build the Modified Model with Gating*\n# model_gated = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n# model_gated.summary()\n\n# *Build the Modified Model with Attention-Based Residuals*\nmodel_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\nmodel_attention.summary()\nearly_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\nhistory = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n\n# ---------------------------\n# 7. Evaluate the Model\n# ---------------------------\ny_pred = model_attention.predict(X_test)\n# Predictions on test set\n##y_pred = model.predict(X_test)\n\n# Inverse transform predictions and actual values\ny_pred_inv = scaler.inverse_transform(y_pred)\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n\n# Inverse transform predictions and actual values\ny_pred_inv = scaler.inverse_transform(y_pred)\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n\n# Calculate Metrics\nr2 = r2_score(y_test_inv, y_pred_inv)\nmae = mean_absolute_error(y_test_inv, y_pred_inv)\nmse = mean_squared_error(y_test_inv, y_pred_inv)\nrmse = np.sqrt(mse)  # Root Mean Squared Error\n\n# Calculate the actual range of the test set\nactual_range = np.max(y_test_inv) - np.min(y_test_inv)\n\n# Compute Scaled Metrics\nmae_scaled = mae / actual_range\nmse_scaled = mse / (actual_range ** 2)\nrmse_scaled = rmse / actual_range\n\n# Explained Variance Score\nevs = explained_variance_score(y_test_inv, y_pred_inv) \n\n# Print the results\nprint(f\"ğŸ“Š Final Results -\")\nprint(f\"  RÂ²: {r2:.4f}\")\nprint(f\"  MAE: {mae:.4f}, MAE Scaled: {mae_scaled:.4f}\")\nprint(f\"  MSE: {mse:.4f}, MSE Scaled: {mse_scaled:.4f}\")\nprint(f\"  RMSE: {rmse:.4f}, RMSE Scaled: {rmse_scaled:.4f}\")\nprint(f\"  MAPE: {mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100:.2f}%\")\nprint(f\"  EVS: {evs:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:15:45.385942Z","iopub.execute_input":"2025-03-01T10:15:45.386211Z","iopub.status.idle":"2025-03-01T10:16:26.580262Z","shell.execute_reply.started":"2025-03-01T10:15:45.386188Z","shell.execute_reply":"2025-03-01T10:16:26.579497Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Sensex with vix","metadata":{}},{"cell_type":"code","source":"vix = pd.read_csv(\"/kaggle/input/paper-5-deepstock-dataset/India VIX Historical Data ( paper 5 ).csv\")\nvix.drop(\"Vol.\",axis=1,inplace=True)\nvix[\"Change %\"] = vix[\"Change %\"].str.replace(\"%\", \"\").astype(float)\nvix.head(),vix.dtypes\ndata=pd.read_csv(\"/kaggle/input/paper-5-deepstock-dataset/BSE Sensex 30 Historical Data ( paper - 5 ).csv\")\ndata=data[::-1]\ndata.reset_index(drop=True, inplace=True)\ndata.head()\ndata.nunique()\n\ndata.sort_index(axis=1,ascending=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True)\ndf = data.copy()  # Ensure we don't modify the original dataset\n\n# Convert financial columns to numeric (remove commas)\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\ndef convert_volume(vol):\n    if isinstance(vol, str):  # Ensure it's a string before replacing\n        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n        if \"B\" in vol:\n            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n        elif \"M\" in vol:\n            return float(vol.replace(\"M\", \"\")) * 1_000_000\n        elif \"K\" in vol:\n            return float(vol.replace(\"K\", \"\")) * 1_000\n    return float(vol)  # Convert directly if already a number\n\ndf[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n\n# Convert 'Change %' column (remove '%' and convert to float)\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Print final DataFrame\nprint(df.dtypes)\nprint(df)\n\n# Assign back to 'data' (if needed)\ndata = df\n\ndata = pd.merge(data, vix, on='Date', how='inner')\ndata.drop(['Date'], axis=1, inplace=True)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:16:26.581222Z","iopub.execute_input":"2025-03-01T10:16:26.581468Z","iopub.status.idle":"2025-03-01T10:16:26.645653Z","shell.execute_reply.started":"2025-03-01T10:16:26.581446Z","shell.execute_reply":"2025-03-01T10:16:26.644957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport tensorflow as tf\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# ---------------------------\n# 1. Set Seed for Reproducibility\n# ---------------------------\ndef set_random_seed(seed=20):\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    tf.config.experimental.enable_op_determinism()\n\nset_random_seed(20)\n\n# ---------------------------\n# 2. Load & Preprocess Data\n# ---------------------------\n# For demonstration, we use a basic univariate \"Close\" series.\n# Ensure that your DataFrame 'data' contains at least a \"Close\" column.\ndf = data[[\"Close\"]].copy()\nscaler = MinMaxScaler()\ndf[\"Close\"] = scaler.fit_transform(df)\n\n# ---------------------------\n# 3. Create 15-day Sequences\n# ---------------------------\ndef create_sequences(data, time_steps=15):\n    X, y = [], []\n    for i in range(len(data) - time_steps):\n        X.append(data[i : i + time_steps])\n        y.append(data[i + time_steps])\n    return np.array(X), np.array(y)\n\nX, y = create_sequences(df[\"Close\"].values, time_steps=15)\n\n# Split data: 70% train, 10% validation, 20% test.\ntrain_size = int(len(X) * 0.7)\nval_size = int(len(X) * 0.1)\nX_train, y_train = X[:train_size], y[:train_size]\nX_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\nX_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n\n# Reshape each sample to (time_steps, 1)\nX_train = X_train.reshape(-1, 15, 1)\nX_val   = X_val.reshape(-1, 15, 1)\nX_test  = X_test.reshape(-1, 15, 1)\n\n# ---------------------------\n# 4. Build Multi-Scale Hybrid Model\n# ---------------------------\nfrom tensorflow.keras.layers import Multiply, Activation\n\ndef build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n    inputs = Input(shape=(seq_length, 1))\n\n    # *Branch 1: Full 15-Day Analysis (Long-Term Trends)*\n    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n    full_branch = Dense(64, activation='relu')(full_branch)\n    long_term_return = Dense(1, activation='tanh')(full_branch)\n    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n\n    # *Branch 2: Recent 7-Day Analysis (Mid-Term Trends)*\n    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)  # Take the last 7 days\n    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n    recent_branch = Dense(64, activation='relu')(recent_branch)\n    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n\n    # *Branch 3: Very Recent 1-Day Analysis (Short-Term Trends)*\n    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)  # Take the last day's price\n    very_recent_branch = Flatten()(very_recent_input)\n    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n\n    # *Gating Mechanism (Forget Gate)*\n    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)  # Gate for 7-day impact\n    gate_1 = Dense(1, activation='sigmoid')(short_term_return)  # Gate for 1-day impact\n\n    gated_mid_term_return = Multiply()([mid_term_return, gate_7])  # 7-day result scaled by gate\n    gated_short_term_return = Multiply()([short_term_return, gate_1])  # 1-day result scaled by gate\n\n    # *Combine Long-Term, Gated Mid-Term, and Gated Short-Term Predictions*\n    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n    fused_returns = Dense(32, activation='relu')(fused_returns)\n    final_return = Dense(1, activation='tanh')(fused_returns)\n    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n\n    # *Use the last day's price for the final output calculation*\n    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, 0], axis=-1))(inputs)\n    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n\n    model = Model(inputs, final_output)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n\n    return model\n\n# *Build the Modified Model with Gating*\n# model_gated = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n# model_gated.summary()\n\n# *Build the Modified Model with Attention-Based Residuals*\nmodel_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\nmodel_attention.summary()\nearly_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\nhistory = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n\n# ---------------------------\n# 7. Evaluate the Model\n# ---------------------------\ny_pred = model_attention.predict(X_test)\n# Predictions on test set\n##y_pred = model.predict(X_test)\n\n# Inverse transform predictions and actual values\ny_pred_inv = scaler.inverse_transform(y_pred)\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n\n# Inverse transform predictions and actual values\ny_pred_inv = scaler.inverse_transform(y_pred)\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n\n# Calculate Metrics\nr2 = r2_score(y_test_inv, y_pred_inv)\nmae = mean_absolute_error(y_test_inv, y_pred_inv)\nmse = mean_squared_error(y_test_inv, y_pred_inv)\nrmse = np.sqrt(mse)  # Root Mean Squared Error\n\n# Calculate the actual range of the test set\nactual_range = np.max(y_test_inv) - np.min(y_test_inv)\n\n# Compute Scaled Metrics\nmae_scaled = mae / actual_range\nmse_scaled = mse / (actual_range ** 2)\nrmse_scaled = rmse / actual_range\n\n# Explained Variance Score\nevs = explained_variance_score(y_test_inv, y_pred_inv) \n\n# Print the results\nprint(f\"ğŸ“Š Final Results -\")\nprint(f\"  RÂ²: {r2:.4f}\")\nprint(f\"  MAE: {mae:.4f}, MAE Scaled: {mae_scaled:.4f}\")\nprint(f\"  MSE: {mse:.4f}, MSE Scaled: {mse_scaled:.4f}\")\nprint(f\"  RMSE: {rmse:.4f}, RMSE Scaled: {rmse_scaled:.4f}\")\nprint(f\"  MAPE: {mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100:.2f}%\")\nprint(f\"  EVS: {evs:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:16:26.646636Z","iopub.execute_input":"2025-03-01T10:16:26.646939Z","iopub.status.idle":"2025-03-01T10:17:06.296883Z","shell.execute_reply.started":"2025-03-01T10:16:26.646904Z","shell.execute_reply":"2025-03-01T10:17:06.296199Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Nifty50 with vix","metadata":{}},{"cell_type":"code","source":"vix = pd.read_csv(\"/kaggle/input/paper-5-deepstock-dataset/India VIX Historical Data ( paper 5 ).csv\")\nvix.drop(\"Vol.\",axis=1,inplace=True)\nvix[\"Change %\"] = vix[\"Change %\"].str.replace(\"%\", \"\").astype(float)\nvix.head(),vix.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:17:06.297641Z","iopub.execute_input":"2025-03-01T10:17:06.297934Z","iopub.status.idle":"2025-03-01T10:17:06.325512Z","shell.execute_reply.started":"2025-03-01T10:17:06.297899Z","shell.execute_reply":"2025-03-01T10:17:06.324841Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/paper-5-deepstock-dataset/Nifty 50 Historical Data - paper( 5 ).csv\")\ndata=data[::-1]\ndata.reset_index(drop=True, inplace=True)\ndata.head()\ndata.nunique()\n\ndata.sort_index(axis=1,ascending=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True)\ndf = data.copy()  # Ensure we don't modify the original dataset\n\n# Convert financial columns to numeric (remove commas)\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\ndef convert_volume(vol):\n    if isinstance(vol, str):  # Ensure it's a string before replacing\n        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n        if \"B\" in vol:\n            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n        elif \"M\" in vol:\n            return float(vol.replace(\"M\", \"\")) * 1_000_000\n        elif \"K\" in vol:\n            return float(vol.replace(\"K\", \"\")) * 1_000\n    return float(vol)  # Convert directly if already a number\n\ndf[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n\n# Convert 'Change %' column (remove '%' and convert to float)\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Print final DataFrame\nprint(df.dtypes)\nprint(df)\n\n# Assign back to 'data' (if needed)\ndata = df\n\ndata = pd.merge(data, vix, on='Date', how='inner')\ndata.drop(['Date'], axis=1, inplace=True)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:17:06.326278Z","iopub.execute_input":"2025-03-01T10:17:06.326576Z","iopub.status.idle":"2025-03-01T10:17:06.398615Z","shell.execute_reply.started":"2025-03-01T10:17:06.326546Z","shell.execute_reply":"2025-03-01T10:17:06.397971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport tensorflow as tf\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# ---------------------------\n# 1. Set Seed for Reproducibility\n# ---------------------------\ndef set_random_seed(seed=20):\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    tf.config.experimental.enable_op_determinism()\n\nset_random_seed(20)\n\n# ---------------------------\n# 2. Load & Preprocess Data\n# ---------------------------\n# For demonstration, we use a basic univariate \"Close\" series.\n# Ensure that your DataFrame 'data' contains at least a \"Close\" column.\ndf = data[[\"Close\"]].copy()\nscaler = MinMaxScaler()\ndf[\"Close\"] = scaler.fit_transform(df)\n\n# ---------------------------\n# 3. Create 15-day Sequences\n# ---------------------------\ndef create_sequences(data, time_steps=15):\n    X, y = [], []\n    for i in range(len(data) - time_steps):\n        X.append(data[i : i + time_steps])\n        y.append(data[i + time_steps])\n    return np.array(X), np.array(y)\n\nX, y = create_sequences(df[\"Close\"].values, time_steps=15)\n\n# Split data: 70% train, 10% validation, 20% test.\ntrain_size = int(len(X) * 0.7)\nval_size = int(len(X) * 0.1)\nX_train, y_train = X[:train_size], y[:train_size]\nX_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\nX_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n\n# Reshape each sample to (time_steps, 1)\nX_train = X_train.reshape(-1, 15, 1)\nX_val   = X_val.reshape(-1, 15, 1)\nX_test  = X_test.reshape(-1, 15, 1)\n\n# ---------------------------\n# 4. Build Multi-Scale Hybrid Model\n# ---------------------------\nfrom tensorflow.keras.layers import Multiply, Activation\n\ndef build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n    inputs = Input(shape=(seq_length, 1))\n\n    # *Branch 1: Full 15-Day Analysis (Long-Term Trends)*\n    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n    full_branch = Dense(64, activation='relu')(full_branch)\n    long_term_return = Dense(1, activation='tanh')(full_branch)\n    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n\n    # *Branch 2: Recent 7-Day Analysis (Mid-Term Trends)*\n    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)  # Take the last 7 days\n    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n    recent_branch = Dense(64, activation='relu')(recent_branch)\n    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n\n    # *Branch 3: Very Recent 1-Day Analysis (Short-Term Trends)*\n    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)  # Take the last day's price\n    very_recent_branch = Flatten()(very_recent_input)\n    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n\n    # *Gating Mechanism (Forget Gate)*\n    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)  # Gate for 7-day impact\n    gate_1 = Dense(1, activation='sigmoid')(short_term_return)  # Gate for 1-day impact\n\n    gated_mid_term_return = Multiply()([mid_term_return, gate_7])  # 7-day result scaled by gate\n    gated_short_term_return = Multiply()([short_term_return, gate_1])  # 1-day result scaled by gate\n\n    # *Combine Long-Term, Gated Mid-Term, and Gated Short-Term Predictions*\n    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n    fused_returns = Dense(32, activation='relu')(fused_returns)\n    final_return = Dense(1, activation='tanh')(fused_returns)\n    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n\n    # *Use the last day's price for the final output calculation*\n    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, 0], axis=-1))(inputs)\n    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n\n    model = Model(inputs, final_output)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n\n    return model\n\n# *Build the Modified Model with Gating*\n# model_gated = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n# model_gated.summary()\n\n# *Build the Modified Model with Attention-Based Residuals*\nmodel_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\nmodel_attention.summary()\nearly_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\nhistory = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n\n# ---------------------------\n# 7. Evaluate the Model\n# ---------------------------\ny_pred = model_attention.predict(X_test)\n# Predictions on test set\n##y_pred = model.predict(X_test)\n\n# Inverse transform predictions and actual values\ny_pred_inv = scaler.inverse_transform(y_pred)\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n\n# Inverse transform predictions and actual values\ny_pred_inv = scaler.inverse_transform(y_pred)\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n\n# Calculate Metrics\nr2 = r2_score(y_test_inv, y_pred_inv)\nmae = mean_absolute_error(y_test_inv, y_pred_inv)\nmse = mean_squared_error(y_test_inv, y_pred_inv)\nrmse = np.sqrt(mse)  # Root Mean Squared Error\n\n# Calculate the actual range of the test set\nactual_range = np.max(y_test_inv) - np.min(y_test_inv)\n\n# Compute Scaled Metrics\nmae_scaled = mae / actual_range\nmse_scaled = mse / (actual_range ** 2)\nrmse_scaled = rmse / actual_range\n\n# Explained Variance Score\nevs = explained_variance_score(y_test_inv, y_pred_inv) \n\n# Print the results\nprint(f\"ğŸ“Š Final Results -\")\nprint(f\"  RÂ²: {r2:.4f}\")\nprint(f\"  MAE: {mae:.4f}, MAE Scaled: {mae_scaled:.4f}\")\nprint(f\"  MSE: {mse:.4f}, MSE Scaled: {mse_scaled:.4f}\")\nprint(f\"  RMSE: {rmse:.4f}, RMSE Scaled: {rmse_scaled:.4f}\")\nprint(f\"  MAPE: {mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100:.2f}%\")\nprint(f\"  EVS: {evs:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:17:06.39929Z","iopub.execute_input":"2025-03-01T10:17:06.399566Z","iopub.status.idle":"2025-03-01T10:18:48.81546Z","shell.execute_reply.started":"2025-03-01T10:17:06.399545Z","shell.execute_reply":"2025-03-01T10:18:48.814739Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Nifty50 without vix","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/paper-5-deepstock-dataset/Nifty 50 Historical Data - paper( 5 ).csv\")\ndata=data[::-1]\ndata.reset_index(drop=True, inplace=True)\ndata.head()\ndata.nunique()\n\ndata.sort_index(axis=1,ascending=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True)\ndf = data.copy()  # Ensure we don't modify the original dataset\n\n# Convert financial columns to numeric (remove commas)\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\ndef convert_volume(vol):\n    if isinstance(vol, str):  # Ensure it's a string before replacing\n        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n        if \"B\" in vol:\n            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n        elif \"M\" in vol:\n            return float(vol.replace(\"M\", \"\")) * 1_000_000\n        elif \"K\" in vol:\n            return float(vol.replace(\"K\", \"\")) * 1_000\n    return float(vol)  # Convert directly if already a number\n\ndf[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n\n# Convert 'Change %' column (remove '%' and convert to float)\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Print final DataFrame\nprint(df.dtypes)\nprint(df)\n\n# Assign back to 'data' (if needed)\ndata = df\n\n#data = pd.merge(data, vix, on='Date', how='inner')\ndata.drop(['Date'], axis=1, inplace=True)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:18:48.816317Z","iopub.execute_input":"2025-03-01T10:18:48.816645Z","iopub.status.idle":"2025-03-01T10:18:48.873334Z","shell.execute_reply.started":"2025-03-01T10:18:48.816611Z","shell.execute_reply":"2025-03-01T10:18:48.87269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport tensorflow as tf\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# ---------------------------\n# 1. Set Seed for Reproducibility\n# ---------------------------\ndef set_random_seed(seed=20):\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    tf.config.experimental.enable_op_determinism()\n\nset_random_seed(20)\n\n# ---------------------------\n# 2. Load & Preprocess Data\n# ---------------------------\n# For demonstration, we use a basic univariate \"Close\" series.\n# Ensure that your DataFrame 'data' contains at least a \"Close\" column.\ndf = data[[\"Close\"]].copy()\nscaler = MinMaxScaler()\ndf[\"Close\"] = scaler.fit_transform(df)\n\n# ---------------------------\n# 3. Create 15-day Sequences\n# ---------------------------\ndef create_sequences(data, time_steps=15):\n    X, y = [], []\n    for i in range(len(data) - time_steps):\n        X.append(data[i : i + time_steps])\n        y.append(data[i + time_steps])\n    return np.array(X), np.array(y)\n\nX, y = create_sequences(df[\"Close\"].values, time_steps=15)\n\n# Split data: 70% train, 10% validation, 20% test.\ntrain_size = int(len(X) * 0.7)\nval_size = int(len(X) * 0.1)\nX_train, y_train = X[:train_size], y[:train_size]\nX_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\nX_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n\n# Reshape each sample to (time_steps, 1)\nX_train = X_train.reshape(-1, 15, 1)\nX_val   = X_val.reshape(-1, 15, 1)\nX_test  = X_test.reshape(-1, 15, 1)\n\n# ---------------------------\n# 4. Build Multi-Scale Hybrid Model\n# ---------------------------\nfrom tensorflow.keras.layers import Multiply, Activation\n\ndef build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n    inputs = Input(shape=(seq_length, 1))\n\n    # *Branch 1: Full 15-Day Analysis (Long-Term Trends)*\n    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n    full_branch = Dense(64, activation='relu')(full_branch)\n    long_term_return = Dense(1, activation='tanh')(full_branch)\n    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n\n    # *Branch 2: Recent 7-Day Analysis (Mid-Term Trends)*\n    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)  # Take the last 7 days\n    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n    recent_branch = Dense(64, activation='relu')(recent_branch)\n    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n\n    # *Branch 3: Very Recent 1-Day Analysis (Short-Term Trends)*\n    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)  # Take the last day's price\n    very_recent_branch = Flatten()(very_recent_input)\n    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n\n    # *Gating Mechanism (Forget Gate)*\n    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)  # Gate for 7-day impact\n    gate_1 = Dense(1, activation='sigmoid')(short_term_return)  # Gate for 1-day impact\n\n    gated_mid_term_return = Multiply()([mid_term_return, gate_7])  # 7-day result scaled by gate\n    gated_short_term_return = Multiply()([short_term_return, gate_1])  # 1-day result scaled by gate\n\n    # *Combine Long-Term, Gated Mid-Term, and Gated Short-Term Predictions*\n    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n    fused_returns = Dense(32, activation='relu')(fused_returns)\n    final_return = Dense(1, activation='tanh')(fused_returns)\n    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n\n    # *Use the last day's price for the final output calculation*\n    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, 0], axis=-1))(inputs)\n    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n\n    model = Model(inputs, final_output)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n\n    return model\n\n# *Build the Modified Model with Gating*\n# model_gated = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n# model_gated.summary()\n\n# *Build the Modified Model with Attention-Based Residuals*\nmodel_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\nmodel_attention.summary()\nearly_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\nhistory = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n\n# ---------------------------\n# 7. Evaluate the Model\n# ---------------------------\ny_pred = model_attention.predict(X_test)\n# Predictions on test set\n##y_pred = model.predict(X_test)\n\n# Inverse transform predictions and actual values\ny_pred_inv = scaler.inverse_transform(y_pred)\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n\n# Inverse transform predictions and actual values\ny_pred_inv = scaler.inverse_transform(y_pred)\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n\n# Calculate Metrics\nr2 = r2_score(y_test_inv, y_pred_inv)\nmae = mean_absolute_error(y_test_inv, y_pred_inv)\nmse = mean_squared_error(y_test_inv, y_pred_inv)\nrmse = np.sqrt(mse)  # Root Mean Squared Error\n\n# Calculate the actual range of the test set\nactual_range = np.max(y_test_inv) - np.min(y_test_inv)\n\n# Compute Scaled Metrics\nmae_scaled = mae / actual_range\nmse_scaled = mse / (actual_range ** 2)\nrmse_scaled = rmse / actual_range\n\n# Explained Variance Score\nevs = explained_variance_score(y_test_inv, y_pred_inv) \n\n# Print the results\nprint(f\"ğŸ“Š Final Results -\")\nprint(f\"  RÂ²: {r2:.4f}\")\nprint(f\"  MAE: {mae:.4f}, MAE Scaled: {mae_scaled:.4f}\")\nprint(f\"  MSE: {mse:.4f}, MSE Scaled: {mse_scaled:.4f}\")\nprint(f\"  RMSE: {rmse:.4f}, RMSE Scaled: {rmse_scaled:.4f}\")\nprint(f\"  MAPE: {mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100:.2f}%\")\nprint(f\"  EVS: {evs:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:18:48.8743Z","iopub.execute_input":"2025-03-01T10:18:48.874647Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# OUR RANGE: SENSEX (15-3-2008 to 15-3-2024)","metadata":{}},{"cell_type":"code","source":"vix = pd.read_csv(\"/kaggle/input/our-data-15-3-2008-to-15-3-2024/India VIX Historical Data (3).csv\")\nvix.drop(\"Vol.\",axis=1,inplace=True)\nvix[\"Change %\"] = vix[\"Change %\"].str.replace(\"%\", \"\").astype(float)\nvix.head(),vix.dtypes\ndata=pd.read_csv(\"/kaggle/input/our-data-15-3-2008-to-15-3-2024/BSE Sensex 30 Historical Data (3).csv\")\ndata=data[::-1]\ndata.reset_index(drop=True, inplace=True)\ndata.head()\ndata.nunique()\n\ndata.sort_index(axis=1,ascending=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True)\ndf = data.copy()  # Ensure we don't modify the original dataset\n\n# Convert financial columns to numeric (remove commas)\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\ndef convert_volume(vol):\n    if isinstance(vol, str):  # Ensure it's a string before replacing\n        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n        if \"B\" in vol:\n            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n        elif \"M\" in vol:\n            return float(vol.replace(\"M\", \"\")) * 1_000_000\n        elif \"K\" in vol:\n            return float(vol.replace(\"K\", \"\")) * 1_000\n    return float(vol)  # Convert directly if already a number\n\ndf[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n\n# Convert 'Change %' column (remove '%' and convert to float)\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Print final DataFrame\nprint(df.dtypes)\nprint(df)\n\n# Assign back to 'data' (if needed)\ndata = df\n\ndata = pd.merge(data, vix, on='Date', how='inner')\ndata.drop(['Date'], axis=1, inplace=True)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T17:40:11.608998Z","iopub.execute_input":"2025-03-04T17:40:11.609304Z","iopub.status.idle":"2025-03-04T17:40:11.747306Z","shell.execute_reply.started":"2025-03-04T17:40:11.609283Z","shell.execute_reply":"2025-03-04T17:40:11.746614Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport tensorflow as tf\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# ---------------------------\n# 1. Set Seed for Reproducibility\n# ---------------------------\ndef set_random_seed(seed=20):\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    tf.config.experimental.enable_op_determinism()\n\nset_random_seed(20)\n\n# ---------------------------\n# 2. Load & Preprocess Data\n# ---------------------------\n# For demonstration, we use a basic univariate \"Close\" series.\n# Ensure that your DataFrame 'data' contains at least a \"Close\" column.\ndf = data[[\"Close\"]].copy()\nscaler = MinMaxScaler()\ndf[\"Close\"] = scaler.fit_transform(df)\n\n# ---------------------------\n# 3. Create 15-day Sequences\n# ---------------------------\ndef create_sequences(data, time_steps=15):\n    X, y = [], []\n    for i in range(len(data) - time_steps):\n        X.append(data[i : i + time_steps])\n        y.append(data[i + time_steps])\n    return np.array(X), np.array(y)\n\nX, y = create_sequences(df[\"Close\"].values, time_steps=15)\n\n# Split data: 70% train, 10% validation, 20% test.\ntrain_size = int(len(X) * 0.7)\nval_size = int(len(X) * 0.1)\nX_train, y_train = X[:train_size], y[:train_size]\nX_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\nX_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n\n# Reshape each sample to (time_steps, 1)\nX_train = X_train.reshape(-1, 15, 1)\nX_val   = X_val.reshape(-1, 15, 1)\nX_test  = X_test.reshape(-1, 15, 1)\n\n# ---------------------------\n# 4. Build Multi-Scale Hybrid Model\n# ---------------------------\nfrom tensorflow.keras.layers import Multiply, Activation\n\ndef build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n    inputs = Input(shape=(seq_length, 1))\n\n    # *Branch 1: Full 15-Day Analysis (Long-Term Trends)*\n    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n    full_branch = Dense(64, activation='relu')(full_branch)\n    long_term_return = Dense(1, activation='tanh')(full_branch)\n    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n\n    # *Branch 2: Recent 7-Day Analysis (Mid-Term Trends)*\n    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)  # Take the last 7 days\n    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n    recent_branch = Dense(64, activation='relu')(recent_branch)\n    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n\n    # *Branch 3: Very Recent 1-Day Analysis (Short-Term Trends)*\n    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)  # Take the last day's price\n    very_recent_branch = Flatten()(very_recent_input)\n    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n\n    # *Gating Mechanism (Forget Gate)*\n    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)  # Gate for 7-day impact\n    gate_1 = Dense(1, activation='sigmoid')(short_term_return)  # Gate for 1-day impact\n\n    gated_mid_term_return = Multiply()([mid_term_return, gate_7])  # 7-day result scaled by gate\n    gated_short_term_return = Multiply()([short_term_return, gate_1])  # 1-day result scaled by gate\n\n    # *Combine Long-Term, Gated Mid-Term, and Gated Short-Term Predictions*\n    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n    fused_returns = Dense(32, activation='relu')(fused_returns)\n    final_return = Dense(1, activation='tanh')(fused_returns)\n    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n\n    # *Use the last day's price for the final output calculation*\n    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, 0], axis=-1))(inputs)\n    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n\n    model = Model(inputs, final_output)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n\n    return model\n\n# *Build the Modified Model with Gating*\n# model_gated = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n# model_gated.summary()\n\n# *Build the Modified Model with Attention-Based Residuals*\nmodel_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\nmodel_attention.summary()\nearly_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\nhistory = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n\n# ---------------------------\n# 7. Evaluate the Model\n# ---------------------------\ny_pred = model_attention.predict(X_test)\n# Predictions on test set\n##y_pred = model.predict(X_test)\n\n# Inverse transform predictions and actual values\ny_pred_inv = scaler.inverse_transform(y_pred)\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n\n# Inverse transform predictions and actual values\ny_pred_inv = scaler.inverse_transform(y_pred)\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n\n# Calculate Metrics\nr2 = r2_score(y_test_inv, y_pred_inv)\nmae = mean_absolute_error(y_test_inv, y_pred_inv)\nmse = mean_squared_error(y_test_inv, y_pred_inv)\nrmse = np.sqrt(mse)  # Root Mean Squared Error\n\n# Calculate the actual range of the test set\nactual_range = np.max(y_test_inv) - np.min(y_test_inv)\n\n# Compute Scaled Metrics\nmae_scaled = mae / actual_range\nmse_scaled = mse / (actual_range ** 2)\nrmse_scaled = rmse / actual_range\n\n# Explained Variance Score\nevs = explained_variance_score(y_test_inv, y_pred_inv) \n\n# Print the results\nprint(f\"ğŸ“Š Final Results -\")\nprint(f\"  RÂ²: {r2:.4f}\")\nprint(f\"  MAE: {mae:.4f}, MAE Scaled: {mae_scaled:.4f}\")\nprint(f\"  MSE: {mse:.4f}, MSE Scaled: {mse_scaled:.4f}\")\nprint(f\"  RMSE: {rmse:.4f}, RMSE Scaled: {rmse_scaled:.4f}\")\nprint(f\"  MAPE: {mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100:.2f}%\")\nprint(f\"  EVS: {evs:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T17:40:31.682308Z","iopub.execute_input":"2025-03-04T17:40:31.68263Z","iopub.status.idle":"2025-03-04T17:41:55.346636Z","shell.execute_reply.started":"2025-03-04T17:40:31.682601Z","shell.execute_reply":"2025-03-04T17:41:55.345909Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# OUR RANGE: NIFTY50 (15-3-2008 to 15-3-2024)","metadata":{}},{"cell_type":"code","source":"vix = pd.read_csv(\"/kaggle/input/our-data-15-3-2008-to-15-3-2024/India VIX Historical Data (3).csv\")\nvix.drop(\"Vol.\",axis=1,inplace=True)\nvix[\"Change %\"] = vix[\"Change %\"].str.replace(\"%\", \"\").astype(float)\nvix.head(),vix.dtypes\ndata=pd.read_csv(\"/kaggle/input/our-data-15-3-2008-to-15-3-2024/Nifty 50 Historical Data (2).csv\")\ndata=data[::-1]\ndata.reset_index(drop=True, inplace=True)\ndata.head()\ndata.nunique()\n\ndata.sort_index(axis=1,ascending=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True)\ndf = data.copy()  # Ensure we don't modify the original dataset\n\n# Convert financial columns to numeric (remove commas)\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\ndef convert_volume(vol):\n    if isinstance(vol, str):  # Ensure it's a string before replacing\n        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n        if \"B\" in vol:\n            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n        elif \"M\" in vol:\n            return float(vol.replace(\"M\", \"\")) * 1_000_000\n        elif \"K\" in vol:\n            return float(vol.replace(\"K\", \"\")) * 1_000\n    return float(vol)  # Convert directly if already a number\n\ndf[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n\n# Convert 'Change %' column (remove '%' and convert to float)\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Print final DataFrame\nprint(df.dtypes)\nprint(df)\n\n# Assign back to 'data' (if needed)\ndata = df\n\ndata = pd.merge(data, vix, on='Date', how='inner')\ndata.drop(['Date'], axis=1, inplace=True)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T17:44:19.236863Z","iopub.execute_input":"2025-03-04T17:44:19.237485Z","iopub.status.idle":"2025-03-04T17:44:19.328615Z","shell.execute_reply.started":"2025-03-04T17:44:19.237456Z","shell.execute_reply":"2025-03-04T17:44:19.327767Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport tensorflow as tf\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# ---------------------------\n# 1. Set Seed for Reproducibility\n# ---------------------------\ndef set_random_seed(seed=20):\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    tf.config.experimental.enable_op_determinism()\n\nset_random_seed(20)\n\n# ---------------------------\n# 2. Load & Preprocess Data\n# ---------------------------\n# For demonstration, we use a basic univariate \"Close\" series.\n# Ensure that your DataFrame 'data' contains at least a \"Close\" column.\ndf = data[[\"Close\"]].copy()\nscaler = MinMaxScaler()\ndf[\"Close\"] = scaler.fit_transform(df)\n\n# ---------------------------\n# 3. Create 15-day Sequences\n# ---------------------------\ndef create_sequences(data, time_steps=15):\n    X, y = [], []\n    for i in range(len(data) - time_steps):\n        X.append(data[i : i + time_steps])\n        y.append(data[i + time_steps])\n    return np.array(X), np.array(y)\n\nX, y = create_sequences(df[\"Close\"].values, time_steps=15)\n\n# Split data: 70% train, 10% validation, 20% test.\ntrain_size = int(len(X) * 0.7)\nval_size = int(len(X) * 0.1)\nX_train, y_train = X[:train_size], y[:train_size]\nX_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\nX_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n\n# Reshape each sample to (time_steps, 1)\nX_train = X_train.reshape(-1, 15, 1)\nX_val   = X_val.reshape(-1, 15, 1)\nX_test  = X_test.reshape(-1, 15, 1)\n\n# ---------------------------\n# 4. Build Multi-Scale Hybrid Model\n# ---------------------------\nfrom tensorflow.keras.layers import Multiply, Activation\n\ndef build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n    inputs = Input(shape=(seq_length, 1))\n\n    # *Branch 1: Full 15-Day Analysis (Long-Term Trends)*\n    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n    full_branch = Dense(64, activation='relu')(full_branch)\n    long_term_return = Dense(1, activation='tanh')(full_branch)\n    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n\n    # *Branch 2: Recent 7-Day Analysis (Mid-Term Trends)*\n    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)  # Take the last 7 days\n    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n    recent_branch = Dense(64, activation='relu')(recent_branch)\n    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n\n    # *Branch 3: Very Recent 1-Day Analysis (Short-Term Trends)*\n    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)  # Take the last day's price\n    very_recent_branch = Flatten()(very_recent_input)\n    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n\n    # *Gating Mechanism (Forget Gate)*\n    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)  # Gate for 7-day impact\n    gate_1 = Dense(1, activation='sigmoid')(short_term_return)  # Gate for 1-day impact\n\n    gated_mid_term_return = Multiply()([mid_term_return, gate_7])  # 7-day result scaled by gate\n    gated_short_term_return = Multiply()([short_term_return, gate_1])  # 1-day result scaled by gate\n\n    # *Combine Long-Term, Gated Mid-Term, and Gated Short-Term Predictions*\n    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n    fused_returns = Dense(32, activation='relu')(fused_returns)\n    final_return = Dense(1, activation='tanh')(fused_returns)\n    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n\n    # *Use the last day's price for the final output calculation*\n    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, 0], axis=-1))(inputs)\n    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n\n    model = Model(inputs, final_output)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n\n    return model\n\n# *Build the Modified Model with Gating*\n# model_gated = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n# model_gated.summary()\n\n# *Build the Modified Model with Attention-Based Residuals*\nmodel_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\nmodel_attention.summary()\nearly_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\nhistory = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n\n# ---------------------------\n# 7. Evaluate the Model\n# ---------------------------\ny_pred = model_attention.predict(X_test)\n# Predictions on test set\n##y_pred = model.predict(X_test)\n\n# Inverse transform predictions and actual values\ny_pred_inv = scaler.inverse_transform(y_pred)\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n\n# Inverse transform predictions and actual values\ny_pred_inv = scaler.inverse_transform(y_pred)\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n\n# Calculate Metrics\nr2 = r2_score(y_test_inv, y_pred_inv)\nmae = mean_absolute_error(y_test_inv, y_pred_inv)\nmse = mean_squared_error(y_test_inv, y_pred_inv)\nrmse = np.sqrt(mse)  # Root Mean Squared Error\n\n# Calculate the actual range of the test set\nactual_range = np.max(y_test_inv) - np.min(y_test_inv)\n\n# Compute Scaled Metrics\nmae_scaled = mae / actual_range\nmse_scaled = mse / (actual_range ** 2)\nrmse_scaled = rmse / actual_range\n\n# Explained Variance Score\nevs = explained_variance_score(y_test_inv, y_pred_inv) \n\n# Print the results\nprint(f\"ğŸ“Š Final Results -\")\nprint(f\"  RÂ²: {r2:.4f}\")\nprint(f\"  MAE: {mae:.4f}, MAE Scaled: {mae_scaled:.4f}\")\nprint(f\"  MSE: {mse:.4f}, MSE Scaled: {mse_scaled:.4f}\")\nprint(f\"  RMSE: {rmse:.4f}, RMSE Scaled: {rmse_scaled:.4f}\")\nprint(f\"  MAPE: {mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100:.2f}%\")\nprint(f\"  EVS: {evs:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T17:44:26.082007Z","iopub.execute_input":"2025-03-04T17:44:26.082281Z","iopub.status.idle":"2025-03-04T17:46:06.758786Z","shell.execute_reply.started":"2025-03-04T17:44:26.08226Z","shell.execute_reply":"2025-03-04T17:46:06.757895Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DOING FOR SP500","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the CSV file\ndf = pd.read_csv(\"/kaggle/input/personal-dateset-vix/SP 500 (2).csv\")\n\n# Ensure the required columns exist\nif \"Price\" in df.columns and \"Open\" in df.columns:\n    # Calculate the percentage change\n    df[\"Change %\"] = ((df[\"Price\"] - df[\"Open\"]) / df[\"Price\"]) * 100\n\n    # Save the updated CSV file\n    # df.to_csv(\"SP 500 (2).csv\", index=False)\n\n    # Display first few rows\n    print(df.head())\nelse:\n    print(\"Error: CSV file must contain 'Price' and 'Open' columns.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T10:59:27.048012Z","iopub.execute_input":"2025-03-11T10:59:27.048337Z","iopub.status.idle":"2025-03-11T10:59:27.087290Z","shell.execute_reply.started":"2025-03-11T10:59:27.048312Z","shell.execute_reply":"2025-03-11T10:59:27.086302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nvix = pd.read_csv(\"/kaggle/input/personal-dateset-vix/SP 500 VIX.csv\")\nvix.drop(\"Vol.\",axis=1,inplace=True)\ndef convert_volume(val):\n    if pd.isna(val) or val == '-':  # Handle NaN or '-' cases\n        return np.nan\n    val = val.strip().replace(',', '')  # Remove spaces & commas\n    if val[-1] == 'K': return float(val[:-1]) * 1e3\n    if val[-1] == 'M': return float(val[:-1]) * 1e6\n    if val[-1] == 'B': return float(val[:-1]) * 1e9\n    return float(val)\n\n#vix[\"Vol.\"] = vix[\"Vol.\"].apply(convert_volume)\nvix.rename(columns={'Open':'Vopen','High':'Vhigh','Low':'Vlow','Change %':'Vchange%'}, inplace=True)\nvix[\"Vchange%\"] = vix[\"Vchange%\"].str.replace(\"%\", \"\").astype(float)\nvix.head(),vix.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T10:59:44.306282Z","iopub.execute_input":"2025-03-11T10:59:44.306598Z","iopub.status.idle":"2025-03-11T10:59:44.349902Z","shell.execute_reply.started":"2025-03-11T10:59:44.306576Z","shell.execute_reply":"2025-03-11T10:59:44.348799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data=df\ndata=data[::-1]\ndata.reset_index(drop=True, inplace=True)\ndata.head()\ndata.nunique()\n\ndata.sort_index(axis=1,ascending=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True)\ndf = data.copy()  # Ensure we don't modify the original dataset\n\n# Convert financial columns to numeric (remove commas)\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\n#df.drop(\"Vol.\",axis=1,inplace=True)\n\n# Convert 'Change %' column (remove '%' and convert to float)\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Print final DataFrame\nprint(df.dtypes)\nprint(df)\n\n# Assign back to 'data' (if needed)\ndata = df\ndata['Date'] = pd.to_datetime(data['Date'], format=\"%d-%m-%Y\")\n\n# For 'vix', the date format is \"MM/DD/YYYY\"\nvix['Date'] = pd.to_datetime(vix['Date'], format=\"%m/%d/%Y\")\ndata = pd.merge(data, vix, on='Date', how='inner')\nrow=data[data[\"Vol.\"].isna()]\ndata.drop(['Date'], axis=1, inplace=True)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T10:59:49.571556Z","iopub.execute_input":"2025-03-11T10:59:49.571912Z","iopub.status.idle":"2025-03-11T10:59:49.676868Z","shell.execute_reply.started":"2025-03-11T10:59:49.571869Z","shell.execute_reply":"2025-03-11T10:59:49.675988Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport tensorflow as tf\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Lambda, Concatenate, Add, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# ---------------------------\n# 1. Set Seed for Reproducibility\n# ---------------------------\ndef set_random_seed(seed=20):\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    tf.config.experimental.enable_op_determinism()\n\nset_random_seed(20)\n\n# ---------------------------\n# 2. Load & Preprocess Data\n# ---------------------------\n# For demonstration, we use a basic univariate \"Close\" series.\n# Ensure that your DataFrame 'data' contains at least a \"Close\" column.\ndf = data[[\"Close\"]].copy()\nscaler = MinMaxScaler()\ndf[\"Close\"] = scaler.fit_transform(df)\n\n# ---------------------------\n# 3. Create 15-day Sequences\n# ---------------------------\ndef create_sequences(data, time_steps=15):\n    X, y = [], []\n    for i in range(len(data) - time_steps):\n        X.append(data[i : i + time_steps])\n        y.append(data[i + time_steps])\n    return np.array(X), np.array(y)\n\nX, y = create_sequences(df[\"Close\"].values, time_steps=15)\n\n# Split data: 70% train, 10% validation, 20% test.\ntrain_size = int(len(X) * 0.7)\nval_size = int(len(X) * 0.1)\nX_train, y_train = X[:train_size], y[:train_size]\nX_val, y_val     = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\nX_test, y_test   = X[train_size+val_size:], y[train_size+val_size:]\n\n# Reshape each sample to (time_steps, 1)\nX_train = X_train.reshape(-1, 15, 1)\nX_val   = X_val.reshape(-1, 15, 1)\nX_test  = X_test.reshape(-1, 15, 1)\n\n# ---------------------------\n# 4. Build Multi-Scale Hybrid Model\n# ---------------------------\nfrom tensorflow.keras.layers import Multiply, Activation\n\ndef build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1):\n    inputs = Input(shape=(seq_length, 1))\n\n    # *Branch 1: Full 15-Day Analysis (Long-Term Trends)*\n    full_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(inputs)\n    full_branch = Dense(64, activation='relu')(full_branch)\n    long_term_return = Dense(1, activation='tanh')(full_branch)\n    long_term_return = Lambda(lambda x: x * scale_factor)(long_term_return)\n\n    # *Branch 2: Recent 7-Day Analysis (Mid-Term Trends)*\n    recent_input = Lambda(lambda x: x[:, -7:, :])(inputs)  # Take the last 7 days\n    recent_branch = Bidirectional(LSTM(128, return_sequences=False, dropout=0.2))(recent_input)\n    recent_branch = Dense(64, activation='relu')(recent_branch)\n    mid_term_return = Dense(1, activation='tanh')(recent_branch)\n    mid_term_return = Lambda(lambda x: x * scale_factor)(mid_term_return)\n\n    # *Branch 3: Very Recent 1-Day Analysis (Short-Term Trends)*\n    very_recent_input = Lambda(lambda x: x[:, -1:, :])(inputs)  # Take the last day's price\n    very_recent_branch = Flatten()(very_recent_input)\n    very_recent_branch = Dense(32, activation='relu')(very_recent_branch)\n    short_term_return = Dense(1, activation='tanh')(very_recent_branch)\n    short_term_return = Lambda(lambda x: x * recent_scale)(short_term_return)\n\n    # *Gating Mechanism (Forget Gate)*\n    gate_7 = Dense(1, activation='sigmoid')(mid_term_return)  # Gate for 7-day impact\n    gate_1 = Dense(1, activation='sigmoid')(short_term_return)  # Gate for 1-day impact\n\n    gated_mid_term_return = Multiply()([mid_term_return, gate_7])  # 7-day result scaled by gate\n    gated_short_term_return = Multiply()([short_term_return, gate_1])  # 1-day result scaled by gate\n\n    # *Combine Long-Term, Gated Mid-Term, and Gated Short-Term Predictions*\n    fused_returns = Add()([long_term_return, gated_mid_term_return, gated_short_term_return])\n    fused_returns = Dense(32, activation='relu')(fused_returns)\n    final_return = Dense(1, activation='tanh')(fused_returns)\n    final_return = Lambda(lambda x: x * scale_factor)(final_return)\n\n    # *Use the last day's price for the final output calculation*\n    last_day = Lambda(lambda x: tf.expand_dims(x[:, -1, 0], axis=-1))(inputs)\n    final_output = Lambda(lambda inputs: inputs[0] * (1 + inputs[1]))([last_day, final_return])\n\n    model = Model(inputs, final_output)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n\n    return model\n\n# *Build the Modified Model with Gating*\n# model_gated = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\n# model_gated.summary()\n\n# *Build the Modified Model with Attention-Based Residuals*\nmodel_attention = build_gated_multi_scale_model(seq_length=15, scale_factor=0.2, recent_scale=0.1)\nmodel_attention.summary()\nearly_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\nhistory = model_attention.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=8, callbacks=[early_stop], verbose=1)\n\n# ---------------------------\n# 7. Evaluate the Model\n# ---------------------------\ny_pred = model_attention.predict(X_test)\n# Predictions on test set\n##y_pred = model.predict(X_test)\n\n# Inverse transform predictions and actual values\ny_pred_inv = scaler.inverse_transform(y_pred)\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n\n# Inverse transform predictions and actual values\ny_pred_inv = scaler.inverse_transform(y_pred)\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n\n# Calculate Metrics\nr2 = r2_score(y_test_inv, y_pred_inv)\nmae = mean_absolute_error(y_test_inv, y_pred_inv)\nmse = mean_squared_error(y_test_inv, y_pred_inv)\nrmse = np.sqrt(mse)  # Root Mean Squared Error\n\n# Calculate the actual range of the test set\nactual_range = np.max(y_test_inv) - np.min(y_test_inv)\n\n# Compute Scaled Metrics\nmae_scaled = mae / actual_range\nmse_scaled = mse / (actual_range ** 2)\nrmse_scaled = rmse / actual_range\n\n# Explained Variance Score\nevs = explained_variance_score(y_test_inv, y_pred_inv) \n\n# Print the results\nprint(f\"ğŸ“Š Final Results -\")\nprint(f\"  RÂ²: {r2:.4f}\")\nprint(f\"  MAE: {mae:.4f}, MAE Scaled: {mae_scaled:.4f}\")\nprint(f\"  MSE: {mse:.4f}, MSE Scaled: {mse_scaled:.4f}\")\nprint(f\"  RMSE: {rmse:.4f}, RMSE Scaled: {rmse_scaled:.4f}\")\nprint(f\"  MAPE: {mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100:.2f}%\")\nprint(f\"  EVS: {evs:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T11:02:01.363970Z","iopub.execute_input":"2025-03-11T11:02:01.364323Z","iopub.status.idle":"2025-03-11T11:03:22.727985Z","shell.execute_reply.started":"2025-03-11T11:02:01.364298Z","shell.execute_reply":"2025-03-11T11:03:22.726934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nplt.figure(figsize=(10,5))\nplt.plot(y_test_inv, label='Actual')\nplt.plot(y_pred_inv, label='Predicted')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Close Price\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T11:03:22.729452Z","iopub.execute_input":"2025-03-11T11:03:22.729974Z","iopub.status.idle":"2025-03-11T11:03:23.036119Z","shell.execute_reply.started":"2025-03-11T11:03:22.729948Z","shell.execute_reply":"2025-03-11T11:03:23.035106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}